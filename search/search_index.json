{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to AIGVE!","text":""},{"location":"blog/","title":"Blog","text":"<p>...</p>"},{"location":"contact/","title":"Contact us","text":""},{"location":"contact/#for-contributing","title":"For contributing","text":"<p>Contributing to the project, including report/fix bugs, add new features, improve documentation, and send feedbacks, can all be done via the project github issue page at https://github.com/ShaneXiangH/VQA_Toolkit/issues.</p>"},{"location":"contact/#for-licenses","title":"For Licenses","text":"<p><code>AIGVE</code> is an open-source project. For more information about the licenses, please refer to the license page.</p>"},{"location":"contact/license/","title":"Licenses","text":"<p>Copyright \u00a9 2025 IFM Lab. All rights reserved.</p>"},{"location":"documentations/","title":"AIGVE-Tool Documentation","text":"<p>The documentations of the <code>AIGVE</code> library will be organized as follows.</p> <ul> <li> <code>aigve</code> library for assessing AI-generated video quality</li> </ul> <ul> <li> <code>aigve.config</code> for parameter configuration and management</li> </ul> <ul> <li> <code>aigve.core</code> for video evaluation process design</li> </ul> <ul> <li> <code>aigve.datasets</code> for dataset loading design</li> </ul> <ul> <li> <code>aigve.metrics</code> for video evaluation metrics design and building</li> </ul> <ul> <li> <code>aigve.utils</code> for utility function definition</li> </ul>"},{"location":"documentations/aigve/","title":"aigve","text":"<p>This aigve library provides a comprehensive and structured evaluation framework  for assessing AI-generated video quality. It integrates multiple evaluation metrics,  covering diverse aspects of video evaluation, including neural-network-based assessment,  distribution comparison, vision-language alignment, and multi-faceted analysis.</p>"},{"location":"documentations/aigve/#aigve.CLIPSimScore","title":"<code>CLIPSimScore</code>","text":"<p>               Bases: <code>BaseMetric</code></p> <p>Initialize the <code>CLIPSimScore</code> evaluator.</p> <p>Parameters:</p> Name Type Description Default <code>processor_name</code> <code>str</code> <p>The name of the CLIP processor, which wraps a CLIP feature extractor and a CLIP tokenizer into this single procesor.                Defaults to <code>openai/clip-vit-base-patch32</code>.</p> <code>'openai/clip-vit-base-patch32'</code> <code>model_name</code> <code>str</code> <p>The name of the CLIP model. Defaults to <code>openai/clip-vit-base-patch32</code>.</p> <code>'openai/clip-vit-base-patch32'</code> <code>logit_scale</code> <code>bool</code> <p>Whether to calcualte the cosine similarity as logits. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <p>None</p> Source code in <code>aigve/metrics/text_video_alignment/similarity_based/clipscore/clipsim.py</code> <pre><code>@METRICS.register_module()\nclass CLIPSimScore(BaseMetric):\n    \"\"\" Initialize the ``CLIPSimScore`` evaluator.\n\n    Args:\n            processor_name (str): The name of the CLIP processor, which wraps a CLIP feature extractor and a CLIP tokenizer into this single procesor. \n                                  Defaults to ``openai/clip-vit-base-patch32``.\n            model_name (str): The name of the CLIP model. Defaults to ``openai/clip-vit-base-patch32``.\n            logit_scale (bool): Whether to calcualte the cosine similarity as logits. Defaults to False.\n\n    Returns:\n            None\n    \"\"\"\n    def __init__(self,\n                 processor_name: str = \"openai/clip-vit-base-patch32\",\n                 model_name: str = \"openai/clip-vit-base-patch32\",\n                 logit_scale: bool = False,\n                #  train_index: int = 4\n                 ) -&gt; None:\n        super().__init__()\n        self.processor_name = processor_name\n        self.model_name = model_name\n        self.logit_scale = logit_scale\n\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self.processor = AutoProcessor.from_pretrained(self.processor_name)\n        self.model = CLIPModel.from_pretrained(self.model_name).to(self.device)\n        self.model.eval()\n\n\n# def process(self, data_batch: dict, data_samples: Sequence[dict]) -&gt; None:\n    def process(self, data_batch: Sequence, data_samples: Sequence) -&gt; None:\n        \"\"\"CLIPSimScore process\n        Process one batch of data samples and predictions. The processed\n        results should be stored in ``self.results``, which will be used to\n        compute the metrics when all batches have been processed.\n\n        Args:\n            data_batch (Sequence): A batch of data from the dataloader.\n            data_samples (Sequence): A batch of data samples that\n                contain annotations and predictions.\n        \"\"\"\n\n        result = dict()\n\n        input_prompts, input_videos = data_samples\n        bsz = len(input_prompts)\n\n        # Ensure prompt_input is a tensor\n        if isinstance(input_prompts, tuple):\n            input_prompts = list(input_prompts)\n\n        if isinstance(input_videos, tuple):\n            input_videos = list(input_videos)\n\n        # Initialize an empty list to store each similarity score\n        clip_score_sum, clip_score_cnt = 0, 0\n        logit_scale = self.model.logit_scale.exp() if self.logit_scale else 1\n        with torch.no_grad():\n            for input_prompt, input_frames in zip(input_prompts, input_videos):\n                input_prompt = input_prompt.to(self.device)\n                text_feature = self.model.get_text_features(input_prompt) # [bsz, hid_dim]\n                text_feature = text_feature / torch.norm(text_feature, dim=-1, keepdim=True)\n\n                input_frames = input_frames.to(self.device)  # Add batch dimension and move the frame to the device\n                frame_feature = self.model.get_image_features(input_frames)\n                frame_feature = frame_feature / torch.norm(frame_feature, dim=-1, keepdim=True)\n\n                clip_score = logit_scale * (frame_feature @ text_feature.T).mean().item()\n                print('current clip similarity score', clip_score)\n                clip_score_sum += clip_score\n                clip_score_cnt += 1\n\n        # Calculate the average CLIP score across all frames\n        clip_score_videos_avg = clip_score_sum/clip_score_cnt\n\n        result['clip_sim_score'] = clip_score_videos_avg\n\n        self.results.append(result)\n\n\n    def compute_metrics(self, results: list) -&gt; Dict[str, float]:\n        \"\"\"Compute the metrics from processed results.\n\n        Args:\n            results (list): The processed results of each batch.\n\n        Returns:\n            Dict[str, float]: The computed metrics. The keys are the names of\n            the metrics, and the values are corresponding results.\n        \"\"\"\n        logger: MMLogger = MMLogger.get_current_instance()\n\n        clip_score_np = np.zeros(len(results))\n        for i, result in enumerate(results):\n            clip_score_np[i] = result['clip_sim_score']\n\n        clip_sim_mean = np.mean(clip_score_np) \n\n        print(\"Test results: clip similarity score={:.4f}\"\n              .format(clip_sim_mean))\n\n        return result\n</code></pre>"},{"location":"documentations/aigve/#aigve.CLIPSimScore.compute_metrics","title":"<code>compute_metrics(results)</code>","text":"<p>Compute the metrics from processed results.</p> <p>Parameters:</p> Name Type Description Default <code>results</code> <code>list</code> <p>The processed results of each batch.</p> required <p>Returns:</p> Type Description <code>Dict[str, float]</code> <p>Dict[str, float]: The computed metrics. The keys are the names of</p> <code>Dict[str, float]</code> <p>the metrics, and the values are corresponding results.</p> Source code in <code>aigve/metrics/text_video_alignment/similarity_based/clipscore/clipsim.py</code> <pre><code>def compute_metrics(self, results: list) -&gt; Dict[str, float]:\n    \"\"\"Compute the metrics from processed results.\n\n    Args:\n        results (list): The processed results of each batch.\n\n    Returns:\n        Dict[str, float]: The computed metrics. The keys are the names of\n        the metrics, and the values are corresponding results.\n    \"\"\"\n    logger: MMLogger = MMLogger.get_current_instance()\n\n    clip_score_np = np.zeros(len(results))\n    for i, result in enumerate(results):\n        clip_score_np[i] = result['clip_sim_score']\n\n    clip_sim_mean = np.mean(clip_score_np) \n\n    print(\"Test results: clip similarity score={:.4f}\"\n          .format(clip_sim_mean))\n\n    return result\n</code></pre>"},{"location":"documentations/aigve/#aigve.CLIPSimScore.process","title":"<code>process(data_batch, data_samples)</code>","text":"<p>CLIPSimScore process Process one batch of data samples and predictions. The processed results should be stored in <code>self.results</code>, which will be used to compute the metrics when all batches have been processed.</p> <p>Parameters:</p> Name Type Description Default <code>data_batch</code> <code>Sequence</code> <p>A batch of data from the dataloader.</p> required <code>data_samples</code> <code>Sequence</code> <p>A batch of data samples that contain annotations and predictions.</p> required Source code in <code>aigve/metrics/text_video_alignment/similarity_based/clipscore/clipsim.py</code> <pre><code>def process(self, data_batch: Sequence, data_samples: Sequence) -&gt; None:\n    \"\"\"CLIPSimScore process\n    Process one batch of data samples and predictions. The processed\n    results should be stored in ``self.results``, which will be used to\n    compute the metrics when all batches have been processed.\n\n    Args:\n        data_batch (Sequence): A batch of data from the dataloader.\n        data_samples (Sequence): A batch of data samples that\n            contain annotations and predictions.\n    \"\"\"\n\n    result = dict()\n\n    input_prompts, input_videos = data_samples\n    bsz = len(input_prompts)\n\n    # Ensure prompt_input is a tensor\n    if isinstance(input_prompts, tuple):\n        input_prompts = list(input_prompts)\n\n    if isinstance(input_videos, tuple):\n        input_videos = list(input_videos)\n\n    # Initialize an empty list to store each similarity score\n    clip_score_sum, clip_score_cnt = 0, 0\n    logit_scale = self.model.logit_scale.exp() if self.logit_scale else 1\n    with torch.no_grad():\n        for input_prompt, input_frames in zip(input_prompts, input_videos):\n            input_prompt = input_prompt.to(self.device)\n            text_feature = self.model.get_text_features(input_prompt) # [bsz, hid_dim]\n            text_feature = text_feature / torch.norm(text_feature, dim=-1, keepdim=True)\n\n            input_frames = input_frames.to(self.device)  # Add batch dimension and move the frame to the device\n            frame_feature = self.model.get_image_features(input_frames)\n            frame_feature = frame_feature / torch.norm(frame_feature, dim=-1, keepdim=True)\n\n            clip_score = logit_scale * (frame_feature @ text_feature.T).mean().item()\n            print('current clip similarity score', clip_score)\n            clip_score_sum += clip_score\n            clip_score_cnt += 1\n\n    # Calculate the average CLIP score across all frames\n    clip_score_videos_avg = clip_score_sum/clip_score_cnt\n\n    result['clip_sim_score'] = clip_score_videos_avg\n\n    self.results.append(result)\n</code></pre>"},{"location":"documentations/aigve/#aigve.CLIPTempDataset","title":"<code>CLIPTempDataset</code>","text":"<p>               Bases: <code>Dataset</code></p> Source code in <code>aigve/datasets/cliptemp_dataset.py</code> <pre><code>@DATASETS.register_module()\nclass CLIPTempDataset(Dataset):\n    def __init__(self, processor_name, prompt_dir, video_dir):\n        super(CLIPTempDataset, self).__init__()\n        self.prompt_dir = prompt_dir\n        self.video_dir = video_dir\n        self.processor_name = processor_name\n\n        self.processor = AutoProcessor.from_pretrained(self.processor_name)\n        self.video_names = self._read_videoname()\n\n    def _read_videoname(self):\n        with open(self.prompt_dir, 'r') as reader:\n            read_data = json.load(reader)\n\n        video_name_list = []\n        for item in read_data[\"datset_list\"]:\n            video_name = item['video_path_pd'].strip()\n            video_name_list.append(video_name)\n\n        return video_name_list\n\n    def __len__(self):\n        return len(self.video_names)-1\n\n    def __getitem__(self, index):\n        '''return video frame pairs\n        '''\n        video_name = self.video_names[index]\n        video_path = self.video_dir + video_name\n        frames = []\n        cap = cv2.VideoCapture(video_path)\n        while cap.isOpened():\n            ret, frame = cap.read()\n            if not ret:\n                break\n            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n            resized_frame = cv2.resize(frame,(224,224))  # Resize the frame to match the expected input size\n            frames.append(resized_frame)\n\n        input_frame_tensor = self.processor(\n            images=frames,\n            padding=True,\n            truncation=True,\n            max_length=77,\n            return_tensors=\"pt\",\n        )['pixel_values']\n\n        return input_frame_tensor\n</code></pre>"},{"location":"documentations/aigve/#aigve.CLIPTempDataset.__getitem__","title":"<code>__getitem__(index)</code>","text":"<p>return video frame pairs</p> Source code in <code>aigve/datasets/cliptemp_dataset.py</code> <pre><code>def __getitem__(self, index):\n    '''return video frame pairs\n    '''\n    video_name = self.video_names[index]\n    video_path = self.video_dir + video_name\n    frames = []\n    cap = cv2.VideoCapture(video_path)\n    while cap.isOpened():\n        ret, frame = cap.read()\n        if not ret:\n            break\n        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n        resized_frame = cv2.resize(frame,(224,224))  # Resize the frame to match the expected input size\n        frames.append(resized_frame)\n\n    input_frame_tensor = self.processor(\n        images=frames,\n        padding=True,\n        truncation=True,\n        max_length=77,\n        return_tensors=\"pt\",\n    )['pixel_values']\n\n    return input_frame_tensor\n</code></pre>"},{"location":"documentations/aigve/#aigve.CLIPTempScore","title":"<code>CLIPTempScore</code>","text":"<p>               Bases: <code>BaseMetric</code></p> <p>Initialize the <code>CLIPTempScore</code> evaluator.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>The name of the CLIP encoder model. Defaults to <code>openai/clip-vit-base-patch32</code>.</p> <code>'openai/clip-vit-base-patch32'</code> <code>logit_scale</code> <code>bool</code> <p>Whether to calcualte the cosine similarity as logits. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <p>None</p> Source code in <code>aigve/metrics/text_video_alignment/similarity_based/clipscore/cliptemp.py</code> <pre><code>@METRICS.register_module()\nclass CLIPTempScore(BaseMetric):\n    \"\"\" Initialize the ``CLIPTempScore`` evaluator.\n\n    Args:\n            model_name (str): The name of the CLIP encoder model. Defaults to ``openai/clip-vit-base-patch32``.\n            logit_scale (bool): Whether to calcualte the cosine similarity as logits. Defaults to False.\n\n    Returns:\n            None\n    \"\"\"\n    def __init__(self,\n                 model_name: str = \"openai/clip-vit-base-patch32\",\n                 logit_scale: bool = False,\n                #  train_index: int = 4\n                 ) -&gt; None:\n        super().__init__()\n        self.model_name = model_name\n        self.logit_scale = logit_scale\n\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self.model = CLIPModel.from_pretrained(self.model_name).to(self.device)\n        self.model.eval()\n\n\n# def process(self, data_batch: dict, data_samples: Sequence[dict]) -&gt; None:\n    def process(self, data_batch: Sequence, data_samples: Sequence) -&gt; None:\n        \"\"\"CLIPTempScore process\n        Process one batch of data samples and predictions. The processed\n        results should be stored in ``self.results``, which will be used to\n        compute the metrics when all batches have been processed.\n\n        Args:\n            data_batch (Sequence): A batch of data from the dataloader.\n            data_samples (Sequence): A batch of data samples that\n                contain annotations and predictions.\n        \"\"\"\n\n        result = dict()\n\n        input_videos = data_samples\n        # bsz = len(input_videos)\n\n\n        # Ensure prompt_input is a tensor        \n        if isinstance(input_videos, tuple):\n            input_videos = list(input_videos)\n\n        # Generate embeddings for each frame and concatenate the features\n        clip_temp_score_sum, clip_temp_score_cnt = 0, 0\n        logit_scale = self.model.logit_scale.exp() if self.logit_scale else 1\n        with torch.no_grad():  \n            for input_frames in input_videos: # Too many frames in a video, must split before CLIP embedding, limited by the memory\n                input_frames = input_frames.to(self.device)\n                frame_feature = self.model.get_image_features(input_frames)\n                frame_feature = frame_feature / torch.norm(frame_feature, dim=-1, keepdim=True)\n                # print(frame_feature.shape)\n\n                clip_temp_score_list = []\n                for i in range(frame_feature.shape[0]-1):\n                    clip_temp_score = logit_scale * frame_feature[i].unsqueeze(0) @ frame_feature[i+1].unsqueeze(0).T\n                    clip_temp_score = clip_temp_score.item()\n                    # print(clip_temp_score)\n                    clip_temp_score_list.append(clip_temp_score)\n                clip_temp_cur_avg_score = sum(clip_temp_score_list)/len(clip_temp_score_list)\n                clip_temp_score_sum += clip_temp_cur_avg_score\n                clip_temp_score_cnt += 1\n                print('current clip temp similarity score', clip_temp_cur_avg_score)\n\n        clip_temp_score_avg = clip_temp_score_sum/clip_temp_score_cnt\n\n        result['clip_temp_score'] = clip_temp_score_avg\n\n        self.results.append(result)\n\n\n    def compute_metrics(self, results: list) -&gt; Dict[str, float]:\n        \"\"\"Compute the metrics from processed results.\n\n        Args:\n            results (list): The processed results of each batch.\n\n        Returns:\n            Dict[str, float]: The computed metrics. The keys are the names of\n            the metrics, and the values are corresponding results.\n        \"\"\"\n        logger: MMLogger = MMLogger.get_current_instance()\n\n        clip_score_np = np.zeros(len(results))\n        for i, result in enumerate(results):\n            clip_score_np[i] = result['clip_temp_score']\n\n        clip_temp_mean = np.mean(clip_score_np) \n\n        print(\"Test results: clip temporal consistency score={:.4f}\"\n              .format(clip_temp_mean))\n\n        return result\n</code></pre>"},{"location":"documentations/aigve/#aigve.CLIPTempScore.compute_metrics","title":"<code>compute_metrics(results)</code>","text":"<p>Compute the metrics from processed results.</p> <p>Parameters:</p> Name Type Description Default <code>results</code> <code>list</code> <p>The processed results of each batch.</p> required <p>Returns:</p> Type Description <code>Dict[str, float]</code> <p>Dict[str, float]: The computed metrics. The keys are the names of</p> <code>Dict[str, float]</code> <p>the metrics, and the values are corresponding results.</p> Source code in <code>aigve/metrics/text_video_alignment/similarity_based/clipscore/cliptemp.py</code> <pre><code>def compute_metrics(self, results: list) -&gt; Dict[str, float]:\n    \"\"\"Compute the metrics from processed results.\n\n    Args:\n        results (list): The processed results of each batch.\n\n    Returns:\n        Dict[str, float]: The computed metrics. The keys are the names of\n        the metrics, and the values are corresponding results.\n    \"\"\"\n    logger: MMLogger = MMLogger.get_current_instance()\n\n    clip_score_np = np.zeros(len(results))\n    for i, result in enumerate(results):\n        clip_score_np[i] = result['clip_temp_score']\n\n    clip_temp_mean = np.mean(clip_score_np) \n\n    print(\"Test results: clip temporal consistency score={:.4f}\"\n          .format(clip_temp_mean))\n\n    return result\n</code></pre>"},{"location":"documentations/aigve/#aigve.CLIPTempScore.process","title":"<code>process(data_batch, data_samples)</code>","text":"<p>CLIPTempScore process Process one batch of data samples and predictions. The processed results should be stored in <code>self.results</code>, which will be used to compute the metrics when all batches have been processed.</p> <p>Parameters:</p> Name Type Description Default <code>data_batch</code> <code>Sequence</code> <p>A batch of data from the dataloader.</p> required <code>data_samples</code> <code>Sequence</code> <p>A batch of data samples that contain annotations and predictions.</p> required Source code in <code>aigve/metrics/text_video_alignment/similarity_based/clipscore/cliptemp.py</code> <pre><code>def process(self, data_batch: Sequence, data_samples: Sequence) -&gt; None:\n    \"\"\"CLIPTempScore process\n    Process one batch of data samples and predictions. The processed\n    results should be stored in ``self.results``, which will be used to\n    compute the metrics when all batches have been processed.\n\n    Args:\n        data_batch (Sequence): A batch of data from the dataloader.\n        data_samples (Sequence): A batch of data samples that\n            contain annotations and predictions.\n    \"\"\"\n\n    result = dict()\n\n    input_videos = data_samples\n    # bsz = len(input_videos)\n\n\n    # Ensure prompt_input is a tensor        \n    if isinstance(input_videos, tuple):\n        input_videos = list(input_videos)\n\n    # Generate embeddings for each frame and concatenate the features\n    clip_temp_score_sum, clip_temp_score_cnt = 0, 0\n    logit_scale = self.model.logit_scale.exp() if self.logit_scale else 1\n    with torch.no_grad():  \n        for input_frames in input_videos: # Too many frames in a video, must split before CLIP embedding, limited by the memory\n            input_frames = input_frames.to(self.device)\n            frame_feature = self.model.get_image_features(input_frames)\n            frame_feature = frame_feature / torch.norm(frame_feature, dim=-1, keepdim=True)\n            # print(frame_feature.shape)\n\n            clip_temp_score_list = []\n            for i in range(frame_feature.shape[0]-1):\n                clip_temp_score = logit_scale * frame_feature[i].unsqueeze(0) @ frame_feature[i+1].unsqueeze(0).T\n                clip_temp_score = clip_temp_score.item()\n                # print(clip_temp_score)\n                clip_temp_score_list.append(clip_temp_score)\n            clip_temp_cur_avg_score = sum(clip_temp_score_list)/len(clip_temp_score_list)\n            clip_temp_score_sum += clip_temp_cur_avg_score\n            clip_temp_score_cnt += 1\n            print('current clip temp similarity score', clip_temp_cur_avg_score)\n\n    clip_temp_score_avg = clip_temp_score_sum/clip_temp_score_cnt\n\n    result['clip_temp_score'] = clip_temp_score_avg\n\n    self.results.append(result)\n</code></pre>"},{"location":"documentations/aigve/#aigve.DSGScore","title":"<code>DSGScore</code>","text":"<p>               Bases: <code>BaseMetric</code></p> <p>Initialize the <code>DSGScore</code> evaluator.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>The name of the VQA model used in the DSGScore evaluator. Defaults to <code>InstructBLIP</code>, you can also choose the \"MPLUG\" as the VQA model.</p> required <code>verbose</code> <code>bool</code> <p>Whether the intermediate output processes is required. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <p>None</p> Source code in <code>aigve/metrics/text_video_alignment/gpt_based/dsg/dsg_eval.py</code> <pre><code>@METRICS.register_module()\nclass DSGScore(BaseMetric):\n    \"\"\" Initialize the ``DSGScore`` evaluator.\n\n    Args:\n            model_name (str): The name of the VQA model used in the DSGScore evaluator. Defaults to ``InstructBLIP``, you can also choose the \"MPLUG\" as the VQA model.\n            verbose (bool): Whether the intermediate output processes is required. Defaults to False.\n\n    Returns:\n            None\n    \"\"\"\n    def __init__(self, \n                 vqa_model_name: str = \"InstructBLIP\",\n                 verbose: bool = False):\n        super().__init__()\n\n        self.submodel_path = 'metrics/text_video_alignment/gpt_based/dsg'\n        if not submodule_exists(self.submodel_path):\n            add_git_submodule(\n                repo_url='https://github.com/j-min/DSG.git', \n                submodule_path=self.submodel_path\n            )     \n        from .DSG.dsg.vqa_utils import MPLUG, InstructBLIP\n\n        self.vqa_model_name = vqa_model_name\n        assert self.vqa_model_name in [\"InstructBLIP\", \"MPLUG\"]\n        if self.vqa_model_name == 'InstructBLIP':\n            self.vqa_model = InstructBLIP()\n        else:\n            self.vqa_model = MPLUG()\n\n        self.verbose = verbose\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\n    def evaluate_image_dsg(self, qid_list, frame_index, frame):\n        \"\"\" Evaluate a generated image with DSG evaluator; this is the intermediate process of the ``process`` function. \n\n        Args:\n                qid_list (List[str]): The list of DSG parse question generation results.\n                frame_index (int): The index number of the currently evaluated frame.\n                frame (List[List[float]): The current evaluated frame.\n\n        Returns:\n                None\n        \"\"\"\n        if self.verbose:\n            print(\"#\"*50)\n            print(\"2) Answer questions given the generated image, with VQA\")\n            print(\"#\"*50)\n\n        # 2) answer questions with the generated image\n        qid2answer = {}\n        qid2scores = {}\n\n        qid2tuple, qid2dependency, qid2question = qid_list\n        for id, question in qid2question.items():\n            answer = self.vqa_model.vqa(image=frame, question=question)\n            print(answer)\n            qid2answer[id] = answer\n            qid2scores[id] = float('yes' in answer)\n\n        average_score_without_dep = sum(qid2scores.values()) / len(qid2scores)\n        print(average_score_without_dep, qid2answer, qid2scores)\n\n        if self.verbose:\n            print(\"#\"*50)\n            print(\"3) Zero-out scores from invalid questions\")\n            print(\"#\"*50)\n\n        # 3) zero-out scores from invalid questions \n        qid2validity = {}\n        qid2scores_after_filtering = deepcopy(qid2scores)\n\n        # print('qid2scores', qid2scores)\n        # print('qid2dependency', qid2dependency)\n        for id, parent_ids in qid2dependency.items():\n            # zero-out scores if parent questions are answered 'no'\n            any_parent_answered_no = False\n            for parent_id in parent_ids:\n                parent_id = list(parent_id)[0]\n                if parent_id == 0:\n                    continue\n                if qid2scores[parent_id] == 0:\n                    any_parent_answered_no = True\n                    break\n            if any_parent_answered_no:\n                qid2scores_after_filtering[id] = 0.0\n                qid2validity[id] = False\n            else:\n                qid2validity[id] = True\n\n        if self.verbose:\n            print(\"Per-quesiton eval results (after using dependency)\")\n            for id in qid2question:\n                print(\"ID\", id)\n                print(\"question\", qid2question[id])\n                print(\"answer\", qid2answer[id])\n                print(\"validity\", qid2validity[id])\n                print(\"score (before filtering)\", qid2scores[id])\n                print(\"score (after filtering)\", qid2scores_after_filtering[id])\n                print()\n\n        if self.verbose:\n            print(\"#\"*50)\n            print(\"4) Calculate the final score by averaging\")\n            print(\"#\"*50)\n\n        average_score_with_dep = sum(qid2scores_after_filtering.values()) / len(qid2scores)\n\n        return {\n            'frame_index': frame_index,\n            'qid2tuple': qid2tuple,\n            'qid2dependency': qid2dependency,\n            'qid2question': qid2question,\n            'qid2answer': qid2answer,\n            'qid2scores': qid2scores,\n            'qid2validity': qid2validity,\n            'average_score_with_dependency': average_score_with_dep,\n            'average_score_without_dependency': average_score_without_dep\n        }\n\n\n    def process(self, data_batch: Sequence, data_samples: Sequence) -&gt; None:\n        \"\"\"DSGScore process\n\n        Process one batch of data samples and predictions. The processed\n        results should be stored in ``self.results``, which will be used to\n        compute the metrics when all batches have been processed.\n\n        Args:\n            data_batch (Sequence): A batch of data from the dataloader.\n            data_samples (Sequence): A batch of data samples that\n                contain annotations and predictions.\n        \"\"\"\n\n        result = dict()\n\n        input_qid_lists, input_videos = data_samples\n        bsz = len(input_qid_lists)\n        # print('input_qid_lists: ', input_qid_lists)\n\n        # Ensure prompt_input is a tensor\n        if isinstance(input_qid_lists, tuple):\n            input_qid_lists = list(input_qid_lists)\n\n        if isinstance(input_videos, tuple):\n            input_videos = list(input_videos)\n\n        average_dep_score_list, average_wo_dep_score_list = [], []\n        for input_qid_list, input_video in zip([input_qid_lists], input_videos):\n            evaluate_dict_list = []\n            dep_score, wo_dep_score = [], []\n            for index, frame in enumerate(input_video):\n                # print('input_qid_list: ', input_qid_list)\n                evaluate_dict = self.evaluate_image_dsg(qid_list=input_qid_list, \n                                                        frame_index=index, \n                                                        frame=frame)\n                evaluate_dict_list.append(evaluate_dict)\n                frame_average_score_with_dependency = evaluate_dict['average_score_with_dependency']\n                dep_score.append(frame_average_score_with_dependency)\n                frame_average_score_without_dependency = evaluate_dict['average_score_without_dependency']\n                wo_dep_score.append(frame_average_score_without_dependency)\n            avg_dep_score, avg_wo_dep_score = sum(dep_score)/len(dep_score), sum(wo_dep_score)/len(dep_score)\n            average_dep_score_list.append(avg_dep_score)\n            average_wo_dep_score_list.append(avg_wo_dep_score)\n\n\n        result['average_dep_dgs_score'] = sum(average_dep_score_list)/len(average_dep_score_list)\n        result['average_wo_dep_dgs_score'] = sum(average_wo_dep_score_list)/len(average_wo_dep_score_list)\n\n        self.results.append(result)\n\n\n    def compute_metrics(self, results: list) -&gt; Dict[str, float]:\n        \"\"\"Compute the metrics from processed results.\n\n        Args:\n            results (list): The processed results of each batch.\n\n        Returns:\n            Dict[str, float]: The computed metrics. The keys are the names of\n            the metrics, and the values are corresponding results.\n        \"\"\"\n        logger: MMLogger = MMLogger.get_current_instance()\n\n        dep_dsg_score_np = np.zeros(len(results))\n        wo_dep_dsg_score_np = np.zeros(len(results))\n        for i, result in enumerate(results):\n            dep_dsg_score_np[i] = result['average_dep_dgs_score']\n            wo_dep_dsg_score_np[i] = result['average_wo_dep_dgs_score']\n\n        dep_dsg_score_np_mean = np.mean(dep_dsg_score_np) \n        wo_dep_dsg_score_np_mean = np.mean(wo_dep_dsg_score_np)\n\n        print(\"Test results: dsg score with dependency={:.4f}\"\n              .format(dep_dsg_score_np_mean))\n        print(\"Test results: dsg score without dependency={:.4f}\"\n              .format(wo_dep_dsg_score_np_mean))\n\n        return result\n</code></pre>"},{"location":"documentations/aigve/#aigve.DSGScore.compute_metrics","title":"<code>compute_metrics(results)</code>","text":"<p>Compute the metrics from processed results.</p> <p>Parameters:</p> Name Type Description Default <code>results</code> <code>list</code> <p>The processed results of each batch.</p> required <p>Returns:</p> Type Description <code>Dict[str, float]</code> <p>Dict[str, float]: The computed metrics. The keys are the names of</p> <code>Dict[str, float]</code> <p>the metrics, and the values are corresponding results.</p> Source code in <code>aigve/metrics/text_video_alignment/gpt_based/dsg/dsg_eval.py</code> <pre><code>def compute_metrics(self, results: list) -&gt; Dict[str, float]:\n    \"\"\"Compute the metrics from processed results.\n\n    Args:\n        results (list): The processed results of each batch.\n\n    Returns:\n        Dict[str, float]: The computed metrics. The keys are the names of\n        the metrics, and the values are corresponding results.\n    \"\"\"\n    logger: MMLogger = MMLogger.get_current_instance()\n\n    dep_dsg_score_np = np.zeros(len(results))\n    wo_dep_dsg_score_np = np.zeros(len(results))\n    for i, result in enumerate(results):\n        dep_dsg_score_np[i] = result['average_dep_dgs_score']\n        wo_dep_dsg_score_np[i] = result['average_wo_dep_dgs_score']\n\n    dep_dsg_score_np_mean = np.mean(dep_dsg_score_np) \n    wo_dep_dsg_score_np_mean = np.mean(wo_dep_dsg_score_np)\n\n    print(\"Test results: dsg score with dependency={:.4f}\"\n          .format(dep_dsg_score_np_mean))\n    print(\"Test results: dsg score without dependency={:.4f}\"\n          .format(wo_dep_dsg_score_np_mean))\n\n    return result\n</code></pre>"},{"location":"documentations/aigve/#aigve.DSGScore.evaluate_image_dsg","title":"<code>evaluate_image_dsg(qid_list, frame_index, frame)</code>","text":"<p>Evaluate a generated image with DSG evaluator; this is the intermediate process of the <code>process</code> function. </p> <p>Parameters:</p> Name Type Description Default <code>qid_list</code> <code>List[str]</code> <p>The list of DSG parse question generation results.</p> required <code>frame_index</code> <code>int</code> <p>The index number of the currently evaluated frame.</p> required <code>frame</code> <code>List[List[float]</code> <p>The current evaluated frame.</p> required <p>Returns:</p> Type Description <p>None</p> Source code in <code>aigve/metrics/text_video_alignment/gpt_based/dsg/dsg_eval.py</code> <pre><code>def evaluate_image_dsg(self, qid_list, frame_index, frame):\n    \"\"\" Evaluate a generated image with DSG evaluator; this is the intermediate process of the ``process`` function. \n\n    Args:\n            qid_list (List[str]): The list of DSG parse question generation results.\n            frame_index (int): The index number of the currently evaluated frame.\n            frame (List[List[float]): The current evaluated frame.\n\n    Returns:\n            None\n    \"\"\"\n    if self.verbose:\n        print(\"#\"*50)\n        print(\"2) Answer questions given the generated image, with VQA\")\n        print(\"#\"*50)\n\n    # 2) answer questions with the generated image\n    qid2answer = {}\n    qid2scores = {}\n\n    qid2tuple, qid2dependency, qid2question = qid_list\n    for id, question in qid2question.items():\n        answer = self.vqa_model.vqa(image=frame, question=question)\n        print(answer)\n        qid2answer[id] = answer\n        qid2scores[id] = float('yes' in answer)\n\n    average_score_without_dep = sum(qid2scores.values()) / len(qid2scores)\n    print(average_score_without_dep, qid2answer, qid2scores)\n\n    if self.verbose:\n        print(\"#\"*50)\n        print(\"3) Zero-out scores from invalid questions\")\n        print(\"#\"*50)\n\n    # 3) zero-out scores from invalid questions \n    qid2validity = {}\n    qid2scores_after_filtering = deepcopy(qid2scores)\n\n    # print('qid2scores', qid2scores)\n    # print('qid2dependency', qid2dependency)\n    for id, parent_ids in qid2dependency.items():\n        # zero-out scores if parent questions are answered 'no'\n        any_parent_answered_no = False\n        for parent_id in parent_ids:\n            parent_id = list(parent_id)[0]\n            if parent_id == 0:\n                continue\n            if qid2scores[parent_id] == 0:\n                any_parent_answered_no = True\n                break\n        if any_parent_answered_no:\n            qid2scores_after_filtering[id] = 0.0\n            qid2validity[id] = False\n        else:\n            qid2validity[id] = True\n\n    if self.verbose:\n        print(\"Per-quesiton eval results (after using dependency)\")\n        for id in qid2question:\n            print(\"ID\", id)\n            print(\"question\", qid2question[id])\n            print(\"answer\", qid2answer[id])\n            print(\"validity\", qid2validity[id])\n            print(\"score (before filtering)\", qid2scores[id])\n            print(\"score (after filtering)\", qid2scores_after_filtering[id])\n            print()\n\n    if self.verbose:\n        print(\"#\"*50)\n        print(\"4) Calculate the final score by averaging\")\n        print(\"#\"*50)\n\n    average_score_with_dep = sum(qid2scores_after_filtering.values()) / len(qid2scores)\n\n    return {\n        'frame_index': frame_index,\n        'qid2tuple': qid2tuple,\n        'qid2dependency': qid2dependency,\n        'qid2question': qid2question,\n        'qid2answer': qid2answer,\n        'qid2scores': qid2scores,\n        'qid2validity': qid2validity,\n        'average_score_with_dependency': average_score_with_dep,\n        'average_score_without_dependency': average_score_without_dep\n    }\n</code></pre>"},{"location":"documentations/aigve/#aigve.DSGScore.process","title":"<code>process(data_batch, data_samples)</code>","text":"<p>DSGScore process</p> <p>Process one batch of data samples and predictions. The processed results should be stored in <code>self.results</code>, which will be used to compute the metrics when all batches have been processed.</p> <p>Parameters:</p> Name Type Description Default <code>data_batch</code> <code>Sequence</code> <p>A batch of data from the dataloader.</p> required <code>data_samples</code> <code>Sequence</code> <p>A batch of data samples that contain annotations and predictions.</p> required Source code in <code>aigve/metrics/text_video_alignment/gpt_based/dsg/dsg_eval.py</code> <pre><code>def process(self, data_batch: Sequence, data_samples: Sequence) -&gt; None:\n    \"\"\"DSGScore process\n\n    Process one batch of data samples and predictions. The processed\n    results should be stored in ``self.results``, which will be used to\n    compute the metrics when all batches have been processed.\n\n    Args:\n        data_batch (Sequence): A batch of data from the dataloader.\n        data_samples (Sequence): A batch of data samples that\n            contain annotations and predictions.\n    \"\"\"\n\n    result = dict()\n\n    input_qid_lists, input_videos = data_samples\n    bsz = len(input_qid_lists)\n    # print('input_qid_lists: ', input_qid_lists)\n\n    # Ensure prompt_input is a tensor\n    if isinstance(input_qid_lists, tuple):\n        input_qid_lists = list(input_qid_lists)\n\n    if isinstance(input_videos, tuple):\n        input_videos = list(input_videos)\n\n    average_dep_score_list, average_wo_dep_score_list = [], []\n    for input_qid_list, input_video in zip([input_qid_lists], input_videos):\n        evaluate_dict_list = []\n        dep_score, wo_dep_score = [], []\n        for index, frame in enumerate(input_video):\n            # print('input_qid_list: ', input_qid_list)\n            evaluate_dict = self.evaluate_image_dsg(qid_list=input_qid_list, \n                                                    frame_index=index, \n                                                    frame=frame)\n            evaluate_dict_list.append(evaluate_dict)\n            frame_average_score_with_dependency = evaluate_dict['average_score_with_dependency']\n            dep_score.append(frame_average_score_with_dependency)\n            frame_average_score_without_dependency = evaluate_dict['average_score_without_dependency']\n            wo_dep_score.append(frame_average_score_without_dependency)\n        avg_dep_score, avg_wo_dep_score = sum(dep_score)/len(dep_score), sum(wo_dep_score)/len(dep_score)\n        average_dep_score_list.append(avg_dep_score)\n        average_wo_dep_score_list.append(avg_wo_dep_score)\n\n\n    result['average_dep_dgs_score'] = sum(average_dep_score_list)/len(average_dep_score_list)\n    result['average_wo_dep_dgs_score'] = sum(average_wo_dep_score_list)/len(average_wo_dep_score_list)\n\n    self.results.append(result)\n</code></pre>"},{"location":"documentations/aigve/#aigve.GSTVQACrossData","title":"<code>GSTVQACrossData</code>","text":"<p>               Bases: <code>BaseMetric</code></p> <p>The GSTVQA evaluation metric. https://arxiv.org/pdf/2012.13936</p> <p>Parameters:</p> Name Type Description Default <code>collect_device</code> <code>str</code> <p>Device used for collecting results from workers. Options: 'cpu' and 'gpu'.</p> <code>'cpu'</code> <code>prefix</code> <code>str</code> <p>The prefix that will be added in the metric names to disambiguate homonymous metrics of different evaluators. Default: None.</p> <code>None</code> <code>metric_path</code> <code>str</code> <p>the file path of the metric </p> <code>''</code> <code>train_index</code> <code>(int, Optional)</code> <p>The specific model used. Details on: https://github.com/Baoliang93/GSTVQA/blob/main/TCSVT_Release/GVQA_Release/GVQA_Cross/cross_test.py#L162</p> required Source code in <code>aigve/metrics/video_quality_assessment/nn_based/gstvqa/gstvqa_crossdata_metric.py</code> <pre><code>@METRICS.register_module()\nclass GSTVQACrossData(BaseMetric):\n    \"\"\"The GSTVQA evaluation metric. https://arxiv.org/pdf/2012.13936\n\n    Args:\n        collect_device (str): Device used for collecting results from workers.\n            Options: 'cpu' and 'gpu'.\n        prefix (str, optional): The prefix that will be added in the metric\n            names to disambiguate homonymous metrics of different evaluators.\n            Default: None.\n        metric_path (str): the file path of the metric \n        train_index (int, Optional): The specific model used. Details on: https://github.com/Baoliang93/GSTVQA/blob/main/TCSVT_Release/GVQA_Release/GVQA_Cross/cross_test.py#L162\n    \"\"\"\n\n    default_prefix: Optional[str] = 'llm_score'\n\n    def __init__(self,\n                 collect_device: str = 'cpu',\n                 prefix: Optional[str] = None, \n                 metric_path: str = '',\n                 model_path: str = '',\n                 scale: int = 1,\n                 test_index: int = None,\n                #  train_index: int = 4\n                 ) -&gt; None:\n        super().__init__(collect_device=collect_device, prefix=prefix)\n        # self.train_index = train_index\n        self.metric_path = metric_path\n        self.model_path = model_path\n        self.scale = scale\n        self.test_index = test_index\n        if not submodule_exists(self.metric_path):\n            add_git_submodule(repo_url='https://github.com/Baoliang93/GSTVQA.git', submodule_path=self.metric_path)\n        from .GSTVQA.TCSVT_Release.GVQA_Release.GVQA_Cross.cross_test import GSTVQA as GSTVQA_model\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self.model = GSTVQA_model().to(self.device)\n        self.model.load_state_dict(torch.load(model_path))\n        self.model.eval()\n        self.criterion = nn.L1Loss().to(self.device)\n\n\n    # def process(self, data_batch: dict, data_samples: Sequence[dict]) -&gt; None:\n    def process(self, data_batch: Sequence, data_samples: Sequence) -&gt; None:\n        \"\"\"GSTVQA process\n        Process one batch of data samples and predictions. The processed\n        results should be stored in ``self.results``, which will be used to\n        compute the metrics when all batches have been processed.\n\n        Args:\n            data_batch (Sequence): A batch of data from the dataloader.\n            data_samples (Sequence): A batch of data samples that\n                contain annotations and predictions.\n        \"\"\"\n\n        result = dict()\n\n        features, length, label, mean_var,std_var,mean_mean,std_mean = data_samples\n        # # prompt_gt = data_sample['prompt_gt'] # str\n        # video_pd = data_sample['video_pd'] # torch.uint8(F, C, H, W)\n\n        result['y_test'] = self.scale * label.item()\n\n        features = features.to(self.device).float()\n        label = label.to(self.device).float()\n        mean_var = mean_var.to(self.device).float()\n        std_var = std_var.to(self.device).float()\n        mean_mean = mean_mean.to(self.device).float()\n        std_mean = std_mean.to(self.device).float()\n\n        outputs = self.model(features, length.float(),mean_var,std_var,mean_mean,std_mean)\n        result['y_pred'] = self.scale * outputs.item()\n        result['loss'] = self.criterion(outputs, label).item()\n\n        self.results.append(result)\n\n\n    def compute_metrics(self, results: list) -&gt; Dict[str, float]:\n        \"\"\"Compute the metrics from processed results.\n\n        Args:\n            results (list): The processed results of each batch.\n\n        Returns:\n            Dict[str, float]: The computed metrics. The keys are the names of\n            the metrics, and the values are corresponding results.\n        \"\"\"\n        logger: MMLogger = MMLogger.get_current_instance()\n\n        assert self.test_index == len(results)\n        test_loss = sum(result.get('loss', 0) for result in results) / len(results)\n        y_pred_np = np.zeros(len(self.test_index))\n        y_test_np = np.zeros(len(self.test_index))\n        for i, result in enumerate(results):\n            y_pred_np[i] = result['y_pred']\n            y_test_np[i] = result['y_test']\n\n        PLCC = stats.pearsonr(y_pred_np, y_test_np)[0]\n        SROCC = stats.spearmanr(y_pred_np, y_test_np)[0]\n        RMSE = np.sqrt(((y_pred_np-y_test_np) ** 2).mean())\n        KROCC = stats.stats.kendalltau(y_pred_np, y_test_np)[0]\n        print(\"Test results: test loss={:.4f}, SROCC={:.4f}, KROCC={:.4f}, PLCC={:.4f}, RMSE={:.4f}\"\n                .format(test_loss, SROCC, KROCC, PLCC, RMSE))\n</code></pre>"},{"location":"documentations/aigve/#aigve.GSTVQACrossData.compute_metrics","title":"<code>compute_metrics(results)</code>","text":"<p>Compute the metrics from processed results.</p> <p>Parameters:</p> Name Type Description Default <code>results</code> <code>list</code> <p>The processed results of each batch.</p> required <p>Returns:</p> Type Description <code>Dict[str, float]</code> <p>Dict[str, float]: The computed metrics. The keys are the names of</p> <code>Dict[str, float]</code> <p>the metrics, and the values are corresponding results.</p> Source code in <code>aigve/metrics/video_quality_assessment/nn_based/gstvqa/gstvqa_crossdata_metric.py</code> <pre><code>def compute_metrics(self, results: list) -&gt; Dict[str, float]:\n    \"\"\"Compute the metrics from processed results.\n\n    Args:\n        results (list): The processed results of each batch.\n\n    Returns:\n        Dict[str, float]: The computed metrics. The keys are the names of\n        the metrics, and the values are corresponding results.\n    \"\"\"\n    logger: MMLogger = MMLogger.get_current_instance()\n\n    assert self.test_index == len(results)\n    test_loss = sum(result.get('loss', 0) for result in results) / len(results)\n    y_pred_np = np.zeros(len(self.test_index))\n    y_test_np = np.zeros(len(self.test_index))\n    for i, result in enumerate(results):\n        y_pred_np[i] = result['y_pred']\n        y_test_np[i] = result['y_test']\n\n    PLCC = stats.pearsonr(y_pred_np, y_test_np)[0]\n    SROCC = stats.spearmanr(y_pred_np, y_test_np)[0]\n    RMSE = np.sqrt(((y_pred_np-y_test_np) ** 2).mean())\n    KROCC = stats.stats.kendalltau(y_pred_np, y_test_np)[0]\n    print(\"Test results: test loss={:.4f}, SROCC={:.4f}, KROCC={:.4f}, PLCC={:.4f}, RMSE={:.4f}\"\n            .format(test_loss, SROCC, KROCC, PLCC, RMSE))\n</code></pre>"},{"location":"documentations/aigve/#aigve.GSTVQACrossData.process","title":"<code>process(data_batch, data_samples)</code>","text":"<p>GSTVQA process Process one batch of data samples and predictions. The processed results should be stored in <code>self.results</code>, which will be used to compute the metrics when all batches have been processed.</p> <p>Parameters:</p> Name Type Description Default <code>data_batch</code> <code>Sequence</code> <p>A batch of data from the dataloader.</p> required <code>data_samples</code> <code>Sequence</code> <p>A batch of data samples that contain annotations and predictions.</p> required Source code in <code>aigve/metrics/video_quality_assessment/nn_based/gstvqa/gstvqa_crossdata_metric.py</code> <pre><code>def process(self, data_batch: Sequence, data_samples: Sequence) -&gt; None:\n    \"\"\"GSTVQA process\n    Process one batch of data samples and predictions. The processed\n    results should be stored in ``self.results``, which will be used to\n    compute the metrics when all batches have been processed.\n\n    Args:\n        data_batch (Sequence): A batch of data from the dataloader.\n        data_samples (Sequence): A batch of data samples that\n            contain annotations and predictions.\n    \"\"\"\n\n    result = dict()\n\n    features, length, label, mean_var,std_var,mean_mean,std_mean = data_samples\n    # # prompt_gt = data_sample['prompt_gt'] # str\n    # video_pd = data_sample['video_pd'] # torch.uint8(F, C, H, W)\n\n    result['y_test'] = self.scale * label.item()\n\n    features = features.to(self.device).float()\n    label = label.to(self.device).float()\n    mean_var = mean_var.to(self.device).float()\n    std_var = std_var.to(self.device).float()\n    mean_mean = mean_mean.to(self.device).float()\n    std_mean = std_mean.to(self.device).float()\n\n    outputs = self.model(features, length.float(),mean_var,std_var,mean_mean,std_mean)\n    result['y_pred'] = self.scale * outputs.item()\n    result['loss'] = self.criterion(outputs, label).item()\n\n    self.results.append(result)\n</code></pre>"},{"location":"documentations/aigve/#aigve.GSTVQADataset","title":"<code>GSTVQADataset</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>Dataset for GSTVQA metric, supports feature extraction using VGG16 or ResNet.</p> Source code in <code>aigve/datasets/gstvqa_dataset.py</code> <pre><code>@DATASETS.register_module()\nclass GSTVQADataset(Dataset):\n    \"\"\"Dataset for GSTVQA metric, supports feature extraction using VGG16 or ResNet.\"\"\"\n\n    def __init__(self, video_dir, prompt_dir, model_name='vgg16', max_len=500):\n        super(GSTVQADataset, self).__init__()\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self.video_dir = video_dir\n        self.prompt_dir = prompt_dir\n        self.model_name = model_name\n        self.max_len = max_len\n        self.feature_extractor = FeatureExtractor(model_name=model_name)\n\n        self.prompts, self.video_names = self._read_prompt_videoname()\n\n    def _read_prompt_videoname(self):\n        with open(self.prompt_dir, 'r') as reader:\n            read_data = json.load(reader)\n\n        prompt_data_list, video_name_list = [], []\n        for item in read_data[\"data_list\"]:\n            prompt = item['prompt_gt'].strip()\n            video_name = item['video_path_pd'].strip()\n            prompt_data_list.append(prompt)\n            video_name_list.append(video_name)\n\n        return prompt_data_list, video_name_list\n\n    def __len__(self):\n        return len(self.prompts)\n\n    def __getitem__(self, index):\n        \"\"\"\n        Returns a tuple of:\n            deep_features (torch.Tensor): Shape [max_len, 2944]\n                Mean and std features extracted from input frames using the chosen model (VGG16 or ResNet).\n                Padded to self.max_len if the number of frames is less.\n            num_frames (int): The number of frames in the video.\n            video_name (str): The file name for the video.\n        \"\"\"\n        video_name = self.video_names[index]\n        video_path = os.path.join(self.video_dir, video_name)\n        input_frames = []\n\n        cap = cv2.VideoCapture(video_path)\n        frame_count = 0\n\n        while cap.isOpened() and frame_count &lt; self.max_len:\n            ret, frame = cap.read()\n            if not ret:\n                break\n            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n            # frame = cv2.resize(frame, self.frame_size)\n            input_frames.append(torch.tensor(frame).float())\n            frame_count += 1\n\n        cap.release()\n\n        # Pad or truncate frames to max_len\n        num_frames = len(input_frames)\n        # print('num_frames: ', num_frames)\n        if num_frames &lt; 30:\n            pad_frames = torch.zeros((30 - num_frames, *input_frames[0].shape))\n            input_frames_tensor = torch.cat((torch.stack(input_frames), pad_frames), dim=0)\n            num_frames = 30 # Force min frames to be 30 (since two att_frams=15(kernel_size) used in GSTVQA)\n        elif num_frames &lt; self.max_len:\n            pad_frames = torch.zeros((self.max_len - num_frames, *input_frames[0].shape))\n            input_frames_tensor = torch.cat((torch.stack(input_frames), pad_frames), dim=0)\n        else:\n            input_frames_tensor = torch.stack(input_frames[:self.max_len])\n        # print('input_frames_tensor: ', input_frames_tensor.shape) # shape: toy data [max_len, H(512), W(512), C(3)]\n\n        # Convert from [T, H, W, C] to [T, C, H, W]\n        input_frames_tensor = input_frames_tensor.permute(0, 3, 1, 2) \n\n        # Extract features using the chosen model (VGG16 or ResNet)\n        with torch.no_grad():\n            mean_features, std_features = self.feature_extractor(input_frames_tensor) # Shape: [T, 1472]: [10, 1472]\n\n        # Concatenate to match GSTVQA expected 2944-dim features\n        deep_features = torch.cat((mean_features, std_features), dim=1)  # Shape: [T, 2944]\n\n        # Ensure output shape [max_len, 2944] (pad if needed)\n        if deep_features.shape[0] &lt; self.max_len:\n            pad_size = self.max_len - deep_features.shape[0]\n            padding = torch.zeros((pad_size, 2944), device=deep_features.device)\n            deep_features = torch.cat((deep_features, padding), dim=0)\n\n\n        return deep_features, num_frames, video_name\n</code></pre>"},{"location":"documentations/aigve/#aigve.GSTVQADataset.__getitem__","title":"<code>__getitem__(index)</code>","text":"Returns a tuple of <p>deep_features (torch.Tensor): Shape [max_len, 2944]     Mean and std features extracted from input frames using the chosen model (VGG16 or ResNet).     Padded to self.max_len if the number of frames is less. num_frames (int): The number of frames in the video. video_name (str): The file name for the video.</p> Source code in <code>aigve/datasets/gstvqa_dataset.py</code> <pre><code>def __getitem__(self, index):\n    \"\"\"\n    Returns a tuple of:\n        deep_features (torch.Tensor): Shape [max_len, 2944]\n            Mean and std features extracted from input frames using the chosen model (VGG16 or ResNet).\n            Padded to self.max_len if the number of frames is less.\n        num_frames (int): The number of frames in the video.\n        video_name (str): The file name for the video.\n    \"\"\"\n    video_name = self.video_names[index]\n    video_path = os.path.join(self.video_dir, video_name)\n    input_frames = []\n\n    cap = cv2.VideoCapture(video_path)\n    frame_count = 0\n\n    while cap.isOpened() and frame_count &lt; self.max_len:\n        ret, frame = cap.read()\n        if not ret:\n            break\n        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n        # frame = cv2.resize(frame, self.frame_size)\n        input_frames.append(torch.tensor(frame).float())\n        frame_count += 1\n\n    cap.release()\n\n    # Pad or truncate frames to max_len\n    num_frames = len(input_frames)\n    # print('num_frames: ', num_frames)\n    if num_frames &lt; 30:\n        pad_frames = torch.zeros((30 - num_frames, *input_frames[0].shape))\n        input_frames_tensor = torch.cat((torch.stack(input_frames), pad_frames), dim=0)\n        num_frames = 30 # Force min frames to be 30 (since two att_frams=15(kernel_size) used in GSTVQA)\n    elif num_frames &lt; self.max_len:\n        pad_frames = torch.zeros((self.max_len - num_frames, *input_frames[0].shape))\n        input_frames_tensor = torch.cat((torch.stack(input_frames), pad_frames), dim=0)\n    else:\n        input_frames_tensor = torch.stack(input_frames[:self.max_len])\n    # print('input_frames_tensor: ', input_frames_tensor.shape) # shape: toy data [max_len, H(512), W(512), C(3)]\n\n    # Convert from [T, H, W, C] to [T, C, H, W]\n    input_frames_tensor = input_frames_tensor.permute(0, 3, 1, 2) \n\n    # Extract features using the chosen model (VGG16 or ResNet)\n    with torch.no_grad():\n        mean_features, std_features = self.feature_extractor(input_frames_tensor) # Shape: [T, 1472]: [10, 1472]\n\n    # Concatenate to match GSTVQA expected 2944-dim features\n    deep_features = torch.cat((mean_features, std_features), dim=1)  # Shape: [T, 2944]\n\n    # Ensure output shape [max_len, 2944] (pad if needed)\n    if deep_features.shape[0] &lt; self.max_len:\n        pad_size = self.max_len - deep_features.shape[0]\n        padding = torch.zeros((pad_size, 2944), device=deep_features.device)\n        deep_features = torch.cat((deep_features, padding), dim=0)\n\n\n    return deep_features, num_frames, video_name\n</code></pre>"},{"location":"documentations/aigve/#aigve.GstVqa","title":"<code>GstVqa</code>","text":"<p>               Bases: <code>BaseMetric</code></p> <p>GstVQA metric modified for the toy dataset. (Supporting 2944-dim features).</p> Source code in <code>aigve/metrics/video_quality_assessment/nn_based/gstvqa/gstvqa_metric.py</code> <pre><code>@METRICS.register_module()\nclass GstVqa(BaseMetric):\n    \"\"\"GstVQA metric modified for the toy dataset. (Supporting 2944-dim features).\"\"\"\n\n    def __init__(self, model_path: str):\n        super(GstVqa, self).__init__()\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self.submodel_path = os.path.join(os.getcwd(), 'metrics/video_quality_assessment/nn_based/gstvqa')\n        if not submodule_exists(self.submodel_path):\n            add_git_submodule(\n                repo_url='https://github.com/Baoliang93/GSTVQA.git', \n                submodule_path=self.submodel_path\n            )\n        from .GSTVQA.TCSVT_Release.GVQA_Release.GVQA_Cross.cross_test import GSTVQA as GSTVQA_model\n        self.model = GSTVQA_model().to(self.device)\n        self.model.load_state_dict(torch.load(model_path, map_location=self.device))\n        self.model.eval()\n        # self.criterion = nn.L1Loss().to(self.device)\n\n    def compute_stat_features(self, features: torch.Tensor, num_valid_frames: int) -&gt; Tuple[torch.Tensor]:\n        \"\"\"Compute statistical features mean_var, std_var, mean_mean, std_mean from extracted deep features.\n\n        Args:\n            features (torch.Tensor): Tensor of shape [T, 2944].\n            num_valid_frames (int): Number of valid frames before padding.\n\n        Returns:\n            Tuple[torch.Tensor]: (mean_var, std_var, mean_mean, std_mean), each of shape [1472].\n        \"\"\"\n        # Ignore padded frames\n        features = features[:num_valid_frames]  # Shape: [num_valid_frames, feature_dim]: [10, 1472]\n\n        if num_valid_frames == 0:  # Edge case: all frames were padded\n            return (\n                torch.zeros(1472, device=self.device),\n                torch.zeros(1472, device=self.device),\n                torch.zeros(1472, device=self.device),\n                torch.zeros(1472, device=self.device),\n            )\n\n        # Split into mean and std components\n        mean_features = features[:, :1472]  # First 1472 features are mean-based\n        std_features = features[:, 1472:]   # Last 1472 features are std-based\n\n        # Compute per-feature statistics over frames\n        mean_mean = mean_features.mean(dim=0)  # Shape: [1472]\n        std_mean = std_features.mean(dim=0)    # Shape: [1472]\n        mean_var = mean_features.var(dim=0, unbiased=False)  # Shape: [1472]\n        std_var = std_features.var(dim=0, unbiased=False)    # Shape: [1472]\n\n        return mean_var, std_var, mean_mean, std_mean\n\n    def process(self, data_batch: Sequence, data_samples: Sequence) -&gt; None:\n        \"\"\"\n        Process a batch of extracted deep features for GSTVQA evaluation and store results in a JSON file.\n\n        Args:\n            data_batch (SequencTuplee): A batch of data from the dataloader (not used here).\n            data_samples (List[ [torch.Tensor], Tuple[int], Tuple[str] ]): \n                A list containing three tuples:\n                - A tuple of `deep_features`: Each item is a Tensor of shape [T, 2944]. \n                - A tuple of `num_frames`: Each item is an integer representing the number of valid frames.\n                - A tuple of `video_name`: Each item is a string representing the file name for the video.\n                The len of these three tuples are the batch size.\n        \"\"\"\n        # data_samples an example: [\n        #     (tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n        #              [0., 0., 0.,  ..., 0., 0., 0.],\n        #              ...\n        #              [0., 0., 0.,  ..., 0., 0., 0.]]), \n        #      tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n        #              [0., 0., 0.,  ..., 0., 0., 0.],\n        #              ...\n        #              [0., 0., 0.,  ..., 0., 0., 0.]])), \n        #     (10, 10)\n        # ]\n        results = []\n        deep_features_tuple, num_frames_tuple, video_name_tuple = data_samples\n        with torch.no_grad():\n            for deep_features, num_valid_frames, video_name in zip(deep_features_tuple, num_frames_tuple, video_name_tuple):\n                if not isinstance(deep_features, torch.Tensor) or not isinstance(num_valid_frames, int):\n                    raise TypeError(\"Expected deep_features to be a torch.Tensor and num_valid_frames to be an int.\")\n\n                if num_valid_frames == 0:  # Edge case: No valid frames\n                    results.append({\"video_name\": 'N/A', \"GSTVQA_Score\": 0.0})\n                    continue\n\n                # Remove padded features\n                features = deep_features[:num_valid_frames].to(self.device)\n\n                # Compute statistical features only on valid frames\n                mean_var, std_var, mean_mean, std_mean = self.compute_stat_features(features, num_valid_frames)\n                mean_var, std_var, mean_mean, std_mean = (\n                    mean_var.to(self.device),\n                    std_var.to(self.device),\n                    mean_mean.to(self.device),\n                    std_mean.to(self.device),\n                )\n\n                # Length tensor indicating the number of valid frames\n                length = torch.tensor([num_valid_frames]).to(self.device)\n                # print('features(input) shape', features.unsqueeze(1).shape) # torch.Size([10, 1, 1472])\n                # print('input_length shape', length.shape) # torch.Size([1])\n                # print('input_length', length) # torch.Size([1])\n                # print('mean_mean shape', mean_mean.shape) # torch.Size([1472])\n                # print('std_mean shape', std_mean.shape) # torch.Size([1472])\n                # print('mean_var shape', mean_var.shape) # torch.Size([1472])\n                # print('std_var shape', std_var.shape) # torch.Size([1472])\n\n                # Run GSTVQA model\n                outputs = self.model(features.unsqueeze(1), length, mean_var, std_var, mean_mean, std_mean)\n                score = outputs.item()\n                results.append({\"video_name\": video_name, \"GSTVQA_Score\": score})\n                # print(f\"Processed score {score:.4f} for {video_name}\")\n\n        self.results.extend(results)\n\n\n    def compute_metrics(self, results: list) -&gt; Dict[str, float]:\n        \"\"\"Compute final GSTVQA-based metrics.\"\"\"\n        scores = np.array([res['GSTVQA_Score'] for res in self.results])\n        mean_score = np.mean(scores)\n        print(f\"GSTVQA mean score: {mean_score:.4f}\")\n\n        json_file_path = os.path.join(os.getcwd(), \"gstvqa_results.json\")\n        final_results = {\"video_results\": self.results, \"GSTVQA_Mean_Score\": mean_score}\n        with open(json_file_path, \"w\") as json_file:\n            json.dump(final_results, json_file, indent=4)\n        print(f\"GSTVQA mean score saved to {json_file_path}\")\n\n        return {'GSTVQA_Mean_Score': mean_score}\n</code></pre>"},{"location":"documentations/aigve/#aigve.GstVqa.compute_metrics","title":"<code>compute_metrics(results)</code>","text":"<p>Compute final GSTVQA-based metrics.</p> Source code in <code>aigve/metrics/video_quality_assessment/nn_based/gstvqa/gstvqa_metric.py</code> <pre><code>def compute_metrics(self, results: list) -&gt; Dict[str, float]:\n    \"\"\"Compute final GSTVQA-based metrics.\"\"\"\n    scores = np.array([res['GSTVQA_Score'] for res in self.results])\n    mean_score = np.mean(scores)\n    print(f\"GSTVQA mean score: {mean_score:.4f}\")\n\n    json_file_path = os.path.join(os.getcwd(), \"gstvqa_results.json\")\n    final_results = {\"video_results\": self.results, \"GSTVQA_Mean_Score\": mean_score}\n    with open(json_file_path, \"w\") as json_file:\n        json.dump(final_results, json_file, indent=4)\n    print(f\"GSTVQA mean score saved to {json_file_path}\")\n\n    return {'GSTVQA_Mean_Score': mean_score}\n</code></pre>"},{"location":"documentations/aigve/#aigve.GstVqa.compute_stat_features","title":"<code>compute_stat_features(features, num_valid_frames)</code>","text":"<p>Compute statistical features mean_var, std_var, mean_mean, std_mean from extracted deep features.</p> <p>Parameters:</p> Name Type Description Default <code>features</code> <code>Tensor</code> <p>Tensor of shape [T, 2944].</p> required <code>num_valid_frames</code> <code>int</code> <p>Number of valid frames before padding.</p> required <p>Returns:</p> Type Description <code>Tuple[Tensor]</code> <p>Tuple[torch.Tensor]: (mean_var, std_var, mean_mean, std_mean), each of shape [1472].</p> Source code in <code>aigve/metrics/video_quality_assessment/nn_based/gstvqa/gstvqa_metric.py</code> <pre><code>def compute_stat_features(self, features: torch.Tensor, num_valid_frames: int) -&gt; Tuple[torch.Tensor]:\n    \"\"\"Compute statistical features mean_var, std_var, mean_mean, std_mean from extracted deep features.\n\n    Args:\n        features (torch.Tensor): Tensor of shape [T, 2944].\n        num_valid_frames (int): Number of valid frames before padding.\n\n    Returns:\n        Tuple[torch.Tensor]: (mean_var, std_var, mean_mean, std_mean), each of shape [1472].\n    \"\"\"\n    # Ignore padded frames\n    features = features[:num_valid_frames]  # Shape: [num_valid_frames, feature_dim]: [10, 1472]\n\n    if num_valid_frames == 0:  # Edge case: all frames were padded\n        return (\n            torch.zeros(1472, device=self.device),\n            torch.zeros(1472, device=self.device),\n            torch.zeros(1472, device=self.device),\n            torch.zeros(1472, device=self.device),\n        )\n\n    # Split into mean and std components\n    mean_features = features[:, :1472]  # First 1472 features are mean-based\n    std_features = features[:, 1472:]   # Last 1472 features are std-based\n\n    # Compute per-feature statistics over frames\n    mean_mean = mean_features.mean(dim=0)  # Shape: [1472]\n    std_mean = std_features.mean(dim=0)    # Shape: [1472]\n    mean_var = mean_features.var(dim=0, unbiased=False)  # Shape: [1472]\n    std_var = std_features.var(dim=0, unbiased=False)    # Shape: [1472]\n\n    return mean_var, std_var, mean_mean, std_mean\n</code></pre>"},{"location":"documentations/aigve/#aigve.GstVqa.process","title":"<code>process(data_batch, data_samples)</code>","text":"<p>Process a batch of extracted deep features for GSTVQA evaluation and store results in a JSON file.</p> <p>Parameters:</p> Name Type Description Default <code>data_batch</code> <code>SequencTuplee</code> <p>A batch of data from the dataloader (not used here).</p> required <code>data_samples</code> <code>List[[Tensor], Tuple[int], Tuple[str]]</code> <p>A list containing three tuples: - A tuple of <code>deep_features</code>: Each item is a Tensor of shape [T, 2944].  - A tuple of <code>num_frames</code>: Each item is an integer representing the number of valid frames. - A tuple of <code>video_name</code>: Each item is a string representing the file name for the video. The len of these three tuples are the batch size.</p> required Source code in <code>aigve/metrics/video_quality_assessment/nn_based/gstvqa/gstvqa_metric.py</code> <pre><code>def process(self, data_batch: Sequence, data_samples: Sequence) -&gt; None:\n    \"\"\"\n    Process a batch of extracted deep features for GSTVQA evaluation and store results in a JSON file.\n\n    Args:\n        data_batch (SequencTuplee): A batch of data from the dataloader (not used here).\n        data_samples (List[ [torch.Tensor], Tuple[int], Tuple[str] ]): \n            A list containing three tuples:\n            - A tuple of `deep_features`: Each item is a Tensor of shape [T, 2944]. \n            - A tuple of `num_frames`: Each item is an integer representing the number of valid frames.\n            - A tuple of `video_name`: Each item is a string representing the file name for the video.\n            The len of these three tuples are the batch size.\n    \"\"\"\n    # data_samples an example: [\n    #     (tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n    #              [0., 0., 0.,  ..., 0., 0., 0.],\n    #              ...\n    #              [0., 0., 0.,  ..., 0., 0., 0.]]), \n    #      tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n    #              [0., 0., 0.,  ..., 0., 0., 0.],\n    #              ...\n    #              [0., 0., 0.,  ..., 0., 0., 0.]])), \n    #     (10, 10)\n    # ]\n    results = []\n    deep_features_tuple, num_frames_tuple, video_name_tuple = data_samples\n    with torch.no_grad():\n        for deep_features, num_valid_frames, video_name in zip(deep_features_tuple, num_frames_tuple, video_name_tuple):\n            if not isinstance(deep_features, torch.Tensor) or not isinstance(num_valid_frames, int):\n                raise TypeError(\"Expected deep_features to be a torch.Tensor and num_valid_frames to be an int.\")\n\n            if num_valid_frames == 0:  # Edge case: No valid frames\n                results.append({\"video_name\": 'N/A', \"GSTVQA_Score\": 0.0})\n                continue\n\n            # Remove padded features\n            features = deep_features[:num_valid_frames].to(self.device)\n\n            # Compute statistical features only on valid frames\n            mean_var, std_var, mean_mean, std_mean = self.compute_stat_features(features, num_valid_frames)\n            mean_var, std_var, mean_mean, std_mean = (\n                mean_var.to(self.device),\n                std_var.to(self.device),\n                mean_mean.to(self.device),\n                std_mean.to(self.device),\n            )\n\n            # Length tensor indicating the number of valid frames\n            length = torch.tensor([num_valid_frames]).to(self.device)\n            # print('features(input) shape', features.unsqueeze(1).shape) # torch.Size([10, 1, 1472])\n            # print('input_length shape', length.shape) # torch.Size([1])\n            # print('input_length', length) # torch.Size([1])\n            # print('mean_mean shape', mean_mean.shape) # torch.Size([1472])\n            # print('std_mean shape', std_mean.shape) # torch.Size([1472])\n            # print('mean_var shape', mean_var.shape) # torch.Size([1472])\n            # print('std_var shape', std_var.shape) # torch.Size([1472])\n\n            # Run GSTVQA model\n            outputs = self.model(features.unsqueeze(1), length, mean_var, std_var, mean_mean, std_mean)\n            score = outputs.item()\n            results.append({\"video_name\": video_name, \"GSTVQA_Score\": score})\n            # print(f\"Processed score {score:.4f} for {video_name}\")\n\n    self.results.extend(results)\n</code></pre>"},{"location":"documentations/aigve/#aigve.LightVQAPlus","title":"<code>LightVQAPlus</code>","text":"<p>               Bases: <code>BaseMetric</code></p> <p>LightVQA+ metric for evaluating video quality.</p> Source code in <code>aigve/metrics/video_quality_assessment/nn_based/lightvqa_plus/lightvqa_plus_metric.py</code> <pre><code>@METRICS.register_module()\nclass LightVQAPlus(BaseMetric):\n    \"\"\"LightVQA+ metric for evaluating video quality.\"\"\"\n\n    def __init__(self, model_path: str, swin_weights: str, is_gpu: bool = True):\n        super(LightVQAPlus, self).__init__()\n        self.model_path = model_path\n        self.swin_weights = swin_weights\n        self.device = torch.device(\"cuda\" if is_gpu else \"cpu\")\n\n        self.submodel_path = os.path.join(os.getcwd(), 'metrics/video_quality_assessment/nn_based/lightvqa_plus')\n        if not submodule_exists(self.submodel_path):\n            add_git_submodule(\n                repo_url='https://github.com/SaMMyCHoo/Light-VQA-plus.git', \n                submodule_path=self.submodel_path\n            )\n        lightvqa_path = os.path.join(self.submodel_path, \"Light_VQA_plus\")\n        if lightvqa_path not in sys.path:\n            sys.path.insert(0, lightvqa_path)\n\n        from .Light_VQA_plus.final_fusion_model import swin_small_patch4_window7_224 as create_model\n        self.model = create_model().to(self.device)\n\n        weights_dict = torch.load(os.path.join(os.getcwd(), self.model_path), map_location=self.device)\n        print(self.model.load_state_dict(weights_dict))\n\n        self.model.eval()\n\n    def process(self, data_batch: list, data_samples: list) -&gt; None:\n        \"\"\"\n        Process a batch of extracted deep features for LightVQA+ evaluation.\n        Args:\n            data_batch (Sequence): A batch of data from the dataloader (not used here).\n            data_samples (List[Tuple[torch.Tensor], Tuple[torch.Tensor], Tuple[torch.Tensor], Tuple[str]]):\n                - spatial_features (torch.Tensor): Extracts 8 evenly spaced key frames. Shape: [8, 3, 672, 1120].\n                - temporal_features (torch.Tensor): Motion features from SlowFast. Shape: [1, feature_dim(2304)].\n                - bns_features (torch.Tensor): Brightness &amp; Noise features. Shape: [8, 300].\n                - bc_features (torch.Tensor): Temporal brightness contrast features. Shape: [8, final_dim(20)].\n                - video_name (str): Video filename.\n        \"\"\"\n        results = []\n        spatial_features_tuple, temporal_features_tuple, bns_features_tuple, bc_features_tuple, video_name_tuple = data_samples\n        # print('spatial_features_tuple len: ', len(spatial_features_tuple)) # B\n        # print('spatial_features_tuple[0]: ', spatial_features_tuple[0].shape) # torch.Size([8, 3, 672, 1120])\n        # print('temporal_features_tuple[0]: ', temporal_features_tuple[0].shape) # torch.Size([1, 2304])\n        # print('bns_features_tuple[0]: ', bns_features_tuple[0].shape) # torch.Size([8, 300])\n        # print('bc_features_tuple[0]: ', bc_features_tuple[0].shape) # torch.Size([8, 20])\n\n        batch_size = len(spatial_features_tuple)\n        with torch.no_grad():\n            for i in range(batch_size):\n                video_name = video_name_tuple[i]\n                spatial_features = spatial_features_tuple[i].to(self.device) # torch.Size([8, 3, 672, 1120])\n                temporal_features = temporal_features_tuple[i].to(self.device) # torch.Size([1, 2304])\n                bns_features = bns_features_tuple[i].to(self.device) # torch.Size([8, 300])\n                bc_features = bc_features_tuple[i].to(self.device)  # Shape: [8, final_dim(20)]\n\n                concat_features = torch.cat([temporal_features, bc_features.view(1, -1)], dim=1) # torch.Size([1, 2304+8*20])\n                # print('concat_features: ', concat_features.shape) # torch.Size([1, 2464])\n                final_temporal_features = F.pad(concat_features, (0, 2604 - concat_features.shape[1]), mode=\"constant\", value=0) # torch.Size([1, 2604])\n                # print('final_temporal_features: ', final_temporal_features.shape) # torch.Size([1, 2604])\n\n                outputs = self.model(spatial_features, final_temporal_features, bns_features)\n                # print('outputs: ', outputs)\n                score = outputs.mean().item()\n\n                results.append({\"video_name\": video_name, \"LightVQAPlus_Score\": score})\n                print(f\"Processed score {score:.4f} for {video_name}\")\n\n        self.results.extend(results)\n\n    def compute_metrics(self, results: list) -&gt; Dict[str, float]:\n        \"\"\"Compute final LightVQA+ metrics.\"\"\"\n        scores = np.array([res[\"LightVQAPlus_Score\"] for res in self.results])\n        mean_score = np.mean(scores) if scores.size &gt; 0 else 0.0\n        print(f\"LightVQA+ mean score: {mean_score:.4f}\")\n\n        json_file_path = os.path.join(os.getcwd(), \"lightvqaplus_results.json\")\n        final_results = {\"video_results\": self.results, \"LightVQAPlus_Mean_Score\": mean_score}\n        with open(json_file_path, \"w\") as json_file:\n            json.dump(final_results, json_file, indent=4)\n        print(f\"LightVQA+ mean score saved to {json_file_path}\")\n\n        return {\"LightVQAPlus_Mean_Score\": mean_score}\n</code></pre>"},{"location":"documentations/aigve/#aigve.LightVQAPlus.compute_metrics","title":"<code>compute_metrics(results)</code>","text":"<p>Compute final LightVQA+ metrics.</p> Source code in <code>aigve/metrics/video_quality_assessment/nn_based/lightvqa_plus/lightvqa_plus_metric.py</code> <pre><code>def compute_metrics(self, results: list) -&gt; Dict[str, float]:\n    \"\"\"Compute final LightVQA+ metrics.\"\"\"\n    scores = np.array([res[\"LightVQAPlus_Score\"] for res in self.results])\n    mean_score = np.mean(scores) if scores.size &gt; 0 else 0.0\n    print(f\"LightVQA+ mean score: {mean_score:.4f}\")\n\n    json_file_path = os.path.join(os.getcwd(), \"lightvqaplus_results.json\")\n    final_results = {\"video_results\": self.results, \"LightVQAPlus_Mean_Score\": mean_score}\n    with open(json_file_path, \"w\") as json_file:\n        json.dump(final_results, json_file, indent=4)\n    print(f\"LightVQA+ mean score saved to {json_file_path}\")\n\n    return {\"LightVQAPlus_Mean_Score\": mean_score}\n</code></pre>"},{"location":"documentations/aigve/#aigve.LightVQAPlus.process","title":"<code>process(data_batch, data_samples)</code>","text":"<p>Process a batch of extracted deep features for LightVQA+ evaluation. Args:     data_batch (Sequence): A batch of data from the dataloader (not used here).     data_samples (List[Tuple[torch.Tensor], Tuple[torch.Tensor], Tuple[torch.Tensor], Tuple[str]]):         - spatial_features (torch.Tensor): Extracts 8 evenly spaced key frames. Shape: [8, 3, 672, 1120].         - temporal_features (torch.Tensor): Motion features from SlowFast. Shape: [1, feature_dim(2304)].         - bns_features (torch.Tensor): Brightness &amp; Noise features. Shape: [8, 300].         - bc_features (torch.Tensor): Temporal brightness contrast features. Shape: [8, final_dim(20)].         - video_name (str): Video filename.</p> Source code in <code>aigve/metrics/video_quality_assessment/nn_based/lightvqa_plus/lightvqa_plus_metric.py</code> <pre><code>def process(self, data_batch: list, data_samples: list) -&gt; None:\n    \"\"\"\n    Process a batch of extracted deep features for LightVQA+ evaluation.\n    Args:\n        data_batch (Sequence): A batch of data from the dataloader (not used here).\n        data_samples (List[Tuple[torch.Tensor], Tuple[torch.Tensor], Tuple[torch.Tensor], Tuple[str]]):\n            - spatial_features (torch.Tensor): Extracts 8 evenly spaced key frames. Shape: [8, 3, 672, 1120].\n            - temporal_features (torch.Tensor): Motion features from SlowFast. Shape: [1, feature_dim(2304)].\n            - bns_features (torch.Tensor): Brightness &amp; Noise features. Shape: [8, 300].\n            - bc_features (torch.Tensor): Temporal brightness contrast features. Shape: [8, final_dim(20)].\n            - video_name (str): Video filename.\n    \"\"\"\n    results = []\n    spatial_features_tuple, temporal_features_tuple, bns_features_tuple, bc_features_tuple, video_name_tuple = data_samples\n    # print('spatial_features_tuple len: ', len(spatial_features_tuple)) # B\n    # print('spatial_features_tuple[0]: ', spatial_features_tuple[0].shape) # torch.Size([8, 3, 672, 1120])\n    # print('temporal_features_tuple[0]: ', temporal_features_tuple[0].shape) # torch.Size([1, 2304])\n    # print('bns_features_tuple[0]: ', bns_features_tuple[0].shape) # torch.Size([8, 300])\n    # print('bc_features_tuple[0]: ', bc_features_tuple[0].shape) # torch.Size([8, 20])\n\n    batch_size = len(spatial_features_tuple)\n    with torch.no_grad():\n        for i in range(batch_size):\n            video_name = video_name_tuple[i]\n            spatial_features = spatial_features_tuple[i].to(self.device) # torch.Size([8, 3, 672, 1120])\n            temporal_features = temporal_features_tuple[i].to(self.device) # torch.Size([1, 2304])\n            bns_features = bns_features_tuple[i].to(self.device) # torch.Size([8, 300])\n            bc_features = bc_features_tuple[i].to(self.device)  # Shape: [8, final_dim(20)]\n\n            concat_features = torch.cat([temporal_features, bc_features.view(1, -1)], dim=1) # torch.Size([1, 2304+8*20])\n            # print('concat_features: ', concat_features.shape) # torch.Size([1, 2464])\n            final_temporal_features = F.pad(concat_features, (0, 2604 - concat_features.shape[1]), mode=\"constant\", value=0) # torch.Size([1, 2604])\n            # print('final_temporal_features: ', final_temporal_features.shape) # torch.Size([1, 2604])\n\n            outputs = self.model(spatial_features, final_temporal_features, bns_features)\n            # print('outputs: ', outputs)\n            score = outputs.mean().item()\n\n            results.append({\"video_name\": video_name, \"LightVQAPlus_Score\": score})\n            print(f\"Processed score {score:.4f} for {video_name}\")\n\n    self.results.extend(results)\n</code></pre>"},{"location":"documentations/aigve/#aigve.LightVQAPlusDataset","title":"<code>LightVQAPlusDataset</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>Dataset for LightVQA+. Extracts:     - spatial_features (torch.Tensor): Extracted key frames.     - temporal_features (torch.Tensor): SlowFast motion features.     - BNS_features (torch.Tensor): Brightness &amp; Noise features.     - BC_features (torch.Tensor): Temporal CLIP-based brightness contrast features.     - video_name (str): Video filename.</p> Source code in <code>aigve/datasets/lightvqa_plus_dataset.py</code> <pre><code>@DATASETS.register_module()\nclass LightVQAPlusDataset(Dataset):\n    \"\"\"\n    Dataset for LightVQA+.\n    Extracts:\n        - spatial_features (torch.Tensor): Extracted key frames.\n        - temporal_features (torch.Tensor): SlowFast motion features.\n        - BNS_features (torch.Tensor): Brightness &amp; Noise features.\n        - BC_features (torch.Tensor): Temporal CLIP-based brightness contrast features.\n        - video_name (str): Video filename.\n    \"\"\"\n\n    def __init__(self, video_dir, prompt_dir, feature_dir=\"extracted_features\", min_video_seconds=8):\n        super(LightVQAPlusDataset, self).__init__()\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self.video_dir = video_dir\n        self.prompt_dir = prompt_dir\n        self.feature_dir = feature_dir\n        self.min_video_seconds = min_video_seconds\n        os.makedirs(self.feature_dir, exist_ok=True)\n\n        self.video_names = self._read_video_names()\n\n        # Load CLIP model for BNS and BC features\n        self.clip_model, _ = clip.load(\"ViT-B/32\", device=\"cpu\")\n        self.preprocess = transforms.Normalize(\n            (0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711)\n        )\n        self.to_tensor = transforms.ToTensor()\n\n        # CLIP text prompts\n        self.text_B = clip.tokenize([  # brightness (B)\n            \"an underexposed photo\", \"a slightly underexposed photo\",\n            \"a well-exposed photo\", \"a slightly overexposed photo\", \"an overexposed photo\"\n        ])\n\n        self.text_N = clip.tokenize([ # noise (N)\n            \"a photo with no noise\", \"a photo with little noise\",\n            \"a photo with considerable noise\", \"a photo with serious noise\", \"a photo with extreme noise\"\n        ])\n\n        self.submodel_path = os.path.join(os.getcwd(), 'metrics/video_quality_assessment/nn_based/lightvqa_plus')\n        if not submodule_exists(self.submodel_path):\n            add_git_submodule(\n                repo_url='https://github.com/SaMMyCHoo/Light-VQA-plus.git', \n                submodule_path=self.submodel_path\n            )\n        # original_path = os.path.join(self.submodel_path, \"Light-VQA-plus\")\n        lightvqa_path = os.path.join(self.submodel_path, \"Light_VQA_plus\")\n        # if os.path.exists(original_path) and not os.path.exists(lightvqa_path):\n        #     os.rename(original_path, lightvqa_path)\n        if lightvqa_path not in sys.path:\n            sys.path.insert(0, lightvqa_path)\n        # print(sys.path)\n\n        # Load SlowFast model\n\n        slowfast, _ = lazy_import()\n        self.slowfast_model = slowfast()\n\n    def _read_video_names(self):\n        \"\"\"Reads video names from the dataset JSON file.\"\"\"\n        with open(self.prompt_dir, 'r') as reader:\n            read_data = json.load(reader)\n        return [item['video_path_pd'].strip() for item in read_data[\"data_list\"]]\n\n    def __len__(self):\n        return len(self.video_names)\n\n    def extract_key_frames(self, video_path):\n        \"\"\"\n        Extracts 8 evenly spaced key frames across the entire video duration.\n\n        Args:\n            video_path (str): Path to the video file.\n\n        Returns:\n            spatial_features (torch.Tensor): Shape [8, 3, 672, 1120] containing 8 key frames.\n        \"\"\"\n        cap = cv2.VideoCapture(video_path)\n        video_name = os.path.basename(video_path).split('.')[0]\n\n        video_length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n\n        if video_length &gt;= 8:\n            # Select 8 unique frame indices evenly spaced across the entire video\n            frame_indices = np.round(np.linspace(0, video_length - 1, num=8)).astype(int)\n        else:\n            # Select all available frames and repeat the last one to reach 8\n            frame_indices = list(range(video_length)) + [video_length - 1] * (8 - video_length)\n\n        spatial_features = torch.zeros([8, 3, 672, 1120])  # Ensure exactly 8 frames\n        transform = transforms.Compose([\n            transforms.Resize([672, 1120]),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n        ])\n\n        last_valid_frame = None\n        for idx, frame_idx in enumerate(frame_indices):\n            cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)\n            ret, frame = cap.read()\n            if ret:\n                frame = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n                spatial_features[idx] = transform(frame)\n                last_valid_frame = spatial_features[idx]\n            elif last_valid_frame is not None:  # If total frames are less than 8, repeat the last valid frame\n                spatial_features[idx] = last_valid_frame\n\n        cap.release()\n        # print('spatial_features: ', spatial_features.shape) # torch.Size([8, 3, 672, 1120])\n        return spatial_features\n\n    def get_global_sf(self, video_path) -&gt; torch.Tensor:\n        \"\"\"Extracts global brightness &amp; noise features across full video.\n\n        Args:\n            video_path (str): Path to video file.\n\n        Returns:\n            torch.Tensor: Extracted global features (Shape: [8, 150]).\n        \"\"\"\n        cap = cv2.VideoCapture(video_path)\n        video_length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n        # print('video_length: ', video_length)  # 16\n\n        frames = []\n        for _ in range(video_length):\n            ret, frame = cap.read()\n            if ret:\n                frame = cv2.resize(frame, (1120, 672))\n                frames.append(frame)\n        cap.release()\n\n        if not frames:\n            raise ValueError(f\"Failed to extract frames from {video_path}\")\n\n        res = []\n        length = len(frames)\n        now = 0\n        interval = 10  # Process 10 frames at a time\n        while now + interval - 1 &lt; length:\n            final = [self.to_tensor(Image.fromarray(cv2.cvtColor(frames[i + now], cv2.COLOR_BGR2RGB)))\n                    for i in range(interval)]\n\n            # Step 1: Convert to tensor batch\n            images = torch.stack(final, dim=0)  # Shape: [10, 3, 672, 1120]\n\n            # Step 2: Unfold into patches (Strictly following GET_SF)\n            images = images.unfold(2, 224, 224).unfold(3, 224, 224)  # Shape: [10, 3, 3, 5, 224, 224]\n            images = images.permute(0, 3, 2, 1, 4, 5).contiguous()  # Shape: [10, 5, 3, 3, 224, 224]\n            images = images.reshape(-1, 15, 3, 224, 224)  # Shape: [10, 15, 3, 224, 224]\n            images = images.view(-1, 3, 224, 224)  # Shape: [150, 3, 224, 224]\n            images = self.preprocess(images)  # Normalize for CLIP\n            # print('images get_global_sf: ', images.shape) # torch.Size([10*15, 3, 224, 224])\n\n            # Step 3: Extract features using CLIP\n            with torch.no_grad():\n                logits_N, _ = self.clip_model(images, self.text_N)\n                logits_B, _ = self.clip_model(images, self.text_B)\n\n            tmp_N = logits_N.softmax(dim=-1).view(interval, -1) * 10\n            tmp_B = logits_B.softmax(dim=-1).view(interval, -1) * 10\n            # print('tmp_N get_global_sf', tmp_N.shape) # torch.Size([10, 75])\n            # print('tmp_B get_global_sf', tmp_B.shape) # torch.Size([10, 75])\n            res.append(torch.cat([tmp_N, tmp_B], dim=1))\n            now += interval\n\n        # Handle remaining frames\n        if length &gt; now:\n            final = [self.to_tensor(Image.fromarray(cv2.cvtColor(frames[i], cv2.COLOR_BGR2RGB)))\n                    for i in range(now, length)]\n\n            images = torch.stack(final, dim=0)  # Shape: [remaining(6), 3, 672, 1120]\n            images = images.unfold(2, 224, 224).unfold(3, 224, 224)  # Shape: [remaining, 3, 3, 5, 224, 224]\n            images = images.permute(0, 3, 2, 1, 4, 5).contiguous()  # Shape: [remaining, 5, 3, 3, 224, 224]\n            images = images.reshape(-1, 15, 3, 224, 224)  # Shape: [remaining, 15, 3, 224, 224]\n            images = images.view(-1, 3, 224, 224)  # Shape: [remaining*15, 3, 224, 224]\n            images = self.preprocess(images)\n\n            with torch.no_grad():\n                logits_N, _ = self.clip_model(images, self.text_N) # Shape: [remaining, 5(num_text_prompts)]\n                logits_B, _ = self.clip_model(images, self.text_B) # Shape: [remaining, 5]\n                # print('logits_N last get_global_sf', logits_N.shape) # torch.Size([6*15, 5])\n                # print('logits_B last get_global_sf', logits_B.shape) #torch.Size([6*15, 5])\n\n            tmp_N = logits_N.softmax(dim=-1).view(length - now, -1) * 10 # Shape: [remaining, 75]\n            tmp_B = logits_B.softmax(dim=-1).view(length - now, -1) * 10 # Shape: [remaining, 75]\n            # print('tmp_N last get_global_sf', tmp_N.shape)  # torch.Size([6, 75])\n            # print('tmp_B last get_global_sf', tmp_B.shape)  # torch.Size([6, 75])\n\n            res.append(torch.cat([tmp_N, tmp_B], dim=1))\n\n        res = torch.cat(res, dim=0)  # Shape: [length, 150]\n        # print('res: ', res.shape)  # torch.Size([16, 150]) for toy dataset\n\n        # Step 4: Aggregate into 8 time slots\n        chunk_size = length // 8\n        final_res = [\n            torch.mean(res[i * chunk_size: (i + 1) * chunk_size], dim=0) if i &lt; 7 else torch.mean(res[7 * chunk_size:], dim=0)\n            for i in range(8)\n        ]\n\n        return torch.stack(final_res, dim=0)  # Shape: [8, 150]\n\n    def extract_bns_features(self, video_path):\n        \"\"\"Extracts Brightness &amp; Noise Sensitivity (BNS) features using CLIP.\n        Local Feature Extraction (res1) \u2192 Uses 8 key frames\n        Global Feature Extraction (res2) \u2192 Uses all frames\n\n        Args:\n            video_path (str): Path to the video file.\n\n        Returns:\n            spatial_features (torch.Tensor): Extracted 8 evenly spaced key frames across the entire video duration.\n                Shape [8, 3, 672, 1120] containing 8 key frames.\n            final_res (torch.Tensor): Extracted BNS feature (Shape: [8, 300]).\n        \"\"\"\n        # Local Feature Extraction Step 1: Extract key frames\n        spatial_features = self.extract_key_frames(video_path) # Shape: [8, 3, 672, 1120]\n\n        # Step 2: Apply unfolding transformation (Strictly following GET_S_F)\n        images = spatial_features.unfold(2, 224, 224).unfold(3, 224, 224)  # Break into patches. Shape: [8, 3, 3, 5, 224, 224]\n        images = images.permute(0, 3, 2, 1, 4, 5).contiguous()  # Shape: [8, 5, 3, 3, 224, 224]\n        images = images.reshape(-1, 15, 3, 224, 224)  # Shape: [8, 15, 3, 224, 224]\n        images = images.view(-1, 3, 224, 224)  # Shape: [120, 3, 224, 224]\n        images = self.preprocess(images)  # Normalize for CLIP\n        # print('images: ', images.shape) # torch.Size([120, 3, 224, 224])\n        # print(images.device)\n        # print(self.text_N.device)\n\n        # Step 3: Pass through CLIP\n        with torch.no_grad():\n            logits_N, _ = self.clip_model(images, self.text_N)\n            logits_B, _ = self.clip_model(images, self.text_B)\n\n        res_N = logits_N.softmax(dim=-1).view(8, -1) * 10\n        # print('res_N: ', res_N.shape) # torch.Size([8, 75])\n        res_B = logits_B.softmax(dim=-1).view(8, -1) * 10\n        # print('res_B: ', res_N.shape) # torch.Size([8, 75])\n        res1 = torch.cat((res_N, res_B), dim=1)\n        # print('res1: ', res1.shape) # torch.Size([8, 150])\n\n        # Global Feature Extraction (GET_SF Equivalent)\n        res2 = self.get_global_sf(video_path)\n        # print('res2: ', res2.shape) # res2:  torch.Size([8, 150])\n\n        # Split &amp; Combine Features\n        Nl, Bl = torch.split(res1, 75, dim=1)\n        Ng, Bg = torch.split(res2, 75, dim=1)\n        final_res = torch.cat([Nl, Ng, Bl, Bg], dim=1)\n        # print('final_res: ', final_res.shape)\n\n        return spatial_features, final_res  # Shape: [8, 300]\n\n    def extract_bc_features(self, video_path) -&gt; torch.Tensor:\n        \"\"\"\n        Extracts Brightness Consistency features using CLIP-based temporal processing.\n\n        Returns:\n            torch.Tensor: Extracted BC feature (Shape: [8, final_dim]).\n        \"\"\"\n\n        cap = cv2.VideoCapture(video_path)\n        video_length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n\n        frames = []\n        for _ in range(video_length):\n            ret, frame = cap.read()\n            if ret:\n                frame = cv2.resize(frame, (1120, 672))\n                frames.append(frame)\n        cap.release()\n\n        if not frames:\n            raise ValueError(f\"Failed to extract frames from {video_path}\")\n\n        res = []\n        now = 0\n        interval = 10  # Process 10 frames at a time\n        length = len(frames)\n\n        # Step 1: Extract CLIP Features at Fixed Intervals\n        while now + interval - 1 &lt; length:\n            batch = [self.to_tensor(Image.fromarray(cv2.cvtColor(frames[i + now], cv2.COLOR_BGR2RGB)))\n                    for i in range(interval)]\n            images = torch.stack(batch, dim=0)\n            images = images.unfold(2, 224, 224).unfold(3, 224, 224)  # Shape: [10, 3, 3, 5, 224, 224]\n            images = images.permute(0, 3, 2, 1, 4, 5).contiguous()  # Shape: [10, 5, 3, 3, 224, 224]\n            images = images.reshape(-1, 15, 3, 224, 224)  # Shape: [10, 15, 3, 224, 224]\n            images = images.view(-1, 3, 224, 224)  # Shape: [10*15, 3, 224, 224]\n            images = self.preprocess(images)\n            # print('images extract_bc_features', images.shape) # torch.Size([150, 3, 224, 224])\n\n            with torch.no_grad():\n                logits, _ = self.clip_model(images, self.text_B)\n\n            tmp = logits.softmax(dim=-1) * 10\n            res.append(tmp)\n            now += interval\n\n        # Handle Remaining Frames\n        if length &gt; now:\n            batch = [self.to_tensor(Image.fromarray(cv2.cvtColor(frames[i], cv2.COLOR_BGR2RGB)))\n                    for i in range(now, length)]\n            images = torch.stack(batch, dim=0)\n            images = images.unfold(2, 224, 224).unfold(3, 224, 224)  # Shape: [remaining(6), 3, 3, 5, 224, 224]\n            images = images.permute(0, 3, 2, 1, 4, 5).contiguous()  # Shape: [remaining, 5, 3, 3, 224, 224]\n            images = images.reshape(-1, 15, 3, 224, 224)  # Shape: [remaining, 15, 3, 224, 224]\n            images = images.view(-1, 3, 224, 224)  # Shape: [remaining, 15, 3, 224, 224]\n            images = self.preprocess(images)\n            # print('images: ', images.shape) #  torch.Size([6*15, 3, 224, 224])\n\n            with torch.no_grad():\n                logits, _ = self.clip_model(images, self.text_B)\n\n            tmp = logits.softmax(dim=-1) * 10\n            res.append(tmp)\n\n        res = torch.cat(res, dim=0)  # Shape: [length, 5]\n        # print('res extract_bc_features: ', res.shape) # torch.Size([150+90, 5])\n\n        # Step 2: Multi-Scale Variance Computation: downsample frames steps\n        # smaller step: Captures fast, fine-grained changes.\n        # larger step:  Captures slow, long-term trends.\n        final_res = []\n        for step in [1, 2, 4, 8]:  # Multi-scale temporal steps\n            chunk_number = 8 // step\n            chunk_size = length // chunk_number\n            chunks = []\n            for i in range(chunk_number):\n                if i &lt; chunk_number - 1:\n                    chunk = res[i * chunk_size : (i + 1) * chunk_size, :]\n                else:\n                    chunk = res[(chunk_number - 1) * chunk_size:, :]  # Handle remaining frames\n                tmp = []\n                for j in range(step):\n                    temp = chunk[j::step, :]  \n                    tmp.append(torch.var(temp.float(), dim=0))  # Variance computation\n                chunks.append(tmp)  # final chunks len: 8; 4; 2; 1 \n            final_res.append(chunks) # final final_res len: 4\n\n        # Step 3: Aggregate Multi-Scale Features\n        temp = []\n        for i in range(8):  # Aggregate temporal information across 8 time slots\n            temp.append(torch.cat(final_res[0][i]                                                # variance for step size = 1\n                                + [torch.mean(torch.stack(final_res[1][i // 2], dim=0), dim=0)]  # for step size = 2\n                                + [torch.mean(torch.stack(final_res[2][i // 4], dim=0), dim=0)]  # Every 4 slots share the same value.\n                                + [torch.mean(torch.stack(final_res[3][i // 8], dim=0), dim=0)]  # for step size = 8\n                                , dim=0))\n\n        final_res = torch.stack(temp, dim=0)  # Shape: [8, final_dim]  \n        # print('final_res extract_bc_featuresx: ', final_res.shape) # torch.Size([8, 20])\n\n        return final_res\n\n    def extract_temporal_features(self, video_path) -&gt; torch.Tensor:\n        \"\"\"Extracts SlowFast motion features on the entire video segment.\n\n        Args:\n            video_path (str): Path to the video file.\n\n        Returns:\n            torch.Tensor: Extracted motion features (Shape: [1, feature_dim(2304)]).\n        \"\"\"\n        cap = cv2.VideoCapture(video_path)\n        video_length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n        frame_indices = np.round(np.linspace(0, video_length - 1, num=8)).astype(int)\n\n        transform = transforms.Compose([\n            transforms.Resize([224, 224]),  # Match SlowFast input size\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.45, 0.45, 0.45], std=[0.225, 0.225, 0.225])  # Original normalization\n        ])\n\n        frames = []\n        for idx in frame_indices:\n            cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n            ret, frame = cap.read()\n            if ret:\n                frame = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n                frames.append(transform(frame))  # Resize &amp; normalize\n        cap.release()\n\n        if len(frames) &lt; 8:\n            raise ValueError(f\"Insufficient frames in {video_path}, expected 8.\")\n\n        video_tensor = torch.stack(frames, dim=0)  # Shape: [8, 3, 224, 224]\n\n        # Prepare for SlowFast input\n        video_tensor = video_tensor.unsqueeze(0)  # Add batch dimension: [1, 8, 3, 224, 224]\n        video_tensor = video_tensor.permute(0, 2, 1, 3, 4)  # Shape: [1, 3, 8, 224, 224]\n\n        # Pack pathways for SlowFast model\n        _, pack_pathway_output = lazy_import()\n        inputs = pack_pathway_output(video_tensor, device='cpu')\n        # print('inputs len: ', len(inputs))\n        # print('inputs[0]: ', inputs[0].shape) # torch.Size([1, 3, 2, 224, 224])\n        # print('inputs[1]: ', inputs[1].shape) # torch.Size([1, 3, 8, 224, 224])\n\n        # Extract features using SlowFast\n        with torch.no_grad():\n            slow_feature, fast_feature = self.slowfast_model(inputs)\n\n        # print('slow_feature extract_temporal_features: ', slow_feature.shape) # torch.Size([1, 2048, 1, 1, 1])\n        # print('fast_feature extract_temporal_features: ', fast_feature.shape) # torch.Size([1, 256, 1, 1, 1])\n\n        # Concatenate slow and fast features\n        features = torch.cat([slow_feature, fast_feature], dim=1).squeeze(-1).squeeze(-1).squeeze(-1)\n        # print('features extract_temporal_features: ', features.shape) # torch.Size([1, 2304])\n\n        return features\n\n    def __getitem__(self, index):\n        \"\"\"\n        Returns:\n            spatial_features (torch.Tensor): Spatial features. Shape: [8, 3, 672, 1120].\n            bns_features (torch.Tensor): Brightness &amp; Noise features. Shape: [8, 300].\n            (bc_features (torch.Tensor): Temporal brightness contrast features. Shape: [8, final_dim].)\n            temporal_features (torch.Tensor): SlowFast motion features. Shape: [1, feature_dim(2304)]\n            video_name (str): Video filename.\n        \"\"\"\n        video_name = self.video_names[index]\n        video_path = os.path.join(self.video_dir, video_name)\n\n        spatial_features, bns_features = self.extract_bns_features(video_path)\n        bc_features = self.extract_bc_features(video_path)\n        temporal_features = self.extract_temporal_features(video_path)\n\n        # # Save extracted features\n        # feature_path = os.path.join(self.feature_dir, f\"{video_name}_features.pth\")\n        # torch.save({\n        #     \"spatial\": spatial_features,\n        #     \"temporal\": temporal_features,\n        #     \"bns\": bns_features,\n        #     \"bc\": bc_features\n        # }, feature_path)\n\n        return spatial_features, temporal_features, bns_features, bc_features, video_name\n</code></pre>"},{"location":"documentations/aigve/#aigve.LightVQAPlusDataset.__getitem__","title":"<code>__getitem__(index)</code>","text":"<p>Returns:</p> Name Type Description <code>spatial_features</code> <code>Tensor</code> <p>Spatial features. Shape: [8, 3, 672, 1120].</p> <code>bns_features</code> <code>Tensor</code> <p>Brightness &amp; Noise features. Shape: [8, 300].</p> <code>bc_features (torch.Tensor</code> <p>Temporal brightness contrast features. Shape: [8, final_dim].)</p> <code>temporal_features</code> <code>Tensor</code> <p>SlowFast motion features. Shape: [1, feature_dim(2304)]</p> <code>video_name</code> <code>str</code> <p>Video filename.</p> Source code in <code>aigve/datasets/lightvqa_plus_dataset.py</code> <pre><code>def __getitem__(self, index):\n    \"\"\"\n    Returns:\n        spatial_features (torch.Tensor): Spatial features. Shape: [8, 3, 672, 1120].\n        bns_features (torch.Tensor): Brightness &amp; Noise features. Shape: [8, 300].\n        (bc_features (torch.Tensor): Temporal brightness contrast features. Shape: [8, final_dim].)\n        temporal_features (torch.Tensor): SlowFast motion features. Shape: [1, feature_dim(2304)]\n        video_name (str): Video filename.\n    \"\"\"\n    video_name = self.video_names[index]\n    video_path = os.path.join(self.video_dir, video_name)\n\n    spatial_features, bns_features = self.extract_bns_features(video_path)\n    bc_features = self.extract_bc_features(video_path)\n    temporal_features = self.extract_temporal_features(video_path)\n\n    # # Save extracted features\n    # feature_path = os.path.join(self.feature_dir, f\"{video_name}_features.pth\")\n    # torch.save({\n    #     \"spatial\": spatial_features,\n    #     \"temporal\": temporal_features,\n    #     \"bns\": bns_features,\n    #     \"bc\": bc_features\n    # }, feature_path)\n\n    return spatial_features, temporal_features, bns_features, bc_features, video_name\n</code></pre>"},{"location":"documentations/aigve/#aigve.LightVQAPlusDataset._read_video_names","title":"<code>_read_video_names()</code>","text":"<p>Reads video names from the dataset JSON file.</p> Source code in <code>aigve/datasets/lightvqa_plus_dataset.py</code> <pre><code>def _read_video_names(self):\n    \"\"\"Reads video names from the dataset JSON file.\"\"\"\n    with open(self.prompt_dir, 'r') as reader:\n        read_data = json.load(reader)\n    return [item['video_path_pd'].strip() for item in read_data[\"data_list\"]]\n</code></pre>"},{"location":"documentations/aigve/#aigve.LightVQAPlusDataset.extract_bc_features","title":"<code>extract_bc_features(video_path)</code>","text":"<p>Extracts Brightness Consistency features using CLIP-based temporal processing.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: Extracted BC feature (Shape: [8, final_dim]).</p> Source code in <code>aigve/datasets/lightvqa_plus_dataset.py</code> <pre><code>def extract_bc_features(self, video_path) -&gt; torch.Tensor:\n    \"\"\"\n    Extracts Brightness Consistency features using CLIP-based temporal processing.\n\n    Returns:\n        torch.Tensor: Extracted BC feature (Shape: [8, final_dim]).\n    \"\"\"\n\n    cap = cv2.VideoCapture(video_path)\n    video_length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n\n    frames = []\n    for _ in range(video_length):\n        ret, frame = cap.read()\n        if ret:\n            frame = cv2.resize(frame, (1120, 672))\n            frames.append(frame)\n    cap.release()\n\n    if not frames:\n        raise ValueError(f\"Failed to extract frames from {video_path}\")\n\n    res = []\n    now = 0\n    interval = 10  # Process 10 frames at a time\n    length = len(frames)\n\n    # Step 1: Extract CLIP Features at Fixed Intervals\n    while now + interval - 1 &lt; length:\n        batch = [self.to_tensor(Image.fromarray(cv2.cvtColor(frames[i + now], cv2.COLOR_BGR2RGB)))\n                for i in range(interval)]\n        images = torch.stack(batch, dim=0)\n        images = images.unfold(2, 224, 224).unfold(3, 224, 224)  # Shape: [10, 3, 3, 5, 224, 224]\n        images = images.permute(0, 3, 2, 1, 4, 5).contiguous()  # Shape: [10, 5, 3, 3, 224, 224]\n        images = images.reshape(-1, 15, 3, 224, 224)  # Shape: [10, 15, 3, 224, 224]\n        images = images.view(-1, 3, 224, 224)  # Shape: [10*15, 3, 224, 224]\n        images = self.preprocess(images)\n        # print('images extract_bc_features', images.shape) # torch.Size([150, 3, 224, 224])\n\n        with torch.no_grad():\n            logits, _ = self.clip_model(images, self.text_B)\n\n        tmp = logits.softmax(dim=-1) * 10\n        res.append(tmp)\n        now += interval\n\n    # Handle Remaining Frames\n    if length &gt; now:\n        batch = [self.to_tensor(Image.fromarray(cv2.cvtColor(frames[i], cv2.COLOR_BGR2RGB)))\n                for i in range(now, length)]\n        images = torch.stack(batch, dim=0)\n        images = images.unfold(2, 224, 224).unfold(3, 224, 224)  # Shape: [remaining(6), 3, 3, 5, 224, 224]\n        images = images.permute(0, 3, 2, 1, 4, 5).contiguous()  # Shape: [remaining, 5, 3, 3, 224, 224]\n        images = images.reshape(-1, 15, 3, 224, 224)  # Shape: [remaining, 15, 3, 224, 224]\n        images = images.view(-1, 3, 224, 224)  # Shape: [remaining, 15, 3, 224, 224]\n        images = self.preprocess(images)\n        # print('images: ', images.shape) #  torch.Size([6*15, 3, 224, 224])\n\n        with torch.no_grad():\n            logits, _ = self.clip_model(images, self.text_B)\n\n        tmp = logits.softmax(dim=-1) * 10\n        res.append(tmp)\n\n    res = torch.cat(res, dim=0)  # Shape: [length, 5]\n    # print('res extract_bc_features: ', res.shape) # torch.Size([150+90, 5])\n\n    # Step 2: Multi-Scale Variance Computation: downsample frames steps\n    # smaller step: Captures fast, fine-grained changes.\n    # larger step:  Captures slow, long-term trends.\n    final_res = []\n    for step in [1, 2, 4, 8]:  # Multi-scale temporal steps\n        chunk_number = 8 // step\n        chunk_size = length // chunk_number\n        chunks = []\n        for i in range(chunk_number):\n            if i &lt; chunk_number - 1:\n                chunk = res[i * chunk_size : (i + 1) * chunk_size, :]\n            else:\n                chunk = res[(chunk_number - 1) * chunk_size:, :]  # Handle remaining frames\n            tmp = []\n            for j in range(step):\n                temp = chunk[j::step, :]  \n                tmp.append(torch.var(temp.float(), dim=0))  # Variance computation\n            chunks.append(tmp)  # final chunks len: 8; 4; 2; 1 \n        final_res.append(chunks) # final final_res len: 4\n\n    # Step 3: Aggregate Multi-Scale Features\n    temp = []\n    for i in range(8):  # Aggregate temporal information across 8 time slots\n        temp.append(torch.cat(final_res[0][i]                                                # variance for step size = 1\n                            + [torch.mean(torch.stack(final_res[1][i // 2], dim=0), dim=0)]  # for step size = 2\n                            + [torch.mean(torch.stack(final_res[2][i // 4], dim=0), dim=0)]  # Every 4 slots share the same value.\n                            + [torch.mean(torch.stack(final_res[3][i // 8], dim=0), dim=0)]  # for step size = 8\n                            , dim=0))\n\n    final_res = torch.stack(temp, dim=0)  # Shape: [8, final_dim]  \n    # print('final_res extract_bc_featuresx: ', final_res.shape) # torch.Size([8, 20])\n\n    return final_res\n</code></pre>"},{"location":"documentations/aigve/#aigve.LightVQAPlusDataset.extract_bns_features","title":"<code>extract_bns_features(video_path)</code>","text":"<p>Extracts Brightness &amp; Noise Sensitivity (BNS) features using CLIP. Local Feature Extraction (res1) \u2192 Uses 8 key frames Global Feature Extraction (res2) \u2192 Uses all frames</p> <p>Parameters:</p> Name Type Description Default <code>video_path</code> <code>str</code> <p>Path to the video file.</p> required <p>Returns:</p> Name Type Description <code>spatial_features</code> <code>Tensor</code> <p>Extracted 8 evenly spaced key frames across the entire video duration. Shape [8, 3, 672, 1120] containing 8 key frames.</p> <code>final_res</code> <code>Tensor</code> <p>Extracted BNS feature (Shape: [8, 300]).</p> Source code in <code>aigve/datasets/lightvqa_plus_dataset.py</code> <pre><code>def extract_bns_features(self, video_path):\n    \"\"\"Extracts Brightness &amp; Noise Sensitivity (BNS) features using CLIP.\n    Local Feature Extraction (res1) \u2192 Uses 8 key frames\n    Global Feature Extraction (res2) \u2192 Uses all frames\n\n    Args:\n        video_path (str): Path to the video file.\n\n    Returns:\n        spatial_features (torch.Tensor): Extracted 8 evenly spaced key frames across the entire video duration.\n            Shape [8, 3, 672, 1120] containing 8 key frames.\n        final_res (torch.Tensor): Extracted BNS feature (Shape: [8, 300]).\n    \"\"\"\n    # Local Feature Extraction Step 1: Extract key frames\n    spatial_features = self.extract_key_frames(video_path) # Shape: [8, 3, 672, 1120]\n\n    # Step 2: Apply unfolding transformation (Strictly following GET_S_F)\n    images = spatial_features.unfold(2, 224, 224).unfold(3, 224, 224)  # Break into patches. Shape: [8, 3, 3, 5, 224, 224]\n    images = images.permute(0, 3, 2, 1, 4, 5).contiguous()  # Shape: [8, 5, 3, 3, 224, 224]\n    images = images.reshape(-1, 15, 3, 224, 224)  # Shape: [8, 15, 3, 224, 224]\n    images = images.view(-1, 3, 224, 224)  # Shape: [120, 3, 224, 224]\n    images = self.preprocess(images)  # Normalize for CLIP\n    # print('images: ', images.shape) # torch.Size([120, 3, 224, 224])\n    # print(images.device)\n    # print(self.text_N.device)\n\n    # Step 3: Pass through CLIP\n    with torch.no_grad():\n        logits_N, _ = self.clip_model(images, self.text_N)\n        logits_B, _ = self.clip_model(images, self.text_B)\n\n    res_N = logits_N.softmax(dim=-1).view(8, -1) * 10\n    # print('res_N: ', res_N.shape) # torch.Size([8, 75])\n    res_B = logits_B.softmax(dim=-1).view(8, -1) * 10\n    # print('res_B: ', res_N.shape) # torch.Size([8, 75])\n    res1 = torch.cat((res_N, res_B), dim=1)\n    # print('res1: ', res1.shape) # torch.Size([8, 150])\n\n    # Global Feature Extraction (GET_SF Equivalent)\n    res2 = self.get_global_sf(video_path)\n    # print('res2: ', res2.shape) # res2:  torch.Size([8, 150])\n\n    # Split &amp; Combine Features\n    Nl, Bl = torch.split(res1, 75, dim=1)\n    Ng, Bg = torch.split(res2, 75, dim=1)\n    final_res = torch.cat([Nl, Ng, Bl, Bg], dim=1)\n    # print('final_res: ', final_res.shape)\n\n    return spatial_features, final_res  # Shape: [8, 300]\n</code></pre>"},{"location":"documentations/aigve/#aigve.LightVQAPlusDataset.extract_key_frames","title":"<code>extract_key_frames(video_path)</code>","text":"<p>Extracts 8 evenly spaced key frames across the entire video duration.</p> <p>Parameters:</p> Name Type Description Default <code>video_path</code> <code>str</code> <p>Path to the video file.</p> required <p>Returns:</p> Name Type Description <code>spatial_features</code> <code>Tensor</code> <p>Shape [8, 3, 672, 1120] containing 8 key frames.</p> Source code in <code>aigve/datasets/lightvqa_plus_dataset.py</code> <pre><code>def extract_key_frames(self, video_path):\n    \"\"\"\n    Extracts 8 evenly spaced key frames across the entire video duration.\n\n    Args:\n        video_path (str): Path to the video file.\n\n    Returns:\n        spatial_features (torch.Tensor): Shape [8, 3, 672, 1120] containing 8 key frames.\n    \"\"\"\n    cap = cv2.VideoCapture(video_path)\n    video_name = os.path.basename(video_path).split('.')[0]\n\n    video_length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n\n    if video_length &gt;= 8:\n        # Select 8 unique frame indices evenly spaced across the entire video\n        frame_indices = np.round(np.linspace(0, video_length - 1, num=8)).astype(int)\n    else:\n        # Select all available frames and repeat the last one to reach 8\n        frame_indices = list(range(video_length)) + [video_length - 1] * (8 - video_length)\n\n    spatial_features = torch.zeros([8, 3, 672, 1120])  # Ensure exactly 8 frames\n    transform = transforms.Compose([\n        transforms.Resize([672, 1120]),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    ])\n\n    last_valid_frame = None\n    for idx, frame_idx in enumerate(frame_indices):\n        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)\n        ret, frame = cap.read()\n        if ret:\n            frame = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n            spatial_features[idx] = transform(frame)\n            last_valid_frame = spatial_features[idx]\n        elif last_valid_frame is not None:  # If total frames are less than 8, repeat the last valid frame\n            spatial_features[idx] = last_valid_frame\n\n    cap.release()\n    # print('spatial_features: ', spatial_features.shape) # torch.Size([8, 3, 672, 1120])\n    return spatial_features\n</code></pre>"},{"location":"documentations/aigve/#aigve.LightVQAPlusDataset.extract_temporal_features","title":"<code>extract_temporal_features(video_path)</code>","text":"<p>Extracts SlowFast motion features on the entire video segment.</p> <p>Parameters:</p> Name Type Description Default <code>video_path</code> <code>str</code> <p>Path to the video file.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: Extracted motion features (Shape: [1, feature_dim(2304)]).</p> Source code in <code>aigve/datasets/lightvqa_plus_dataset.py</code> <pre><code>def extract_temporal_features(self, video_path) -&gt; torch.Tensor:\n    \"\"\"Extracts SlowFast motion features on the entire video segment.\n\n    Args:\n        video_path (str): Path to the video file.\n\n    Returns:\n        torch.Tensor: Extracted motion features (Shape: [1, feature_dim(2304)]).\n    \"\"\"\n    cap = cv2.VideoCapture(video_path)\n    video_length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n    frame_indices = np.round(np.linspace(0, video_length - 1, num=8)).astype(int)\n\n    transform = transforms.Compose([\n        transforms.Resize([224, 224]),  # Match SlowFast input size\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.45, 0.45, 0.45], std=[0.225, 0.225, 0.225])  # Original normalization\n    ])\n\n    frames = []\n    for idx in frame_indices:\n        cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n        ret, frame = cap.read()\n        if ret:\n            frame = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n            frames.append(transform(frame))  # Resize &amp; normalize\n    cap.release()\n\n    if len(frames) &lt; 8:\n        raise ValueError(f\"Insufficient frames in {video_path}, expected 8.\")\n\n    video_tensor = torch.stack(frames, dim=0)  # Shape: [8, 3, 224, 224]\n\n    # Prepare for SlowFast input\n    video_tensor = video_tensor.unsqueeze(0)  # Add batch dimension: [1, 8, 3, 224, 224]\n    video_tensor = video_tensor.permute(0, 2, 1, 3, 4)  # Shape: [1, 3, 8, 224, 224]\n\n    # Pack pathways for SlowFast model\n    _, pack_pathway_output = lazy_import()\n    inputs = pack_pathway_output(video_tensor, device='cpu')\n    # print('inputs len: ', len(inputs))\n    # print('inputs[0]: ', inputs[0].shape) # torch.Size([1, 3, 2, 224, 224])\n    # print('inputs[1]: ', inputs[1].shape) # torch.Size([1, 3, 8, 224, 224])\n\n    # Extract features using SlowFast\n    with torch.no_grad():\n        slow_feature, fast_feature = self.slowfast_model(inputs)\n\n    # print('slow_feature extract_temporal_features: ', slow_feature.shape) # torch.Size([1, 2048, 1, 1, 1])\n    # print('fast_feature extract_temporal_features: ', fast_feature.shape) # torch.Size([1, 256, 1, 1, 1])\n\n    # Concatenate slow and fast features\n    features = torch.cat([slow_feature, fast_feature], dim=1).squeeze(-1).squeeze(-1).squeeze(-1)\n    # print('features extract_temporal_features: ', features.shape) # torch.Size([1, 2304])\n\n    return features\n</code></pre>"},{"location":"documentations/aigve/#aigve.LightVQAPlusDataset.get_global_sf","title":"<code>get_global_sf(video_path)</code>","text":"<p>Extracts global brightness &amp; noise features across full video.</p> <p>Parameters:</p> Name Type Description Default <code>video_path</code> <code>str</code> <p>Path to video file.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: Extracted global features (Shape: [8, 150]).</p> Source code in <code>aigve/datasets/lightvqa_plus_dataset.py</code> <pre><code>def get_global_sf(self, video_path) -&gt; torch.Tensor:\n    \"\"\"Extracts global brightness &amp; noise features across full video.\n\n    Args:\n        video_path (str): Path to video file.\n\n    Returns:\n        torch.Tensor: Extracted global features (Shape: [8, 150]).\n    \"\"\"\n    cap = cv2.VideoCapture(video_path)\n    video_length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n    # print('video_length: ', video_length)  # 16\n\n    frames = []\n    for _ in range(video_length):\n        ret, frame = cap.read()\n        if ret:\n            frame = cv2.resize(frame, (1120, 672))\n            frames.append(frame)\n    cap.release()\n\n    if not frames:\n        raise ValueError(f\"Failed to extract frames from {video_path}\")\n\n    res = []\n    length = len(frames)\n    now = 0\n    interval = 10  # Process 10 frames at a time\n    while now + interval - 1 &lt; length:\n        final = [self.to_tensor(Image.fromarray(cv2.cvtColor(frames[i + now], cv2.COLOR_BGR2RGB)))\n                for i in range(interval)]\n\n        # Step 1: Convert to tensor batch\n        images = torch.stack(final, dim=0)  # Shape: [10, 3, 672, 1120]\n\n        # Step 2: Unfold into patches (Strictly following GET_SF)\n        images = images.unfold(2, 224, 224).unfold(3, 224, 224)  # Shape: [10, 3, 3, 5, 224, 224]\n        images = images.permute(0, 3, 2, 1, 4, 5).contiguous()  # Shape: [10, 5, 3, 3, 224, 224]\n        images = images.reshape(-1, 15, 3, 224, 224)  # Shape: [10, 15, 3, 224, 224]\n        images = images.view(-1, 3, 224, 224)  # Shape: [150, 3, 224, 224]\n        images = self.preprocess(images)  # Normalize for CLIP\n        # print('images get_global_sf: ', images.shape) # torch.Size([10*15, 3, 224, 224])\n\n        # Step 3: Extract features using CLIP\n        with torch.no_grad():\n            logits_N, _ = self.clip_model(images, self.text_N)\n            logits_B, _ = self.clip_model(images, self.text_B)\n\n        tmp_N = logits_N.softmax(dim=-1).view(interval, -1) * 10\n        tmp_B = logits_B.softmax(dim=-1).view(interval, -1) * 10\n        # print('tmp_N get_global_sf', tmp_N.shape) # torch.Size([10, 75])\n        # print('tmp_B get_global_sf', tmp_B.shape) # torch.Size([10, 75])\n        res.append(torch.cat([tmp_N, tmp_B], dim=1))\n        now += interval\n\n    # Handle remaining frames\n    if length &gt; now:\n        final = [self.to_tensor(Image.fromarray(cv2.cvtColor(frames[i], cv2.COLOR_BGR2RGB)))\n                for i in range(now, length)]\n\n        images = torch.stack(final, dim=0)  # Shape: [remaining(6), 3, 672, 1120]\n        images = images.unfold(2, 224, 224).unfold(3, 224, 224)  # Shape: [remaining, 3, 3, 5, 224, 224]\n        images = images.permute(0, 3, 2, 1, 4, 5).contiguous()  # Shape: [remaining, 5, 3, 3, 224, 224]\n        images = images.reshape(-1, 15, 3, 224, 224)  # Shape: [remaining, 15, 3, 224, 224]\n        images = images.view(-1, 3, 224, 224)  # Shape: [remaining*15, 3, 224, 224]\n        images = self.preprocess(images)\n\n        with torch.no_grad():\n            logits_N, _ = self.clip_model(images, self.text_N) # Shape: [remaining, 5(num_text_prompts)]\n            logits_B, _ = self.clip_model(images, self.text_B) # Shape: [remaining, 5]\n            # print('logits_N last get_global_sf', logits_N.shape) # torch.Size([6*15, 5])\n            # print('logits_B last get_global_sf', logits_B.shape) #torch.Size([6*15, 5])\n\n        tmp_N = logits_N.softmax(dim=-1).view(length - now, -1) * 10 # Shape: [remaining, 75]\n        tmp_B = logits_B.softmax(dim=-1).view(length - now, -1) * 10 # Shape: [remaining, 75]\n        # print('tmp_N last get_global_sf', tmp_N.shape)  # torch.Size([6, 75])\n        # print('tmp_B last get_global_sf', tmp_B.shape)  # torch.Size([6, 75])\n\n        res.append(torch.cat([tmp_N, tmp_B], dim=1))\n\n    res = torch.cat(res, dim=0)  # Shape: [length, 150]\n    # print('res: ', res.shape)  # torch.Size([16, 150]) for toy dataset\n\n    # Step 4: Aggregate into 8 time slots\n    chunk_size = length // 8\n    final_res = [\n        torch.mean(res[i * chunk_size: (i + 1) * chunk_size], dim=0) if i &lt; 7 else torch.mean(res[7 * chunk_size:], dim=0)\n        for i in range(8)\n    ]\n\n    return torch.stack(final_res, dim=0)  # Shape: [8, 150]\n</code></pre>"},{"location":"documentations/aigve/#aigve.SimpleVQADataset","title":"<code>SimpleVQADataset</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>Dataset for SimpleVQA. Each sample returns:     - spatial_features (torch.Tensor): Extracted spatial frames.     - motion_features (torch.Tensor): Extracted motion-based clips.     - video_name (str): Video filename.</p> Source code in <code>aigve/datasets/simplevqa_dataset.py</code> <pre><code>@DATASETS.register_module()\nclass SimpleVQADataset(Dataset):\n    \"\"\"\n    Dataset for SimpleVQA.\n    Each sample returns:\n        - spatial_features (torch.Tensor): Extracted spatial frames.\n        - motion_features (torch.Tensor): Extracted motion-based clips.\n        - video_name (str): Video filename.\n    \"\"\"\n\n    def __init__(self, video_dir, prompt_dir, min_video_seconds=8):\n        super(SimpleVQADataset, self).__init__()\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self.video_dir = video_dir\n        self.prompt_dir = prompt_dir\n        self.min_video_seconds = min_video_seconds\n\n        self.prompts, self.video_names = self._read_prompt_videoname()\n\n    def _read_prompt_videoname(self):\n        with open(self.prompt_dir, 'r') as reader:\n            read_data = json.load(reader)\n\n        prompt_data_list, video_name_list = [], []\n        for item in read_data[\"data_list\"]:\n            prompt = item['prompt_gt'].strip()\n            video_name = item['video_path_pd'].strip()\n            prompt_data_list.append(prompt)\n            video_name_list.append(video_name)\n\n        return prompt_data_list, video_name_list\n\n    def __len__(self):\n        return len(self.prompts)\n\n    def video_processing_spatial(self, video_path):\n        \"\"\"\n        Extracts spatial frames with proper resizing and normalization.\n            - Key frame extraction: It selects 1 frame per second.\n            - Standard input size: It resizes frames to 448 * 448 (after an initial resize to 520).\n        Return:\n            transformed_video (torch.Tensor): shape[video_length_read, 3, 448, 448]. \n                `video_length_read` is total seconds of the video (though 2 for toy dataset) with minium 8 (i.e. min_video_seconds).\n            video_name (str)\n        \"\"\"\n        video_capture = cv2.VideoCapture(video_path)\n        video_name = os.path.basename(video_path)\n        video_length = int(video_capture.get(cv2.CAP_PROP_FRAME_COUNT))\n        video_frame_rate = int(round(video_capture.get(cv2.CAP_PROP_FPS)))\n\n        # Compute the number of total seconds of the video\n        video_length_read = int(video_length/video_frame_rate) # math.ceil()\n        # print('video_length_read (s): ', video_length_read)\n        transformations = transforms.Compose([\n            transforms.Resize(520),\n            transforms.CenterCrop(448),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) # Standard ImageNet normalization\n        ])\n        transformed_video = torch.zeros([max(video_length_read, self.min_video_seconds), 3, 448, 448])\n\n        video_read_index = 0\n        frame_idx = 0\n        for i in range(video_length):\n            has_frames, frame = video_capture.read()\n            if has_frames:\n                # Key frames extraction\n                if (video_read_index &lt; video_length_read) and (frame_idx % video_frame_rate == 0):\n                    read_frame = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n                    read_frame = transformations(read_frame)\n                    transformed_video[video_read_index] = read_frame\n                    video_read_index += 1\n                frame_idx += 1\n\n        # Pads remaining frames by repeating the last available frame.\n        if video_read_index &lt; self.min_video_seconds:\n            for i in range(video_read_index, self.min_video_seconds):\n                transformed_video[i] = transformed_video[video_read_index - 1]\n\n        video_capture.release()\n        return transformed_video, video_name\n\n    def video_processing_motion(self, video_path):\n        \"\"\"\n        Extracts motion-based clips suitable for SlowFast.\n            - Standard input size: It resizes frames to 224 * 224.\n            - Motion-based clips: Processes at leaset 8-second clips, select 32 consecutive frames from each second.\n        Return:\n            transformed_video_all (List[torch.Tensor]): Tensor shape[video_length_clip(32), 3, 224, 224]. \n                Len(List) is total seconds of the video, with minium 8.\n            video_name (str)\n        \"\"\"\n        video_capture = cv2.VideoCapture(video_path)\n        video_name = os.path.basename(video_path)\n        video_length = int(video_capture.get(cv2.CAP_PROP_FRAME_COUNT))\n        video_frame_rate = int(round(video_capture.get(cv2.CAP_PROP_FPS)))\n\n        transform = transforms.Compose([\n            transforms.Resize([224, 224]),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.45, 0.45, 0.45], std=[0.225, 0.225, 0.225]) # General purpose\n        ])\n        transformed_frame_all = torch.zeros([video_length, 3, 224, 224])\n        video_read_index = 0\n        for i in range(video_length): # All frames extraction\n            has_frames, frame = video_capture.read()\n            if has_frames:\n                read_frame = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n                read_frame = transform(read_frame)\n                transformed_frame_all[video_read_index] = read_frame\n                video_read_index += 1\n\n        # Pads remaining frames by repeating the last available frame.\n        if video_read_index &lt; video_length: \n            for i in range(video_read_index, video_length):\n                transformed_frame_all[i] = transformed_frame_all[video_read_index - 1]\n\n        video_capture.release()\n\n        # Compute the number of total seconds of the video\n        video_clip = int(video_length/video_frame_rate)\n        # print('video_clip (s): ', video_clip)\n        video_length_clip = 32\n        transformed_video_all = []\n\n        # Extract motion-based clips: select 32 consecutive frames from each second\n        for i in range(video_clip):\n            transformed_video = torch.zeros([video_length_clip, 3, 224, 224])\n            if (i * video_frame_rate + video_length_clip) &lt;= video_length: # if the clip can be fully extracted, select 32 consecutive frames starting at i*video_frame_rate\n                transformed_video = transformed_frame_all[i * video_frame_rate:(i * video_frame_rate + video_length_clip)]\n            else: # Copy all rest available frames. Pads remaining frames by repeating the last available frame.\n                transformed_video[:(video_length - i * video_frame_rate)] = transformed_frame_all[i * video_frame_rate:]\n                for j in range((video_length - i * video_frame_rate), video_length_clip):\n                    transformed_video[j] = transformed_video[video_length - i * video_frame_rate - 1]\n            transformed_video_all.append(transformed_video)\n\n        if video_clip &lt; self.min_video_seconds:\n            for i in range(video_clip, self.min_video_seconds):\n                transformed_video_all.append(transformed_video_all[video_clip - 1])\n\n        return transformed_video_all, video_name\n\n    def __getitem__(self, index):\n        \"\"\"\n        Returns:\n            spatial_features (torch.Tensor): Shape [v_len_second, 3, 448, 448]\n                `v_len_second` is total seconds of the video (though 2 for toy dataset) with minium 8 (i.e. min_video_seconds).\n            motion_features (List[torch.Tensor]): List of motion feature tensors.\n                Each tensor has shape [32, 3, 224, 224].\n                Len(List) is total seconds of the video (i.e. v_len_second), with minium 8 (i.e. min_video_seconds).\n            video_name (str): Video filename\n        \"\"\"\n        video_name = self.video_names[index]\n        video_path = os.path.join(self.video_dir, video_name)\n\n        spatial_features, video_name = self.video_processing_spatial(video_path)\n        motion_features, video_name = self.video_processing_motion(video_path)\n        # print('spatial_features: ', spatial_features.shape) # torch.Size([8, 3, 448, 448]) for toy dataset\n        # print('motion_features len: ', len(motion_features)) # 8\n        # print('motion_features[0]: ', motion_features[0].shape) # torch.Size([32, 3, 224, 224])\n\n        return spatial_features, motion_features, video_name\n</code></pre>"},{"location":"documentations/aigve/#aigve.SimpleVQADataset.__getitem__","title":"<code>__getitem__(index)</code>","text":"<p>Returns:</p> Name Type Description <code>spatial_features</code> <code>Tensor</code> <p>Shape [v_len_second, 3, 448, 448] <code>v_len_second</code> is total seconds of the video (though 2 for toy dataset) with minium 8 (i.e. min_video_seconds).</p> <code>motion_features</code> <code>List[Tensor]</code> <p>List of motion feature tensors. Each tensor has shape [32, 3, 224, 224]. Len(List) is total seconds of the video (i.e. v_len_second), with minium 8 (i.e. min_video_seconds).</p> <code>video_name</code> <code>str</code> <p>Video filename</p> Source code in <code>aigve/datasets/simplevqa_dataset.py</code> <pre><code>def __getitem__(self, index):\n    \"\"\"\n    Returns:\n        spatial_features (torch.Tensor): Shape [v_len_second, 3, 448, 448]\n            `v_len_second` is total seconds of the video (though 2 for toy dataset) with minium 8 (i.e. min_video_seconds).\n        motion_features (List[torch.Tensor]): List of motion feature tensors.\n            Each tensor has shape [32, 3, 224, 224].\n            Len(List) is total seconds of the video (i.e. v_len_second), with minium 8 (i.e. min_video_seconds).\n        video_name (str): Video filename\n    \"\"\"\n    video_name = self.video_names[index]\n    video_path = os.path.join(self.video_dir, video_name)\n\n    spatial_features, video_name = self.video_processing_spatial(video_path)\n    motion_features, video_name = self.video_processing_motion(video_path)\n    # print('spatial_features: ', spatial_features.shape) # torch.Size([8, 3, 448, 448]) for toy dataset\n    # print('motion_features len: ', len(motion_features)) # 8\n    # print('motion_features[0]: ', motion_features[0].shape) # torch.Size([32, 3, 224, 224])\n\n    return spatial_features, motion_features, video_name\n</code></pre>"},{"location":"documentations/aigve/#aigve.SimpleVQADataset.video_processing_motion","title":"<code>video_processing_motion(video_path)</code>","text":"<p>Extracts motion-based clips suitable for SlowFast.     - Standard input size: It resizes frames to 224 * 224.     - Motion-based clips: Processes at leaset 8-second clips, select 32 consecutive frames from each second. Return:     transformed_video_all (List[torch.Tensor]): Tensor shape[video_length_clip(32), 3, 224, 224].          Len(List) is total seconds of the video, with minium 8.     video_name (str)</p> Source code in <code>aigve/datasets/simplevqa_dataset.py</code> <pre><code>def video_processing_motion(self, video_path):\n    \"\"\"\n    Extracts motion-based clips suitable for SlowFast.\n        - Standard input size: It resizes frames to 224 * 224.\n        - Motion-based clips: Processes at leaset 8-second clips, select 32 consecutive frames from each second.\n    Return:\n        transformed_video_all (List[torch.Tensor]): Tensor shape[video_length_clip(32), 3, 224, 224]. \n            Len(List) is total seconds of the video, with minium 8.\n        video_name (str)\n    \"\"\"\n    video_capture = cv2.VideoCapture(video_path)\n    video_name = os.path.basename(video_path)\n    video_length = int(video_capture.get(cv2.CAP_PROP_FRAME_COUNT))\n    video_frame_rate = int(round(video_capture.get(cv2.CAP_PROP_FPS)))\n\n    transform = transforms.Compose([\n        transforms.Resize([224, 224]),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.45, 0.45, 0.45], std=[0.225, 0.225, 0.225]) # General purpose\n    ])\n    transformed_frame_all = torch.zeros([video_length, 3, 224, 224])\n    video_read_index = 0\n    for i in range(video_length): # All frames extraction\n        has_frames, frame = video_capture.read()\n        if has_frames:\n            read_frame = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n            read_frame = transform(read_frame)\n            transformed_frame_all[video_read_index] = read_frame\n            video_read_index += 1\n\n    # Pads remaining frames by repeating the last available frame.\n    if video_read_index &lt; video_length: \n        for i in range(video_read_index, video_length):\n            transformed_frame_all[i] = transformed_frame_all[video_read_index - 1]\n\n    video_capture.release()\n\n    # Compute the number of total seconds of the video\n    video_clip = int(video_length/video_frame_rate)\n    # print('video_clip (s): ', video_clip)\n    video_length_clip = 32\n    transformed_video_all = []\n\n    # Extract motion-based clips: select 32 consecutive frames from each second\n    for i in range(video_clip):\n        transformed_video = torch.zeros([video_length_clip, 3, 224, 224])\n        if (i * video_frame_rate + video_length_clip) &lt;= video_length: # if the clip can be fully extracted, select 32 consecutive frames starting at i*video_frame_rate\n            transformed_video = transformed_frame_all[i * video_frame_rate:(i * video_frame_rate + video_length_clip)]\n        else: # Copy all rest available frames. Pads remaining frames by repeating the last available frame.\n            transformed_video[:(video_length - i * video_frame_rate)] = transformed_frame_all[i * video_frame_rate:]\n            for j in range((video_length - i * video_frame_rate), video_length_clip):\n                transformed_video[j] = transformed_video[video_length - i * video_frame_rate - 1]\n        transformed_video_all.append(transformed_video)\n\n    if video_clip &lt; self.min_video_seconds:\n        for i in range(video_clip, self.min_video_seconds):\n            transformed_video_all.append(transformed_video_all[video_clip - 1])\n\n    return transformed_video_all, video_name\n</code></pre>"},{"location":"documentations/aigve/#aigve.SimpleVQADataset.video_processing_spatial","title":"<code>video_processing_spatial(video_path)</code>","text":"<p>Extracts spatial frames with proper resizing and normalization.     - Key frame extraction: It selects 1 frame per second.     - Standard input size: It resizes frames to 448 * 448 (after an initial resize to 520). Return:     transformed_video (torch.Tensor): shape[video_length_read, 3, 448, 448].          <code>video_length_read</code> is total seconds of the video (though 2 for toy dataset) with minium 8 (i.e. min_video_seconds).     video_name (str)</p> Source code in <code>aigve/datasets/simplevqa_dataset.py</code> <pre><code>def video_processing_spatial(self, video_path):\n    \"\"\"\n    Extracts spatial frames with proper resizing and normalization.\n        - Key frame extraction: It selects 1 frame per second.\n        - Standard input size: It resizes frames to 448 * 448 (after an initial resize to 520).\n    Return:\n        transformed_video (torch.Tensor): shape[video_length_read, 3, 448, 448]. \n            `video_length_read` is total seconds of the video (though 2 for toy dataset) with minium 8 (i.e. min_video_seconds).\n        video_name (str)\n    \"\"\"\n    video_capture = cv2.VideoCapture(video_path)\n    video_name = os.path.basename(video_path)\n    video_length = int(video_capture.get(cv2.CAP_PROP_FRAME_COUNT))\n    video_frame_rate = int(round(video_capture.get(cv2.CAP_PROP_FPS)))\n\n    # Compute the number of total seconds of the video\n    video_length_read = int(video_length/video_frame_rate) # math.ceil()\n    # print('video_length_read (s): ', video_length_read)\n    transformations = transforms.Compose([\n        transforms.Resize(520),\n        transforms.CenterCrop(448),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) # Standard ImageNet normalization\n    ])\n    transformed_video = torch.zeros([max(video_length_read, self.min_video_seconds), 3, 448, 448])\n\n    video_read_index = 0\n    frame_idx = 0\n    for i in range(video_length):\n        has_frames, frame = video_capture.read()\n        if has_frames:\n            # Key frames extraction\n            if (video_read_index &lt; video_length_read) and (frame_idx % video_frame_rate == 0):\n                read_frame = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n                read_frame = transformations(read_frame)\n                transformed_video[video_read_index] = read_frame\n                video_read_index += 1\n            frame_idx += 1\n\n    # Pads remaining frames by repeating the last available frame.\n    if video_read_index &lt; self.min_video_seconds:\n        for i in range(video_read_index, self.min_video_seconds):\n            transformed_video[i] = transformed_video[video_read_index - 1]\n\n    video_capture.release()\n    return transformed_video, video_name\n</code></pre>"},{"location":"documentations/aigve/#aigve.SimpleVqa","title":"<code>SimpleVqa</code>","text":"<p>               Bases: <code>BaseMetric</code></p> <p>SimpleVQA metric for evaluating video quality.</p> Source code in <code>aigve/metrics/video_quality_assessment/nn_based/simplevqa/simplevqa_metric.py</code> <pre><code>@METRICS.register_module()\nclass SimpleVqa(BaseMetric):\n    \"\"\"SimpleVQA metric for evaluating video quality.\"\"\"\n    def __init__(self, model_path: str, is_gpu: bool = True):\n        super(SimpleVqa, self).__init__()\n        self.model_path = model_path\n        self.device = torch.device(\"cuda\" if is_gpu else \"cpu\")\n        self.submodel_path = os.path.join(os.getcwd(), 'metrics/video_quality_assessment/nn_based/simplevqa')\n        if not submodule_exists(self.submodel_path):\n            add_git_submodule(\n                repo_url='https://github.com/sunwei925/SimpleVQA.git', \n                submodule_path=self.submodel_path\n            )\n        simplevqa_path = os.path.join(self.submodel_path, \"SimpleVQA\")\n        if simplevqa_path not in sys.path:\n            sys.path.insert(0, simplevqa_path)\n        from .SimpleVQA.model import UGC_BVQA_model\n        from .SimpleVQA.test_demo import slowfast\n        self.model_motion = slowfast().to(self.device)\n        self.model = UGC_BVQA_model.resnet50(pretrained=False)\n        self.model = torch.nn.DataParallel(self.model).to(self.device)\n        self.model.load_state_dict(torch.load(os.path.join(os.getcwd(), self.model_path), map_location=self.device))\n        self.model.eval()\n\n    def process(self, data_batch: list, data_samples: list) -&gt; None:\n        \"\"\"\n        Process a batch of extracted deep features for SimpleVQA evaluation.\n        Args:\n            data_batch (Sequence): A batch of data from the dataloader (not used here).\n            data_samples (List[ Tuple[torch.Tensor], List[Tuple[torch.Tensor]], Tuple[str] ]):\n                A list containing three tuples:\n                - A tuple of `spatial_features` (torch.Tensor): Shape [v_len_second, 3, 448, 448]. \n                    `v_len_second` is total seconds of the video (though 2 for toy dataset) with minium 8 (i.e. min_video_seconds). \n                    The len of the tuple is the batch size. \n                - A list of `motion_features` (Tuple[torch.Tensor]): \n                    len(List) is total seconds of the video, with minium 8 (i.e. min_video_seconds).\n                    Each item of the list is a Tuple of motion feature tensors. Each has shape [32, 3, 224, 224].\n                    The len of the tuple is the batch size.\n                - A tuple of `video_name` (str): Video filename. The len of the tuple is the batch size.\n        \"\"\"\n        from .SimpleVQA.test_demo import pack_pathway_output\n\n        results = []\n        # print(type(data_samples)) # list\n        spatial_features_tuple, motion_features_list, video_name_tuple = data_samples\n        # print(len(spatial_features_tuple)) # 1\n        # print(spatial_features_tuple[0].shape) # torch.Size([8, 3, 448, 448])\n\n        # print(type(motion_features_list)) # List\n        # print(len(motion_features_list)) # 8\n        # print(type(motion_features_list[0])) # tuple\n        # print(len(motion_features_list[0])) # 1\n        # print(type(motion_features_list[0][0])) # Tensor\n        # print(motion_features_list[0][0].shape) # torch.Size([32, 3, 224, 224])\n\n        batch_size = len(spatial_features_tuple)\n        with torch.no_grad():\n            for i in range(batch_size):\n                video_name = video_name_tuple[i]\n                spatial_features = spatial_features_tuple[i].to(self.device).unsqueeze(0)  # Add batch dim. Shape: tensor with Size([1, v_len_second, 3, 448, 448])\n\n                # Take the i-th element from each tuple in motion_features_list\n                motion_features = [motion_features_list[j][i] for j in range(len(motion_features_list))] # Shape: List[tensor with Size([32, 3, 224, 224])], len of it is total seconds of the video, with minium 8.\n\n                if not all(isinstance(mf, torch.Tensor) for mf in motion_features):\n                    raise TypeError(\"Expected motion_features to be a list of tensors.\")\n\n                if len(motion_features) == 0:  # Edge case: No valid motion features\n                    results.append({\"video_name\": video_name, \"SimpleVQA_Score\": 0.0})\n                    continue\n\n                n_clip = len(motion_features)  # 8\n                feature_motion = torch.zeros([n_clip, 2048 + 256], device=self.device) \n                # Process each motion clip\n                for idx, clip in enumerate(motion_features):\n                    clip = clip.unsqueeze(dim=0).permute(0, 2, 1, 3, 4)  # Reshape to [1, C(3), T(32), H(224), W(224)]\n                    clip = pack_pathway_output(clip, self.device)  # Convert to SlowFast format\n                    slow_feature, fast_feature = self.model_motion(clip)\n                    slow_feature = slow_feature.squeeze()\n                    fast_feature = fast_feature.squeeze()\n\n                    motion_feature = torch.cat([slow_feature, fast_feature]).unsqueeze(0)  # Shape: [1, 2304]\n                    feature_motion[idx] = motion_feature \n\n                feature_motion = feature_motion.unsqueeze(0)  # Shape: [1, n_clip, 2304]\n\n                outputs = self.model(spatial_features, feature_motion)\n                score = outputs.item()\n\n                results.append({\"video_name\": video_name, \"SimpleVQA_Score\": score})\n                print(f\"Processed score {score:.4f} for {video_name}\")\n\n        self.results.extend(results)\n\n    def compute_metrics(self, results: list) -&gt; Dict[str, float]:\n        \"\"\"Compute final SimpleVQA-based metrics.\"\"\"\n        scores = np.array([res[\"SimpleVQA_Score\"] for res in self.results])\n        mean_score = np.mean(scores) if scores.size &gt; 0 else 0.0\n        print(f\"SimpleVQA mean score: {mean_score:.4f}\")\n\n        json_file_path = os.path.join(os.getcwd(), \"simplevqa_results.json\")\n        final_results = {\"video_results\": self.results, \"SimpleVQA_Mean_Score\": mean_score}\n        with open(json_file_path, \"w\") as json_file:\n            json.dump(final_results, json_file, indent=4)\n        print(f\"SimpleVQA mean score saved to {json_file_path}\")\n\n        return {\"SimpleVQA_Mean_Score\": mean_score}\n</code></pre>"},{"location":"documentations/aigve/#aigve.SimpleVqa.compute_metrics","title":"<code>compute_metrics(results)</code>","text":"<p>Compute final SimpleVQA-based metrics.</p> Source code in <code>aigve/metrics/video_quality_assessment/nn_based/simplevqa/simplevqa_metric.py</code> <pre><code>def compute_metrics(self, results: list) -&gt; Dict[str, float]:\n    \"\"\"Compute final SimpleVQA-based metrics.\"\"\"\n    scores = np.array([res[\"SimpleVQA_Score\"] for res in self.results])\n    mean_score = np.mean(scores) if scores.size &gt; 0 else 0.0\n    print(f\"SimpleVQA mean score: {mean_score:.4f}\")\n\n    json_file_path = os.path.join(os.getcwd(), \"simplevqa_results.json\")\n    final_results = {\"video_results\": self.results, \"SimpleVQA_Mean_Score\": mean_score}\n    with open(json_file_path, \"w\") as json_file:\n        json.dump(final_results, json_file, indent=4)\n    print(f\"SimpleVQA mean score saved to {json_file_path}\")\n\n    return {\"SimpleVQA_Mean_Score\": mean_score}\n</code></pre>"},{"location":"documentations/aigve/#aigve.SimpleVqa.process","title":"<code>process(data_batch, data_samples)</code>","text":"<p>Process a batch of extracted deep features for SimpleVQA evaluation. Args:     data_batch (Sequence): A batch of data from the dataloader (not used here).     data_samples (List[ Tuple[torch.Tensor], List[Tuple[torch.Tensor]], Tuple[str] ]):         A list containing three tuples:         - A tuple of <code>spatial_features</code> (torch.Tensor): Shape [v_len_second, 3, 448, 448].              <code>v_len_second</code> is total seconds of the video (though 2 for toy dataset) with minium 8 (i.e. min_video_seconds).              The len of the tuple is the batch size.          - A list of <code>motion_features</code> (Tuple[torch.Tensor]):              len(List) is total seconds of the video, with minium 8 (i.e. min_video_seconds).             Each item of the list is a Tuple of motion feature tensors. Each has shape [32, 3, 224, 224].             The len of the tuple is the batch size.         - A tuple of <code>video_name</code> (str): Video filename. The len of the tuple is the batch size.</p> Source code in <code>aigve/metrics/video_quality_assessment/nn_based/simplevqa/simplevqa_metric.py</code> <pre><code>def process(self, data_batch: list, data_samples: list) -&gt; None:\n    \"\"\"\n    Process a batch of extracted deep features for SimpleVQA evaluation.\n    Args:\n        data_batch (Sequence): A batch of data from the dataloader (not used here).\n        data_samples (List[ Tuple[torch.Tensor], List[Tuple[torch.Tensor]], Tuple[str] ]):\n            A list containing three tuples:\n            - A tuple of `spatial_features` (torch.Tensor): Shape [v_len_second, 3, 448, 448]. \n                `v_len_second` is total seconds of the video (though 2 for toy dataset) with minium 8 (i.e. min_video_seconds). \n                The len of the tuple is the batch size. \n            - A list of `motion_features` (Tuple[torch.Tensor]): \n                len(List) is total seconds of the video, with minium 8 (i.e. min_video_seconds).\n                Each item of the list is a Tuple of motion feature tensors. Each has shape [32, 3, 224, 224].\n                The len of the tuple is the batch size.\n            - A tuple of `video_name` (str): Video filename. The len of the tuple is the batch size.\n    \"\"\"\n    from .SimpleVQA.test_demo import pack_pathway_output\n\n    results = []\n    # print(type(data_samples)) # list\n    spatial_features_tuple, motion_features_list, video_name_tuple = data_samples\n    # print(len(spatial_features_tuple)) # 1\n    # print(spatial_features_tuple[0].shape) # torch.Size([8, 3, 448, 448])\n\n    # print(type(motion_features_list)) # List\n    # print(len(motion_features_list)) # 8\n    # print(type(motion_features_list[0])) # tuple\n    # print(len(motion_features_list[0])) # 1\n    # print(type(motion_features_list[0][0])) # Tensor\n    # print(motion_features_list[0][0].shape) # torch.Size([32, 3, 224, 224])\n\n    batch_size = len(spatial_features_tuple)\n    with torch.no_grad():\n        for i in range(batch_size):\n            video_name = video_name_tuple[i]\n            spatial_features = spatial_features_tuple[i].to(self.device).unsqueeze(0)  # Add batch dim. Shape: tensor with Size([1, v_len_second, 3, 448, 448])\n\n            # Take the i-th element from each tuple in motion_features_list\n            motion_features = [motion_features_list[j][i] for j in range(len(motion_features_list))] # Shape: List[tensor with Size([32, 3, 224, 224])], len of it is total seconds of the video, with minium 8.\n\n            if not all(isinstance(mf, torch.Tensor) for mf in motion_features):\n                raise TypeError(\"Expected motion_features to be a list of tensors.\")\n\n            if len(motion_features) == 0:  # Edge case: No valid motion features\n                results.append({\"video_name\": video_name, \"SimpleVQA_Score\": 0.0})\n                continue\n\n            n_clip = len(motion_features)  # 8\n            feature_motion = torch.zeros([n_clip, 2048 + 256], device=self.device) \n            # Process each motion clip\n            for idx, clip in enumerate(motion_features):\n                clip = clip.unsqueeze(dim=0).permute(0, 2, 1, 3, 4)  # Reshape to [1, C(3), T(32), H(224), W(224)]\n                clip = pack_pathway_output(clip, self.device)  # Convert to SlowFast format\n                slow_feature, fast_feature = self.model_motion(clip)\n                slow_feature = slow_feature.squeeze()\n                fast_feature = fast_feature.squeeze()\n\n                motion_feature = torch.cat([slow_feature, fast_feature]).unsqueeze(0)  # Shape: [1, 2304]\n                feature_motion[idx] = motion_feature \n\n            feature_motion = feature_motion.unsqueeze(0)  # Shape: [1, n_clip, 2304]\n\n            outputs = self.model(spatial_features, feature_motion)\n            score = outputs.item()\n\n            results.append({\"video_name\": video_name, \"SimpleVQA_Score\": score})\n            print(f\"Processed score {score:.4f} for {video_name}\")\n\n    self.results.extend(results)\n</code></pre>"},{"location":"documentations/aigve/#aigve.TIFAScore","title":"<code>TIFAScore</code>","text":"<p>               Bases: <code>BaseMetric</code></p> <p>Initialize the <code>TIFAScore</code> evaluator.</p> <p>Parameters:</p> Name Type Description Default <code>openai_key</code> <code>str</code> <p>The user's api key of the LLM models openai provides.</p> required <code>llm_model</code> <code>str</code> <p>The name of the LLM model used in the TIFAScore evaluator. Defaults to <code>gpt-3.5-turbo</code>.</p> <code>'gpt-3.5-turbo'</code> <code>unifiedqa_model_name</code> <code>str</code> <p>The name of the <code>UnifiedQAModel</code> used in TIFAScore evaluator. Defaults to <code>allenai/unifiedqa-v2-t5-large-1363200</code>.</p> <code>'allenai/unifiedqa-v2-t5-large-1363200'</code> <code>vqa_model_name</code> <code>str</code> <p>The name of the <code>VQAModel used</code> in TIFAScore evaluator. Defaults to <code>mplug-large</code>.</p> <code>'mplug-large'</code> <p>Returns:</p> Type Description <p>None</p> Source code in <code>aigve/metrics/text_video_alignment/gpt_based/TIFA/tifa_eval.py</code> <pre><code>@METRICS.register_module()\nclass TIFAScore(BaseMetric):\n    \"\"\" Initialize the ``TIFAScore`` evaluator.\n\n    Args:   \n            openai_key (str): The user's api key of the LLM models openai provides.\n            llm_model (str): The name of the LLM model used in the TIFAScore evaluator. Defaults to ``gpt-3.5-turbo``.\n            unifiedqa_model_name (str): The name of the ``UnifiedQAModel`` used in TIFAScore evaluator. Defaults to ``allenai/unifiedqa-v2-t5-large-1363200``.\n            vqa_model_name (str): The name of the ``VQAModel used`` in TIFAScore evaluator. Defaults to ``mplug-large``.\n\n    Returns:\n            None\n    \"\"\"\n    def __init__(self, \n                 openai_key,\n                 llm_model: str = 'gpt-3.5-turbo',\n                 unifiedqa_model_name: str = 'allenai/unifiedqa-v2-t5-large-1363200',\n                 vqa_model_name: str = 'mplug-large'):\n        super().__init__()\n\n        self.openai_key = openai_key\n        self.llm_model = llm_model\n        self.unifiedqa_model_name = unifiedqa_model_name\n        self.openai_completion, self.get_question_and_answers, self.filter_question_and_answers, self.unifiedqa_model, self.tifa_score_single, self.vqa_model = lazy_import()\n        self.unifiedqa_model = self.UnifiedQAModel(self.unifiedqa_model_name)\n        self.vqa_model_name = vqa_model_name\n        self.vqa_model = self.VQAModel(self.vqa_model_name)\n\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n        self.openai_setup()\n\n    def openai_setup(self):\n        print('set up openai client')\n        openai.api_key = self.openai_key\n        assert openai.api_key is not None\n        test_prompt_string = 'hello, how are you doing?'\n        print('test prompt: ', test_prompt_string)\n        response = self.openai_completion(\n            test_prompt_string,\n            model=self.llm_model,\n        )\n        print('test response: ', response)\n\n\n    def process(self, data_batch: Sequence, data_samples: Sequence) -&gt; None:\n        \"\"\" TIFAScore process\n        Process one batch of data samples and predictions. The processed\n        results should be stored in ``self.results``, which will be used to\n        compute the metrics when all batches have been processed.\n\n        Args:\n            data_batch (Sequence): A batch of data from the dataloader.\n            data_samples (Sequence): A batch of data samples that\n                contain annotations and predictions.\n        \"\"\"\n\n        result = dict()\n\n        input_prompts, input_videos = data_samples\n        bsz = len(input_prompts)\n\n        # Ensure prompt_input is a tensor\n        if isinstance(input_prompts, tuple):\n            input_prompts = list(input_prompts)\n\n        if isinstance(input_videos, tuple):\n            input_videos = list(input_videos)\n\n        average_tifa_score_list = []\n        for input_prompt, input_video in zip(input_prompts, input_videos):\n            tifa_score = []\n            # Generate questions with GPT-3.5-turbo\n            gpt3_questions = self.get_question_and_answers(input_prompt)\n            # print(gpt3_questions)\n            # Filter questions with UnifiedQA\n            filtered_questions = self.filter_question_and_answers(self.unifiedqa_model, gpt3_questions)\n            for index, frame_path in enumerate(input_video):\n                # calucluate TIFA score\n                result = self.tifa_score_single(self.vqa_model, filtered_questions, frame_path)\n                # print(result)\n                tifa_score.append(result['tifa_score'])\n            average_tifa_score = sum(tifa_score)/len(tifa_score)\n            average_tifa_score_list.append(average_tifa_score)\n\n        result['tifa_score'] = sum(average_tifa_score_list)/len(average_tifa_score_list)\n\n        self.results.append(result)\n\n\n    def compute_metrics(self, results: list) -&gt; Dict[str, float]:\n        \"\"\"Compute the metrics from processed results.\n\n        Args:\n            results (list): The processed results of each batch.\n\n        Returns:\n            Dict[str, float]: The computed metrics. The keys are the names of\n            the metrics, and the values are corresponding results.\n        \"\"\"\n        logger: MMLogger = MMLogger.get_current_instance()\n\n        tifa_score_np = np.zeros(len(results))\n        for i, result in enumerate(results):\n            tifa_score_np[i] = result['tifa_score']\n\n        tifa_score_np_mean = np.mean(tifa_score_np) \n\n        print(\"Test results: tifa score={:.4f}\"\n              .format(tifa_score_np_mean))\n\n        return result\n</code></pre>"},{"location":"documentations/aigve/#aigve.TIFAScore.compute_metrics","title":"<code>compute_metrics(results)</code>","text":"<p>Compute the metrics from processed results.</p> <p>Parameters:</p> Name Type Description Default <code>results</code> <code>list</code> <p>The processed results of each batch.</p> required <p>Returns:</p> Type Description <code>Dict[str, float]</code> <p>Dict[str, float]: The computed metrics. The keys are the names of</p> <code>Dict[str, float]</code> <p>the metrics, and the values are corresponding results.</p> Source code in <code>aigve/metrics/text_video_alignment/gpt_based/TIFA/tifa_eval.py</code> <pre><code>def compute_metrics(self, results: list) -&gt; Dict[str, float]:\n    \"\"\"Compute the metrics from processed results.\n\n    Args:\n        results (list): The processed results of each batch.\n\n    Returns:\n        Dict[str, float]: The computed metrics. The keys are the names of\n        the metrics, and the values are corresponding results.\n    \"\"\"\n    logger: MMLogger = MMLogger.get_current_instance()\n\n    tifa_score_np = np.zeros(len(results))\n    for i, result in enumerate(results):\n        tifa_score_np[i] = result['tifa_score']\n\n    tifa_score_np_mean = np.mean(tifa_score_np) \n\n    print(\"Test results: tifa score={:.4f}\"\n          .format(tifa_score_np_mean))\n\n    return result\n</code></pre>"},{"location":"documentations/aigve/#aigve.TIFAScore.process","title":"<code>process(data_batch, data_samples)</code>","text":"<p>TIFAScore process Process one batch of data samples and predictions. The processed results should be stored in <code>self.results</code>, which will be used to compute the metrics when all batches have been processed.</p> <p>Parameters:</p> Name Type Description Default <code>data_batch</code> <code>Sequence</code> <p>A batch of data from the dataloader.</p> required <code>data_samples</code> <code>Sequence</code> <p>A batch of data samples that contain annotations and predictions.</p> required Source code in <code>aigve/metrics/text_video_alignment/gpt_based/TIFA/tifa_eval.py</code> <pre><code>def process(self, data_batch: Sequence, data_samples: Sequence) -&gt; None:\n    \"\"\" TIFAScore process\n    Process one batch of data samples and predictions. The processed\n    results should be stored in ``self.results``, which will be used to\n    compute the metrics when all batches have been processed.\n\n    Args:\n        data_batch (Sequence): A batch of data from the dataloader.\n        data_samples (Sequence): A batch of data samples that\n            contain annotations and predictions.\n    \"\"\"\n\n    result = dict()\n\n    input_prompts, input_videos = data_samples\n    bsz = len(input_prompts)\n\n    # Ensure prompt_input is a tensor\n    if isinstance(input_prompts, tuple):\n        input_prompts = list(input_prompts)\n\n    if isinstance(input_videos, tuple):\n        input_videos = list(input_videos)\n\n    average_tifa_score_list = []\n    for input_prompt, input_video in zip(input_prompts, input_videos):\n        tifa_score = []\n        # Generate questions with GPT-3.5-turbo\n        gpt3_questions = self.get_question_and_answers(input_prompt)\n        # print(gpt3_questions)\n        # Filter questions with UnifiedQA\n        filtered_questions = self.filter_question_and_answers(self.unifiedqa_model, gpt3_questions)\n        for index, frame_path in enumerate(input_video):\n            # calucluate TIFA score\n            result = self.tifa_score_single(self.vqa_model, filtered_questions, frame_path)\n            # print(result)\n            tifa_score.append(result['tifa_score'])\n        average_tifa_score = sum(tifa_score)/len(tifa_score)\n        average_tifa_score_list.append(average_tifa_score)\n\n    result['tifa_score'] = sum(average_tifa_score_list)/len(average_tifa_score_list)\n\n    self.results.append(result)\n</code></pre>"},{"location":"documentations/aigve/#aigve.ToyDataset","title":"<code>ToyDataset</code>","text":"<p>               Bases: <code>BaseDataset</code></p> <p>ToyDataset for testing.</p> <p>Parameters:</p> Name Type Description Default <code>data_root</code> <code>str</code> <p>Root directory for data.</p> <code>None</code> <code>ann_file</code> <code>str</code> <p>Annotation file path.</p> <code>''</code> <code>metainfo</code> <code>dict</code> <p>Metadata information.</p> <code>None</code> <code>data_prefix</code> <code>dict</code> <p>Prefix paths for different modalities.</p> <code>None</code> <code>pipeline</code> <code>List[Union[Callable, dict]]</code> <p>Data transformation pipeline.</p> <code>[]</code> <code>modality</code> <code>dict</code> <p>Specifies which modalities are used (video, text, image).</p> <code>dict(use_video=True, use_text=True, use_image=False)</code> <code>image_frame</code> <code>int</code> <p>Number of frames for images.</p> <code>None</code> Source code in <code>aigve/datasets/toy_dataset.py</code> <pre><code>@DATASETS.register_module()\nclass ToyDataset(BaseDataset):\n    \"\"\"ToyDataset for testing.\n\n    Args:\n        data_root (str, optional): Root directory for data.\n        ann_file (str): Annotation file path.\n        metainfo (dict, optional): Metadata information.\n        data_prefix (dict): Prefix paths for different modalities.\n        pipeline (List[Union[Callable, dict]]): Data transformation pipeline.\n        modality (dict): Specifies which modalities are used (video, text, image).\n        image_frame (int, optional): Number of frames for images.\n    \"\"\"\n\n    def __init__(self,\n                 data_root: Optional[str] = None,\n                 ann_file: str = '',\n                 metainfo: Optional[dict] = None,\n                 data_prefix: dict = None,\n                 pipeline: List[Union[Callable, dict]] = [],\n                 modality: dict = dict(use_video=True, use_text=True, use_image=False),\n                 image_frame: int = None,\n                 **kwargs) -&gt; None:\n        super().__init__(\n            data_root=data_root,\n            ann_file=ann_file,\n            metainfo=metainfo,\n            data_prefix=data_prefix,\n            pipeline=pipeline,\n            **kwargs\n        )\n        self.modality = modality\n        self.image_frame = image_frame\n        assert self.modality['use_video'] or self.modality['use_text'], (\n            'Please specify the `modality` (`use_video` '\n            f', `use_text`) for {self.__class__.__name__}')\n\n    def parse_data_info(self, raw_data_info: dict) -&gt; dict:\n        \"\"\"Parse raw data info.\"\"\"\n        info = {}\n        info['img_frame'] = None\n        if self.modality['use_text']:\n            info['prompt_gt'] = osp.join(self.data_prefix.get('video', ''), \n                                         raw_data_info['prompt_gt'])\n\n        if self.modality['use_video'] or self.modality['use_image']:\n            info['video_path_pd'] = osp.join(self.data_prefix.get('video', ''), \n                                     raw_data_info['video_path_pd'])\n            if self.modality['use_image']:\n                info['img_frame'] = self.image_frame\n\n        return info\n</code></pre>"},{"location":"documentations/aigve/#aigve.ToyDataset.parse_data_info","title":"<code>parse_data_info(raw_data_info)</code>","text":"<p>Parse raw data info.</p> Source code in <code>aigve/datasets/toy_dataset.py</code> <pre><code>def parse_data_info(self, raw_data_info: dict) -&gt; dict:\n    \"\"\"Parse raw data info.\"\"\"\n    info = {}\n    info['img_frame'] = None\n    if self.modality['use_text']:\n        info['prompt_gt'] = osp.join(self.data_prefix.get('video', ''), \n                                     raw_data_info['prompt_gt'])\n\n    if self.modality['use_video'] or self.modality['use_image']:\n        info['video_path_pd'] = osp.join(self.data_prefix.get('video', ''), \n                                 raw_data_info['video_path_pd'])\n        if self.modality['use_image']:\n            info['img_frame'] = self.image_frame\n\n    return info\n</code></pre>"},{"location":"documentations/aigve/#aigve.VIEEvalScore","title":"<code>VIEEvalScore</code>","text":"<p>               Bases: <code>BaseMetric</code></p> <p>Initialize the <code>VIEEvalScore</code> evaluator.</p> <p>Parameters:</p> Name Type Description Default <code>llm_backbone</code> <code>str</code> <p>The name of the LLM model used in the VIEEvalScore evaluator. Defaults to <code>got4o</code>.</p> <code>'gpt4o'</code> <code>api_key_path</code> <code>str</code> <p>The user's api key path to initialize LLM models provides by openai.</p> <code>'AIGVE_Tool/metrics/text_video_alignment/gpt_based/VIE/api_key.txt'</code> <code>task(str)</code> <p>The task the VIEEvalScore evaluator conducts. Defaults to ''t2v''.</p> required <p>Returns:</p> Type Description <p>None</p> Source code in <code>aigve/metrics/text_video_alignment/gpt_based/VIE/vie_eval.py</code> <pre><code>@METRICS.register_module()\nclass VIEEvalScore(BaseMetric):\n    \"\"\" Initialize the ``VIEEvalScore`` evaluator.\n\n    Args:\n            llm_backbone (str): The name of the LLM model used in the VIEEvalScore evaluator. Defaults to ``got4o``.\n            api_key_path (str): The user's api key path to initialize LLM models provides by openai.\n            task(str): The task the VIEEvalScore evaluator conducts. Defaults to ''t2v''.\n\n    Returns:\n            None\n    \"\"\"\n    def __init__(self,\n                 llm_backbone: str = \"gpt4o\",\n                 api_key_path: str = 'AIGVE_Tool/metrics/text_video_alignment/gpt_based/VIE/api_key.txt',\n                 task: str = 't2v',\n                 ):\n        super().__init__()\n\n        self.api_key_path = api_key_path\n        self.llm_backbone = llm_backbone\n        self.task = task\n\n        self.submodel_path = 'metrics/text_video_alignment/gpt_based/VIE'\n        if not submodule_exists(self.submodel_path):\n            add_git_submodule(\n                repo_url='https://github.com/TIGER-AI-Lab/VIEScore.git', \n                submodule_path=self.submodel_path\n            )  \n        self.submodel_path = 'metrics/text_video_alignment/gpt_based/dsg'\n        if not submodule_exists(self.submodel_path):\n            add_git_submodule(\n                repo_url='https://github.com/j-min/DSG.git', \n                submodule_path=self.submodel_path\n            )  \n        from .VIEScore.viescore import VIEScore \n        from .DSG.dsg.vqa_utils import MPLUG, InstructBLIP\n\n\n        self.vie_score = VIEScore(backbone=self.llm_backbone, task=self.task, key_path=self.api_key_path)\n\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\n    def process(self, data_batch: Sequence, data_samples: Sequence) -&gt; None:\n        \"\"\"VIEScore process\n        Process one batch of data samples and predictions. The processed\n        results should be stored in ``self.results``, which will be used to\n        compute the metrics when all batches have been processed.\n\n        Args:\n            data_batch (Sequence): A batch of data from the dataloader.\n            data_samples (Sequence): A batch of data samples that\n                contain annotations and predictions.\n        \"\"\"\n\n        result = dict()\n\n        input_prompts, input_videos = data_samples\n        bsz = len(input_prompts)\n\n        # Ensure prompt_input is a tensor\n        if isinstance(input_prompts, tuple):\n            input_prompts = list(input_prompts)\n\n        if isinstance(input_videos, tuple):\n            input_videos = list(input_videos)\n\n        average_vie_score_list = []\n        for input_prompt, input_video in zip(input_prompts, input_videos):\n            vie_score_list = []\n            for index, frame_path in enumerate(input_video):\n                pil_image = Image.open(frame_path)\n                score_list = self.vie_score.evaluate(pil_image, input_prompt)\n                sementics_score, quality_score, overall_score = score_list\n                vie_score_list.append(overall_score)\n            average_vie_score = sum(vie_score_list)/len(vie_score_list)\n            average_vie_score_list.append(average_vie_score)\n\n        result['vie_score'] = sum(average_vie_score_list)/len(average_vie_score_list)\n\n        self.results.append(result)\n\n\n    def compute_metrics(self, results: list) -&gt; Dict[str, float]:\n        \"\"\"Compute the metrics from processed results.\n\n        Args:\n            results (list): The processed results of each batch.\n\n        Returns:\n            Dict[str, float]: The computed metrics. The keys are the names of\n            the metrics, and the values are corresponding results.\n        \"\"\"\n        logger: MMLogger = MMLogger.get_current_instance()\n\n        vie_score_np = np.zeros(len(results))\n        for i, result in enumerate(results):\n            vie_score_np[i] = result['vie_score']\n\n        vie_score_np_mean = np.mean(vie_score_np) \n\n        print(\"Test results: vie score with dependency={:.4f}\"\n              .format(vie_score_np_mean))\n\n        return result\n</code></pre>"},{"location":"documentations/aigve/#aigve.VIEEvalScore.compute_metrics","title":"<code>compute_metrics(results)</code>","text":"<p>Compute the metrics from processed results.</p> <p>Parameters:</p> Name Type Description Default <code>results</code> <code>list</code> <p>The processed results of each batch.</p> required <p>Returns:</p> Type Description <code>Dict[str, float]</code> <p>Dict[str, float]: The computed metrics. The keys are the names of</p> <code>Dict[str, float]</code> <p>the metrics, and the values are corresponding results.</p> Source code in <code>aigve/metrics/text_video_alignment/gpt_based/VIE/vie_eval.py</code> <pre><code>def compute_metrics(self, results: list) -&gt; Dict[str, float]:\n    \"\"\"Compute the metrics from processed results.\n\n    Args:\n        results (list): The processed results of each batch.\n\n    Returns:\n        Dict[str, float]: The computed metrics. The keys are the names of\n        the metrics, and the values are corresponding results.\n    \"\"\"\n    logger: MMLogger = MMLogger.get_current_instance()\n\n    vie_score_np = np.zeros(len(results))\n    for i, result in enumerate(results):\n        vie_score_np[i] = result['vie_score']\n\n    vie_score_np_mean = np.mean(vie_score_np) \n\n    print(\"Test results: vie score with dependency={:.4f}\"\n          .format(vie_score_np_mean))\n\n    return result\n</code></pre>"},{"location":"documentations/aigve/#aigve.VIEEvalScore.process","title":"<code>process(data_batch, data_samples)</code>","text":"<p>VIEScore process Process one batch of data samples and predictions. The processed results should be stored in <code>self.results</code>, which will be used to compute the metrics when all batches have been processed.</p> <p>Parameters:</p> Name Type Description Default <code>data_batch</code> <code>Sequence</code> <p>A batch of data from the dataloader.</p> required <code>data_samples</code> <code>Sequence</code> <p>A batch of data samples that contain annotations and predictions.</p> required Source code in <code>aigve/metrics/text_video_alignment/gpt_based/VIE/vie_eval.py</code> <pre><code>def process(self, data_batch: Sequence, data_samples: Sequence) -&gt; None:\n    \"\"\"VIEScore process\n    Process one batch of data samples and predictions. The processed\n    results should be stored in ``self.results``, which will be used to\n    compute the metrics when all batches have been processed.\n\n    Args:\n        data_batch (Sequence): A batch of data from the dataloader.\n        data_samples (Sequence): A batch of data samples that\n            contain annotations and predictions.\n    \"\"\"\n\n    result = dict()\n\n    input_prompts, input_videos = data_samples\n    bsz = len(input_prompts)\n\n    # Ensure prompt_input is a tensor\n    if isinstance(input_prompts, tuple):\n        input_prompts = list(input_prompts)\n\n    if isinstance(input_videos, tuple):\n        input_videos = list(input_videos)\n\n    average_vie_score_list = []\n    for input_prompt, input_video in zip(input_prompts, input_videos):\n        vie_score_list = []\n        for index, frame_path in enumerate(input_video):\n            pil_image = Image.open(frame_path)\n            score_list = self.vie_score.evaluate(pil_image, input_prompt)\n            sementics_score, quality_score, overall_score = score_list\n            vie_score_list.append(overall_score)\n        average_vie_score = sum(vie_score_list)/len(vie_score_list)\n        average_vie_score_list.append(average_vie_score)\n\n    result['vie_score'] = sum(average_vie_score_list)/len(average_vie_score_list)\n\n    self.results.append(result)\n</code></pre>"},{"location":"documentations/aigve/#aigve.VideoPhy","title":"<code>VideoPhy</code>","text":"<p>               Bases: <code>BaseMetric</code></p> Source code in <code>aigve/metrics/multi_aspect_metrics/videophy/videophy_metric.py</code> <pre><code>@METRICS.register_module()\nclass VideoPhy(BaseMetric):\n    def __init__(self,\n                hf_token: str,\n                collect_device: Optional[Union[str, torch.device]] = None,\n                prefix: Optional[str] = None,\n                metric_path: str = None,\n                model_path: str = 'videophysics/videocon_physics',\n                datainfo_path: str = None,\n                test_index: int = None,\n                 **kwargs):\n\n        \"\"\"\n        This function is used to initialize the VideoPhy metric.\n\n        Args:\n            collect_device (str or torch.device): The device to use for collecting the data\n            prefix (str): The prefix to use for the metric name\n            metric_path (str): The path to the metric\n            model_path (str): The path to the model\n            datainfo_path (str): The path to the data info\n            test_index (int): The index of the test\n        \"\"\"\n\n        super().__init__(collect_device=collect_device, prefix=prefix)\n        # self.train_index = train_index\n        self.metric_path = metric_path\n        self.model_path = model_path\n        self.datainfo_path = datainfo_path\n        self.test_index = test_index\n        self.hf_token = hf_token\n        self.results = []\n\n        # self.submodule_path = './metrics/aigve'\n        # if not submodule_exists(self.submodule_path):\n        #     add_git_submodule(\n        #         repo_url='https://github.com/Hritikbansal/videophy.git',\n        #         submodule_path=self.submodule_path\n        #     )\n\n        self.tokenizer = LlamaTokenizer.from_pretrained(self.model_path, token=self.hf_token)\n        self.image_processor = MplugOwlImageProcessor.from_pretrained(self.model_path)\n        self.processor = MplugOwlProcessor(self.image_processor, self.tokenizer)\n        self.model = MplugOwlForConditionalGeneration.from_pretrained(\n            self.model_path,\n            torch_dtype=torch.bfloat16,\n        ).to('cuda')\n        self.model.eval()\n\n    def get_entail(self, logits, input_ids):\n        \"\"\"\n        This function is used to get the entailment scores.\n\n        Args:\n            logits (torch.Tensor): A tensor containing the logits\n            input_ids (torch.Tensor): A tensor containing the input IDs\n        \"\"\"\n        softmax = nn.Softmax(dim=2)\n        logits = softmax(logits)\n        token_id_yes = self.tokenizer.encode('Yes', add_special_tokens=False)[0]\n        token_id_no = self.tokenizer.encode('No', add_special_tokens=False)[0]\n        entailment = []\n        for j in range(len(logits)):\n            for i in range(len(input_ids[j])):\n                if input_ids[j][i] == self.tokenizer.pad_token_id:  # pad token if the answer is not present\n                    i = i - 1\n                    break\n                elif i == len(input_ids[j]) - 1:\n                    break\n            score = logits[j][i][token_id_yes] / (logits[j][i][token_id_yes] + logits[j][i][token_id_no])\n            entailment.append(score)\n        entailment = torch.stack(entailment)\n        return entailment\n\n    def get_logits(self, data_batch):\n        \"\"\"\n        This function is used to get the logits for each input in the data batch.\n\n        Args:\n            data_batch (dict): A dictionary containing the data batch\n        Returns:\n            logits (torch.Tensor): A tensor containing the logits for each input in the data batch\n        \"\"\"\n        # Iterate over each item in the data batch\n        for k, v in data_batch.items():\n            # Check if the item is a tensor\n            if torch.is_tensor(v):\n                # Convert float tensors to bfloat16\n                if v.dtype == torch.float:\n                    data_batch[k] = v.bfloat16()\n                # Move the tensor to the model's device (e.g., GPU)\n                data_batch[k] = data_batch[k].to(self.model.device)\n\n        # print(\"Data batch: \", data_batch.keys())\n        outputs = self.model(pixel_values=data_batch['pixel_values'], video_pixel_values=data_batch['video_pixel_values'],\n                        labels=None, \\\n                        num_images=data_batch['num_images'], num_videos=data_batch['num_videos'], input_ids=data_batch['input_ids'],\n                        non_padding_mask=data_batch['non_padding_mask'], \\\n                        non_media_mask=data_batch['non_media_mask'], prompt_mask=data_batch['prompt_mask'])\n        logits = outputs['logits']\n        return logits\n\n\n    def process(self, data_batch: Any, data_samples: Sequence[dict]) -&gt; None:\n        \"\"\"\n        This function is used to process the data batch and compute the metric.\n\n        Args:\n            data_batch (dict): A dictionary containing the data batch\n            data_samples (list): A list of dictionaries containing the data samples\n        \"\"\"\n        logits = self.get_logits(data_batch)\n        entails_scores =  self.get_entail(logits, data_batch['input_ids'])\n\n        self.results.extend(entails_scores.cpu().detach().to(torch.float32).numpy().tolist())\n        # self.results = entails_scores.cpu().detach().to(torch.float32).numpy().tolist()\n        # print(self.results)\n\n\n    def compute_metrics(self, results: list) -&gt; dict:\n        \"\"\"\n        This function is used to compute the metrics.\n\n        Args:\n            results (list): A list of results\n        \"\"\"\n        return {\n            'entailment': float(np.mean(results))\n        }\n</code></pre>"},{"location":"documentations/aigve/#aigve.VideoPhy.__init__","title":"<code>__init__(hf_token, collect_device=None, prefix=None, metric_path=None, model_path='videophysics/videocon_physics', datainfo_path=None, test_index=None, **kwargs)</code>","text":"<p>This function is used to initialize the VideoPhy metric.</p> <p>Parameters:</p> Name Type Description Default <code>collect_device</code> <code>str or device</code> <p>The device to use for collecting the data</p> <code>None</code> <code>prefix</code> <code>str</code> <p>The prefix to use for the metric name</p> <code>None</code> <code>metric_path</code> <code>str</code> <p>The path to the metric</p> <code>None</code> <code>model_path</code> <code>str</code> <p>The path to the model</p> <code>'videophysics/videocon_physics'</code> <code>datainfo_path</code> <code>str</code> <p>The path to the data info</p> <code>None</code> <code>test_index</code> <code>int</code> <p>The index of the test</p> <code>None</code> Source code in <code>aigve/metrics/multi_aspect_metrics/videophy/videophy_metric.py</code> <pre><code>def __init__(self,\n            hf_token: str,\n            collect_device: Optional[Union[str, torch.device]] = None,\n            prefix: Optional[str] = None,\n            metric_path: str = None,\n            model_path: str = 'videophysics/videocon_physics',\n            datainfo_path: str = None,\n            test_index: int = None,\n             **kwargs):\n\n    \"\"\"\n    This function is used to initialize the VideoPhy metric.\n\n    Args:\n        collect_device (str or torch.device): The device to use for collecting the data\n        prefix (str): The prefix to use for the metric name\n        metric_path (str): The path to the metric\n        model_path (str): The path to the model\n        datainfo_path (str): The path to the data info\n        test_index (int): The index of the test\n    \"\"\"\n\n    super().__init__(collect_device=collect_device, prefix=prefix)\n    # self.train_index = train_index\n    self.metric_path = metric_path\n    self.model_path = model_path\n    self.datainfo_path = datainfo_path\n    self.test_index = test_index\n    self.hf_token = hf_token\n    self.results = []\n\n    # self.submodule_path = './metrics/aigve'\n    # if not submodule_exists(self.submodule_path):\n    #     add_git_submodule(\n    #         repo_url='https://github.com/Hritikbansal/videophy.git',\n    #         submodule_path=self.submodule_path\n    #     )\n\n    self.tokenizer = LlamaTokenizer.from_pretrained(self.model_path, token=self.hf_token)\n    self.image_processor = MplugOwlImageProcessor.from_pretrained(self.model_path)\n    self.processor = MplugOwlProcessor(self.image_processor, self.tokenizer)\n    self.model = MplugOwlForConditionalGeneration.from_pretrained(\n        self.model_path,\n        torch_dtype=torch.bfloat16,\n    ).to('cuda')\n    self.model.eval()\n</code></pre>"},{"location":"documentations/aigve/#aigve.VideoPhy.compute_metrics","title":"<code>compute_metrics(results)</code>","text":"<p>This function is used to compute the metrics.</p> <p>Parameters:</p> Name Type Description Default <code>results</code> <code>list</code> <p>A list of results</p> required Source code in <code>aigve/metrics/multi_aspect_metrics/videophy/videophy_metric.py</code> <pre><code>def compute_metrics(self, results: list) -&gt; dict:\n    \"\"\"\n    This function is used to compute the metrics.\n\n    Args:\n        results (list): A list of results\n    \"\"\"\n    return {\n        'entailment': float(np.mean(results))\n    }\n</code></pre>"},{"location":"documentations/aigve/#aigve.VideoPhy.get_entail","title":"<code>get_entail(logits, input_ids)</code>","text":"<p>This function is used to get the entailment scores.</p> <p>Parameters:</p> Name Type Description Default <code>logits</code> <code>Tensor</code> <p>A tensor containing the logits</p> required <code>input_ids</code> <code>Tensor</code> <p>A tensor containing the input IDs</p> required Source code in <code>aigve/metrics/multi_aspect_metrics/videophy/videophy_metric.py</code> <pre><code>def get_entail(self, logits, input_ids):\n    \"\"\"\n    This function is used to get the entailment scores.\n\n    Args:\n        logits (torch.Tensor): A tensor containing the logits\n        input_ids (torch.Tensor): A tensor containing the input IDs\n    \"\"\"\n    softmax = nn.Softmax(dim=2)\n    logits = softmax(logits)\n    token_id_yes = self.tokenizer.encode('Yes', add_special_tokens=False)[0]\n    token_id_no = self.tokenizer.encode('No', add_special_tokens=False)[0]\n    entailment = []\n    for j in range(len(logits)):\n        for i in range(len(input_ids[j])):\n            if input_ids[j][i] == self.tokenizer.pad_token_id:  # pad token if the answer is not present\n                i = i - 1\n                break\n            elif i == len(input_ids[j]) - 1:\n                break\n        score = logits[j][i][token_id_yes] / (logits[j][i][token_id_yes] + logits[j][i][token_id_no])\n        entailment.append(score)\n    entailment = torch.stack(entailment)\n    return entailment\n</code></pre>"},{"location":"documentations/aigve/#aigve.VideoPhy.get_logits","title":"<code>get_logits(data_batch)</code>","text":"<p>This function is used to get the logits for each input in the data batch.</p> <p>Parameters:</p> Name Type Description Default <code>data_batch</code> <code>dict</code> <p>A dictionary containing the data batch</p> required <p>Returns:     logits (torch.Tensor): A tensor containing the logits for each input in the data batch</p> Source code in <code>aigve/metrics/multi_aspect_metrics/videophy/videophy_metric.py</code> <pre><code>def get_logits(self, data_batch):\n    \"\"\"\n    This function is used to get the logits for each input in the data batch.\n\n    Args:\n        data_batch (dict): A dictionary containing the data batch\n    Returns:\n        logits (torch.Tensor): A tensor containing the logits for each input in the data batch\n    \"\"\"\n    # Iterate over each item in the data batch\n    for k, v in data_batch.items():\n        # Check if the item is a tensor\n        if torch.is_tensor(v):\n            # Convert float tensors to bfloat16\n            if v.dtype == torch.float:\n                data_batch[k] = v.bfloat16()\n            # Move the tensor to the model's device (e.g., GPU)\n            data_batch[k] = data_batch[k].to(self.model.device)\n\n    # print(\"Data batch: \", data_batch.keys())\n    outputs = self.model(pixel_values=data_batch['pixel_values'], video_pixel_values=data_batch['video_pixel_values'],\n                    labels=None, \\\n                    num_images=data_batch['num_images'], num_videos=data_batch['num_videos'], input_ids=data_batch['input_ids'],\n                    non_padding_mask=data_batch['non_padding_mask'], \\\n                    non_media_mask=data_batch['non_media_mask'], prompt_mask=data_batch['prompt_mask'])\n    logits = outputs['logits']\n    return logits\n</code></pre>"},{"location":"documentations/aigve/#aigve.VideoPhy.process","title":"<code>process(data_batch, data_samples)</code>","text":"<p>This function is used to process the data batch and compute the metric.</p> <p>Parameters:</p> Name Type Description Default <code>data_batch</code> <code>dict</code> <p>A dictionary containing the data batch</p> required <code>data_samples</code> <code>list</code> <p>A list of dictionaries containing the data samples</p> required Source code in <code>aigve/metrics/multi_aspect_metrics/videophy/videophy_metric.py</code> <pre><code>def process(self, data_batch: Any, data_samples: Sequence[dict]) -&gt; None:\n    \"\"\"\n    This function is used to process the data batch and compute the metric.\n\n    Args:\n        data_batch (dict): A dictionary containing the data batch\n        data_samples (list): A list of dictionaries containing the data samples\n    \"\"\"\n    logits = self.get_logits(data_batch)\n    entails_scores =  self.get_entail(logits, data_batch['input_ids'])\n\n    self.results.extend(entails_scores.cpu().detach().to(torch.float32).numpy().tolist())\n</code></pre>"},{"location":"documentations/aigve/#aigve.VideoPhyDataset","title":"<code>VideoPhyDataset</code>","text":"<p>               Bases: <code>Dataset</code></p> Source code in <code>aigve/datasets/videophy_dataset.py</code> <pre><code>@DATASETS.register_module()\nclass VideoPhyDataset(Dataset):\n    def __init__(self, data_path, video_root_path, hf_token, tokenizer=None, processor=None, max_length=2048, media_tokens=['&lt;image&gt;', '&lt;|video|&gt;'], hf_checkpoint='videophysics/videocon_physics'):\n        \"\"\"\n        Args:\n            data_path (str): Path to the data folder, it should be a json file\n            tokenizer (Tokenizer): Tokenizer object\n            processor (Processor): Processor object\n            max_length (int): Maximum length of the input sequence\n            media_tokens (list): List of media tokens\n        \"\"\"\n        self.dataset = json.load(open(data_path))\n        self.video_root_path = video_root_path\n\n        self.hf_token = hf_token\n        self.hf_checkpoint = hf_checkpoint\n        self.max_length = max_length\n        self.media_tokens = {k: -int(i + 1) for i, k in enumerate(media_tokens)}\n        self.media_lengths = {'&lt;image&gt;': 1 + 64, '&lt;|video|&gt;': 1 + 64}\n        self.bucket = {}\n\n\n        # initialize tokenizer\n        if tokenizer is not None:\n            self.tokenizer = tokenizer\n        else:\n            self.tokenizer = LlamaTokenizer.from_pretrained(self.hf_checkpoint, token=self.hf_token)\n\n        MplugOwlImageProcessor, MplugOwlProcessor = lazy_import_mplug_owl()\n        self.image_processor = MplugOwlImageProcessor.from_pretrained(self.hf_checkpoint)\n        # initialize processor\n        if processor is not None:\n            self.processor = processor\n        else:\n            self.processor = MplugOwlProcessor(self.image_processor, self.tokenizer)\n\n    def __len__(self) -&gt; int:\n        \"\"\"\n        Returns:\n            int: Length of the dataset\n        \"\"\"\n        return self.dataset['metainfo']['length']\n\n    def __getitem__(self, idx):\n        \"\"\"\n        Args:\n            idx (int): Index of the dataset\n        Returns:\n            dict: Dictionary containing the video, text, video path and caption\n        \"\"\"\n        data = self.dataset['dataset_list'][idx]\n        videopath = os.path.join(self.video_root_path, data['video_path_pd'])\n        caption = data['prompt_gt']\n        # video_input = self.processor(videos=[videopath], num_frames=16, return_tensors='pt') # video_pixel_values\n        video_input = self.processor(videos=[videopath], num_frames=32, return_tensors='pt')  # video_pixel_values\n        text_input = self._extract_text_token_from_conversation(caption, self.max_length, idx)\n        item = {'video': video_input, 'text': text_input, 'videopath': videopath, 'caption': caption}\n        return item\n\n    def _extract_text_token_from_conversation(self, data, max_length, index):\n        \"\"\"\n        Extracts the text tokens from the conversation\n        Args:\n            data (str): Conversation\n            max_length (int): Maximum length of the input sequence\n            index (int): Index of the dataset\n        \"\"\"\n        # output enc_chunk\n        enc_chunk = []\n\n        if self.tokenizer.bos_token_id &gt; 0:\n            prompt_chunk = [self.tokenizer.bos_token_id]\n        else:\n            prompt_chunk = []\n\n        # conversation = data[\"completion\"]\n        conversation = data\n\n        # For Text only data\n        if all([media_token not in conversation for media_token in self.media_tokens.keys()]):\n            pattern = '|'.join(map(re.escape, ['AI: ', '\\nHuman: ']))\n            chunk_strs = re.split(f'({pattern})', conversation)\n            prompt_length = -1\n            stop_flag = False\n            for idx, chunk_str in enumerate(chunk_strs):\n                if idx == 0:\n                    enc_chunk = prompt_chunk + \\\n                                self.tokenizer(chunk_str, add_special_tokens=False)[\n                                    'input_ids']\n                    enc_length = len(enc_chunk)\n                    label_chunk = [0] * enc_length\n                else:\n                    if chunk_strs[idx - 1] == 'AI: ':\n                        curr_chunk = self.tokenizer(\n                            chunk_str, add_special_tokens=False)['input_ids']\n                        if enc_length + len(curr_chunk) &gt;= max_length:\n                            curr_chunk = curr_chunk[:max_length - enc_length]\n                            stop_flag = True\n                        curr_chunk += [self.tokenizer.eos_token_id]\n                        enc_length += len(curr_chunk)\n                        enc_chunk += curr_chunk\n                        label_chunk += [1] * len(curr_chunk)\n                    else:\n                        curr_chunk = self.tokenizer(\n                            chunk_str, add_special_tokens=False)['input_ids']\n                        if enc_length + len(curr_chunk) &gt;= max_length + 1:\n                            curr_chunk = curr_chunk[:max_length + 1 - enc_length]\n                            stop_flag = True\n                        enc_length += len(curr_chunk)\n                        enc_chunk += curr_chunk\n                        label_chunk += [0] * len(curr_chunk)\n                    if stop_flag:\n                        break\n\n        # For Image-Text Data\n        else:\n            enc_length = 0\n            prompt_length = -2\n            pattern = '|'.join(\n                map(re.escape, list(self.media_tokens.keys()) + ['AI: ', '\\nHuman: ']))\n            chunk_strs = re.split(f'({pattern})', conversation)\n            chunk_strs = [x for x in chunk_strs if len(x) &gt; 0]\n            for idx, chunk_str in enumerate(chunk_strs):\n                if enc_length &gt;= max_length + 1:\n                    break\n\n                if idx == 0:\n                    enc_chunk = prompt_chunk + \\\n                                self.tokenizer(chunk_str, add_special_tokens=False)[\n                                    'input_ids']\n                    enc_length = len(enc_chunk)\n                    label_chunk = [0] * enc_length\n                else:\n                    if chunk_str in self.media_tokens:\n                        # [CLS] + 256 + [EOS]\n                        if enc_length + self.media_lengths[chunk_str] &gt; max_length + 1:\n                            break\n                        else:\n                            enc_chunk += [self.media_tokens[chunk_str]\n                                          ] * self.media_lengths[chunk_str]\n                            enc_length += self.media_lengths[chunk_str]\n                            label_chunk += [0] * self.media_lengths[chunk_str]\n                    else:\n\n                        if chunk_strs[idx - 1] == 'AI: ':\n                            curr_chunk = self.tokenizer(\n                                chunk_str, add_special_tokens=False)['input_ids']\n                            if enc_length + len(curr_chunk) &gt;= max_length:\n                                curr_chunk = curr_chunk[:max_length - enc_length]\n                            curr_chunk += [self.tokenizer.eos_token_id]\n                            enc_length += len(curr_chunk)\n                            enc_chunk += curr_chunk\n                            label_chunk += [1] * len(curr_chunk)\n                        else:\n                            curr_chunk = self.tokenizer(\n                                chunk_str, add_special_tokens=False)['input_ids']\n                            if enc_length + len(curr_chunk) &gt;= max_length + 1:\n                                curr_chunk = curr_chunk[:max_length +\n                                                         1 - enc_length]\n                            enc_length += len(curr_chunk)\n                            enc_chunk += curr_chunk\n                            label_chunk += [0] * len(curr_chunk)\n\n        if enc_length &lt; max_length + 1:\n            padding_chunk = [self.tokenizer.pad_token_id] * \\\n                            (max_length + 1 - enc_length)\n            padding_length = len(padding_chunk)\n            label_chunk += [0] * (max_length + 1 - enc_length)\n            enc_chunk = enc_chunk + padding_chunk\n        else:\n            padding_length = 0\n\n        assert enc_length + padding_length == max_length + \\\n               1, (index, prompt_length, enc_length,\n                   padding_length, max_length + 1)\n        assert len(label_chunk) == max_length + \\\n               1, (len(label_chunk), max_length + 1)\n        non_padding_mask = [1 if i &lt; enc_length -\n                                 1 else 0 for i in range(max_length)]\n\n        enc_chunk = torch.tensor(enc_chunk).long()\n        non_padding_mask = torch.tensor(non_padding_mask).long()\n        prompt_mask = torch.tensor(label_chunk)[1:].long()\n        prompt_length = torch.tensor([prompt_length]).long()\n\n        # Create loss mask\n        if all([media_token not in conversation for media_token in self.media_tokens.keys()]):\n            non_media_mask = torch.ones_like(non_padding_mask).long()\n        else:\n            tmp_enc_chunk = enc_chunk.clone()\n            tmp_enc_chunk[tmp_enc_chunk &gt;= 0] = 1\n            tmp_enc_chunk[tmp_enc_chunk &lt; 0] = 0\n            non_media_mask = torch.tensor(tmp_enc_chunk).long()\n            non_media_mask = non_media_mask[1:].long()\n        return {'input_ids': enc_chunk, \"prompt_length\": prompt_length, 'seq_length': enc_length,\n                \"non_padding_mask\": non_padding_mask, 'non_media_mask': non_media_mask, 'prompt_mask': prompt_mask}\n</code></pre>"},{"location":"documentations/aigve/#aigve.VideoPhyDataset.__getitem__","title":"<code>__getitem__(idx)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>idx</code> <code>int</code> <p>Index of the dataset</p> required <p>Returns:     dict: Dictionary containing the video, text, video path and caption</p> Source code in <code>aigve/datasets/videophy_dataset.py</code> <pre><code>def __getitem__(self, idx):\n    \"\"\"\n    Args:\n        idx (int): Index of the dataset\n    Returns:\n        dict: Dictionary containing the video, text, video path and caption\n    \"\"\"\n    data = self.dataset['dataset_list'][idx]\n    videopath = os.path.join(self.video_root_path, data['video_path_pd'])\n    caption = data['prompt_gt']\n    # video_input = self.processor(videos=[videopath], num_frames=16, return_tensors='pt') # video_pixel_values\n    video_input = self.processor(videos=[videopath], num_frames=32, return_tensors='pt')  # video_pixel_values\n    text_input = self._extract_text_token_from_conversation(caption, self.max_length, idx)\n    item = {'video': video_input, 'text': text_input, 'videopath': videopath, 'caption': caption}\n    return item\n</code></pre>"},{"location":"documentations/aigve/#aigve.VideoPhyDataset.__init__","title":"<code>__init__(data_path, video_root_path, hf_token, tokenizer=None, processor=None, max_length=2048, media_tokens=['&lt;image&gt;', '&lt;|video|&gt;'], hf_checkpoint='videophysics/videocon_physics')</code>","text":"<p>Parameters:</p> Name Type Description Default <code>data_path</code> <code>str</code> <p>Path to the data folder, it should be a json file</p> required <code>tokenizer</code> <code>Tokenizer</code> <p>Tokenizer object</p> <code>None</code> <code>processor</code> <code>Processor</code> <p>Processor object</p> <code>None</code> <code>max_length</code> <code>int</code> <p>Maximum length of the input sequence</p> <code>2048</code> <code>media_tokens</code> <code>list</code> <p>List of media tokens</p> <code>['&lt;image&gt;', '&lt;|video|&gt;']</code> Source code in <code>aigve/datasets/videophy_dataset.py</code> <pre><code>def __init__(self, data_path, video_root_path, hf_token, tokenizer=None, processor=None, max_length=2048, media_tokens=['&lt;image&gt;', '&lt;|video|&gt;'], hf_checkpoint='videophysics/videocon_physics'):\n    \"\"\"\n    Args:\n        data_path (str): Path to the data folder, it should be a json file\n        tokenizer (Tokenizer): Tokenizer object\n        processor (Processor): Processor object\n        max_length (int): Maximum length of the input sequence\n        media_tokens (list): List of media tokens\n    \"\"\"\n    self.dataset = json.load(open(data_path))\n    self.video_root_path = video_root_path\n\n    self.hf_token = hf_token\n    self.hf_checkpoint = hf_checkpoint\n    self.max_length = max_length\n    self.media_tokens = {k: -int(i + 1) for i, k in enumerate(media_tokens)}\n    self.media_lengths = {'&lt;image&gt;': 1 + 64, '&lt;|video|&gt;': 1 + 64}\n    self.bucket = {}\n\n\n    # initialize tokenizer\n    if tokenizer is not None:\n        self.tokenizer = tokenizer\n    else:\n        self.tokenizer = LlamaTokenizer.from_pretrained(self.hf_checkpoint, token=self.hf_token)\n\n    MplugOwlImageProcessor, MplugOwlProcessor = lazy_import_mplug_owl()\n    self.image_processor = MplugOwlImageProcessor.from_pretrained(self.hf_checkpoint)\n    # initialize processor\n    if processor is not None:\n        self.processor = processor\n    else:\n        self.processor = MplugOwlProcessor(self.image_processor, self.tokenizer)\n</code></pre>"},{"location":"documentations/aigve/#aigve.VideoPhyDataset.__len__","title":"<code>__len__()</code>","text":"<p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>Length of the dataset</p> Source code in <code>aigve/datasets/videophy_dataset.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"\n    Returns:\n        int: Length of the dataset\n    \"\"\"\n    return self.dataset['metainfo']['length']\n</code></pre>"},{"location":"documentations/aigve/#aigve.VideoPhyDataset._extract_text_token_from_conversation","title":"<code>_extract_text_token_from_conversation(data, max_length, index)</code>","text":"<p>Extracts the text tokens from the conversation Args:     data (str): Conversation     max_length (int): Maximum length of the input sequence     index (int): Index of the dataset</p> Source code in <code>aigve/datasets/videophy_dataset.py</code> <pre><code>def _extract_text_token_from_conversation(self, data, max_length, index):\n    \"\"\"\n    Extracts the text tokens from the conversation\n    Args:\n        data (str): Conversation\n        max_length (int): Maximum length of the input sequence\n        index (int): Index of the dataset\n    \"\"\"\n    # output enc_chunk\n    enc_chunk = []\n\n    if self.tokenizer.bos_token_id &gt; 0:\n        prompt_chunk = [self.tokenizer.bos_token_id]\n    else:\n        prompt_chunk = []\n\n    # conversation = data[\"completion\"]\n    conversation = data\n\n    # For Text only data\n    if all([media_token not in conversation for media_token in self.media_tokens.keys()]):\n        pattern = '|'.join(map(re.escape, ['AI: ', '\\nHuman: ']))\n        chunk_strs = re.split(f'({pattern})', conversation)\n        prompt_length = -1\n        stop_flag = False\n        for idx, chunk_str in enumerate(chunk_strs):\n            if idx == 0:\n                enc_chunk = prompt_chunk + \\\n                            self.tokenizer(chunk_str, add_special_tokens=False)[\n                                'input_ids']\n                enc_length = len(enc_chunk)\n                label_chunk = [0] * enc_length\n            else:\n                if chunk_strs[idx - 1] == 'AI: ':\n                    curr_chunk = self.tokenizer(\n                        chunk_str, add_special_tokens=False)['input_ids']\n                    if enc_length + len(curr_chunk) &gt;= max_length:\n                        curr_chunk = curr_chunk[:max_length - enc_length]\n                        stop_flag = True\n                    curr_chunk += [self.tokenizer.eos_token_id]\n                    enc_length += len(curr_chunk)\n                    enc_chunk += curr_chunk\n                    label_chunk += [1] * len(curr_chunk)\n                else:\n                    curr_chunk = self.tokenizer(\n                        chunk_str, add_special_tokens=False)['input_ids']\n                    if enc_length + len(curr_chunk) &gt;= max_length + 1:\n                        curr_chunk = curr_chunk[:max_length + 1 - enc_length]\n                        stop_flag = True\n                    enc_length += len(curr_chunk)\n                    enc_chunk += curr_chunk\n                    label_chunk += [0] * len(curr_chunk)\n                if stop_flag:\n                    break\n\n    # For Image-Text Data\n    else:\n        enc_length = 0\n        prompt_length = -2\n        pattern = '|'.join(\n            map(re.escape, list(self.media_tokens.keys()) + ['AI: ', '\\nHuman: ']))\n        chunk_strs = re.split(f'({pattern})', conversation)\n        chunk_strs = [x for x in chunk_strs if len(x) &gt; 0]\n        for idx, chunk_str in enumerate(chunk_strs):\n            if enc_length &gt;= max_length + 1:\n                break\n\n            if idx == 0:\n                enc_chunk = prompt_chunk + \\\n                            self.tokenizer(chunk_str, add_special_tokens=False)[\n                                'input_ids']\n                enc_length = len(enc_chunk)\n                label_chunk = [0] * enc_length\n            else:\n                if chunk_str in self.media_tokens:\n                    # [CLS] + 256 + [EOS]\n                    if enc_length + self.media_lengths[chunk_str] &gt; max_length + 1:\n                        break\n                    else:\n                        enc_chunk += [self.media_tokens[chunk_str]\n                                      ] * self.media_lengths[chunk_str]\n                        enc_length += self.media_lengths[chunk_str]\n                        label_chunk += [0] * self.media_lengths[chunk_str]\n                else:\n\n                    if chunk_strs[idx - 1] == 'AI: ':\n                        curr_chunk = self.tokenizer(\n                            chunk_str, add_special_tokens=False)['input_ids']\n                        if enc_length + len(curr_chunk) &gt;= max_length:\n                            curr_chunk = curr_chunk[:max_length - enc_length]\n                        curr_chunk += [self.tokenizer.eos_token_id]\n                        enc_length += len(curr_chunk)\n                        enc_chunk += curr_chunk\n                        label_chunk += [1] * len(curr_chunk)\n                    else:\n                        curr_chunk = self.tokenizer(\n                            chunk_str, add_special_tokens=False)['input_ids']\n                        if enc_length + len(curr_chunk) &gt;= max_length + 1:\n                            curr_chunk = curr_chunk[:max_length +\n                                                     1 - enc_length]\n                        enc_length += len(curr_chunk)\n                        enc_chunk += curr_chunk\n                        label_chunk += [0] * len(curr_chunk)\n\n    if enc_length &lt; max_length + 1:\n        padding_chunk = [self.tokenizer.pad_token_id] * \\\n                        (max_length + 1 - enc_length)\n        padding_length = len(padding_chunk)\n        label_chunk += [0] * (max_length + 1 - enc_length)\n        enc_chunk = enc_chunk + padding_chunk\n    else:\n        padding_length = 0\n\n    assert enc_length + padding_length == max_length + \\\n           1, (index, prompt_length, enc_length,\n               padding_length, max_length + 1)\n    assert len(label_chunk) == max_length + \\\n           1, (len(label_chunk), max_length + 1)\n    non_padding_mask = [1 if i &lt; enc_length -\n                             1 else 0 for i in range(max_length)]\n\n    enc_chunk = torch.tensor(enc_chunk).long()\n    non_padding_mask = torch.tensor(non_padding_mask).long()\n    prompt_mask = torch.tensor(label_chunk)[1:].long()\n    prompt_length = torch.tensor([prompt_length]).long()\n\n    # Create loss mask\n    if all([media_token not in conversation for media_token in self.media_tokens.keys()]):\n        non_media_mask = torch.ones_like(non_padding_mask).long()\n    else:\n        tmp_enc_chunk = enc_chunk.clone()\n        tmp_enc_chunk[tmp_enc_chunk &gt;= 0] = 1\n        tmp_enc_chunk[tmp_enc_chunk &lt; 0] = 0\n        non_media_mask = torch.tensor(tmp_enc_chunk).long()\n        non_media_mask = non_media_mask[1:].long()\n    return {'input_ids': enc_chunk, \"prompt_length\": prompt_length, 'seq_length': enc_length,\n            \"non_padding_mask\": non_padding_mask, 'non_media_mask': non_media_mask, 'prompt_mask': prompt_mask}\n</code></pre>"},{"location":"documentations/aigve/#aigve.VideoScore","title":"<code>VideoScore</code>","text":"<p>               Bases: <code>BaseMetric</code></p> Source code in <code>aigve/metrics/multi_aspect_metrics/videoscore/videoscore_metric.py</code> <pre><code>@METRICS.register_module()\nclass VideoScore(BaseMetric):\n    def __init__(self,\n                collect_device: Optional[Union[str, torch.device]] = None,\n                prefix: Optional[str] = None,\n                metric_path: str = None,\n                model_path: str = 'TIGER-Lab/VideoScore-v1.1',\n                datainfo_path: str = None,\n                test_index: int = None,\n                 **kwargs):\n        \"\"\"\n        Args:\n            collect_device (Optional[Union[str, torch.device]]): The device to collect the data on.\n            prefix (Optional[str]): The prefix to use for the metric.\n            metric_path (str): The path to the metric file.\n            model_path (str): The path to the model file.\n            datainfo_path (str): The path to the datainfo file.\n            test_index (int): The index of the test data.\n        \"\"\"\n        super().__init__(collect_device=collect_device, prefix=prefix)\n        # self.train_index = train_index\n        # TODO: ARE THERE PARAMETERS REQUIRED FOR THIS METRIC?\n        self.metric_path = metric_path\n        self.model_path = model_path\n        self.datainfo_path = datainfo_path\n        self.test_index = test_index\n\n\n        self.model = Idefics2ForSequenceClassification.from_pretrained(self.model_path, torch_dtype=torch.bfloat16).eval()\n        self.device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n        self.model.to(self.device)\n\n        self.results = []\n\n    def process(self, data_batch: Any, data_samples: Sequence[dict]) -&gt; None:\n        \"\"\"\n        Args:\n            data_batch (Any): The data batch to process.\n            data_samples (Sequence[dict]): The data samples to process.\n        \"\"\"\n\n\n        data_batch = {k: v[0].to(self.model.device) for k, v in data_batch.items()}\n\n        with torch.no_grad():\n            outputs = self.model(**data_batch)\n\n        logits = outputs.logits.cpu().detach().to(torch.float32).numpy()\n        num_aspects = logits.shape[-1]\n\n        aspect_scores = []\n        for i in range(num_aspects):\n            aspect_scores.append(round(logits[0, i].item(), 3))\n\n        self.results.append(aspect_scores)\n\n    def compute_metrics(self, results: list) -&gt; dict:\n        \"\"\"\n        Args:\n            results (list): The results to compute the metrics from.\n        \"\"\"\n        results = np.array(results)\n        mean_scores = np.mean(results, axis=1)\n\n        return {'visual_quailty': results[:, 0].tolist(),\n                'temporal_consistency': results[:, 1].tolist(),\n                'dynamic_degree': results[:, 2].tolist(),\n                'text-to-video_alignment': results[:, 3].tolist(),\n                'factual_consistency': results[:, 4].tolist(),\n                'summary': {'visual_quality': mean_scores[0], 'temporal_consistency': mean_scores[1],\n                            'dynamic_degree': mean_scores[2], 'text-to-video_alignment': mean_scores[3],\n                            'factual_consistency': mean_scores[4]}}\n</code></pre>"},{"location":"documentations/aigve/#aigve.VideoScore.__init__","title":"<code>__init__(collect_device=None, prefix=None, metric_path=None, model_path='TIGER-Lab/VideoScore-v1.1', datainfo_path=None, test_index=None, **kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>collect_device</code> <code>Optional[Union[str, device]]</code> <p>The device to collect the data on.</p> <code>None</code> <code>prefix</code> <code>Optional[str]</code> <p>The prefix to use for the metric.</p> <code>None</code> <code>metric_path</code> <code>str</code> <p>The path to the metric file.</p> <code>None</code> <code>model_path</code> <code>str</code> <p>The path to the model file.</p> <code>'TIGER-Lab/VideoScore-v1.1'</code> <code>datainfo_path</code> <code>str</code> <p>The path to the datainfo file.</p> <code>None</code> <code>test_index</code> <code>int</code> <p>The index of the test data.</p> <code>None</code> Source code in <code>aigve/metrics/multi_aspect_metrics/videoscore/videoscore_metric.py</code> <pre><code>def __init__(self,\n            collect_device: Optional[Union[str, torch.device]] = None,\n            prefix: Optional[str] = None,\n            metric_path: str = None,\n            model_path: str = 'TIGER-Lab/VideoScore-v1.1',\n            datainfo_path: str = None,\n            test_index: int = None,\n             **kwargs):\n    \"\"\"\n    Args:\n        collect_device (Optional[Union[str, torch.device]]): The device to collect the data on.\n        prefix (Optional[str]): The prefix to use for the metric.\n        metric_path (str): The path to the metric file.\n        model_path (str): The path to the model file.\n        datainfo_path (str): The path to the datainfo file.\n        test_index (int): The index of the test data.\n    \"\"\"\n    super().__init__(collect_device=collect_device, prefix=prefix)\n    # self.train_index = train_index\n    # TODO: ARE THERE PARAMETERS REQUIRED FOR THIS METRIC?\n    self.metric_path = metric_path\n    self.model_path = model_path\n    self.datainfo_path = datainfo_path\n    self.test_index = test_index\n\n\n    self.model = Idefics2ForSequenceClassification.from_pretrained(self.model_path, torch_dtype=torch.bfloat16).eval()\n    self.device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n    self.model.to(self.device)\n\n    self.results = []\n</code></pre>"},{"location":"documentations/aigve/#aigve.VideoScore.compute_metrics","title":"<code>compute_metrics(results)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>results</code> <code>list</code> <p>The results to compute the metrics from.</p> required Source code in <code>aigve/metrics/multi_aspect_metrics/videoscore/videoscore_metric.py</code> <pre><code>def compute_metrics(self, results: list) -&gt; dict:\n    \"\"\"\n    Args:\n        results (list): The results to compute the metrics from.\n    \"\"\"\n    results = np.array(results)\n    mean_scores = np.mean(results, axis=1)\n\n    return {'visual_quailty': results[:, 0].tolist(),\n            'temporal_consistency': results[:, 1].tolist(),\n            'dynamic_degree': results[:, 2].tolist(),\n            'text-to-video_alignment': results[:, 3].tolist(),\n            'factual_consistency': results[:, 4].tolist(),\n            'summary': {'visual_quality': mean_scores[0], 'temporal_consistency': mean_scores[1],\n                        'dynamic_degree': mean_scores[2], 'text-to-video_alignment': mean_scores[3],\n                        'factual_consistency': mean_scores[4]}}\n</code></pre>"},{"location":"documentations/aigve/#aigve.VideoScore.process","title":"<code>process(data_batch, data_samples)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>data_batch</code> <code>Any</code> <p>The data batch to process.</p> required <code>data_samples</code> <code>Sequence[dict]</code> <p>The data samples to process.</p> required Source code in <code>aigve/metrics/multi_aspect_metrics/videoscore/videoscore_metric.py</code> <pre><code>def process(self, data_batch: Any, data_samples: Sequence[dict]) -&gt; None:\n    \"\"\"\n    Args:\n        data_batch (Any): The data batch to process.\n        data_samples (Sequence[dict]): The data samples to process.\n    \"\"\"\n\n\n    data_batch = {k: v[0].to(self.model.device) for k, v in data_batch.items()}\n\n    with torch.no_grad():\n        outputs = self.model(**data_batch)\n\n    logits = outputs.logits.cpu().detach().to(torch.float32).numpy()\n    num_aspects = logits.shape[-1]\n\n    aspect_scores = []\n    for i in range(num_aspects):\n        aspect_scores.append(round(logits[0, i].item(), 3))\n\n    self.results.append(aspect_scores)\n</code></pre>"},{"location":"documentations/aigve/#organization-of-this-library","title":"Organization of this Library","text":"<ul> <li>aigve.configs</li> <li>aigve.core</li> <li>aigve.datasets</li> <li>aigve.metrics</li> <li>aigve.utils</li> </ul>"},{"location":"documentations/configs/","title":"aigve.configs","text":"<p>In <code>AIGVE</code>, configuration management is handled using MMEngine's configuration  system, which provides a modular, hierarchical, and flexible approach to defining  experiment settings. The config system allows users to efficiently configure  video evaluation metrics, datasets, dataloaders, etc., making  benchmarking and experimentation more streamlined in a structured manner.</p>"},{"location":"documentations/configs/#aigve.configs--key-features-of-aigve-config-system","title":"Key Features of AIGVE Config System","text":"<ul> <li>Modular Design: Uses <code>_base_</code> configurations to reduce redundancy.</li> <li>Customizable Pipelines: Define different evaluation metrics and datasets easily.</li> <li>Flexible Overriding: Modify parameters dynamically via command-line arguments.</li> <li>Scalability: Supports large-scale video evaluation with efficient data loading.</li> </ul>"},{"location":"documentations/configs/#aigve.configs--aigve-configuration-example","title":"AIGVE Configuration Example","text":"<p>AIGVE uses structured configuration files to define evaluation settings.  Below is an example of a CLIPSim metric configuration file:</p> <pre><code># Copyright (c) IFM Lab. All rights reserved.\nfrom mmengine.config import read_base\nfrom metrics.text_video_alignment.similarity_based import CLIPSimScore\n\nwith read_base():\n    from ._base_.datasets.clipsim_dataset import *\n    from ._base_.default import *\n\nval_evaluator = dict(\n    type=CLIPSimScore,\n    model_name='openai/clip-vit-base-patch32',\n    logit_scale=False,\n)\n\nval_dataloader = dict(\n    batch_size=2, \n    num_workers=2,\n    persistent_workers=True,\n    drop_last=False,\n    sampler=dict(type=DefaultSampler, shuffle=False),\n    dataset=dict(\n        type=CLIPSimDataset,\n        processor_name='openai/clip-vit-base-patch32',\n        video_dir='AIGVE_Tool/data/toy/evaluate/',\n        prompt_dir='AIGVE_Tool/data/toy/annotations/evaluate.json',\n    )\n)\n</code></pre>"},{"location":"documentations/core/","title":"aigve.core","text":""},{"location":"documentations/core/#aigve.core.VQALoop","title":"<code>VQALoop</code>","text":"<p>               Bases: <code>BaseLoop</code></p> <p>Loop for VQA metric evaluation.</p> <p>Parameters:</p> Name Type Description Default <code>runner</code> <code>Runner</code> <p>A reference of runner.</p> required <code>dataloader</code> <code>Dataloader or dict</code> <p>A dataloader object or a dict to build a dataloader.</p> required <code>evaluator</code> <code>Evaluator or dict or list</code> <p>Used for computing metrics.</p> required <code>fp16</code> <code>bool</code> <p>Whether to enable fp16 validation. Defaults to False.</p> <code>False</code> Source code in <code>aigve/core/loops.py</code> <pre><code>@LOOPS.register_module()\nclass VQALoop(BaseLoop):\n    \"\"\"Loop for VQA metric evaluation.\n\n    Args:\n        runner (Runner): A reference of runner.\n        dataloader (Dataloader or dict): A dataloader object or a dict to\n            build a dataloader.\n        evaluator (Evaluator or dict or list): Used for computing metrics.\n        fp16 (bool): Whether to enable fp16 validation. Defaults to\n            False.\n    \"\"\"\n\n    def __init__(self,\n                 runner,\n                 dataloader: Union[DataLoader, Dict],\n                 evaluator: Union[Evaluator, Dict, List],\n                 fp16: bool = False) -&gt; None:\n        super().__init__(runner, dataloader)\n        if isinstance(evaluator, (dict, list)):\n            self.evaluator = runner.build_evaluator(evaluator)  # type: ignore\n        else:\n            assert isinstance(evaluator, Evaluator), (\n                'evaluator must be one of dict, list or Evaluator instance, '\n                f'but got {type(evaluator)}.')\n            self.evaluator = evaluator  # type: ignore\n        if hasattr(self.dataloader.dataset, 'metainfo'):\n            self.evaluator.dataset_meta = self.dataloader.dataset.metainfo\n            self.runner.visualizer.dataset_meta = \\\n                self.dataloader.dataset.metainfo\n        else:\n            print_log(\n                f'Dataset {self.dataloader.dataset.__class__.__name__} has no '\n                'metainfo. ``dataset_meta`` in evaluator, metric and '\n                'visualizer will be None.',\n                logger='current',\n                level=logging.WARNING)\n        self.fp16 = fp16\n        self.val_loss: Dict[str, HistoryBuffer] = dict()\n\n    def run(self) -&gt; dict:\n        \"\"\"Launch validation.\"\"\"\n        self.runner.call_hook('before_val')\n        self.runner.call_hook('before_val_epoch')\n        self.runner.model.eval()\n\n        # clear val loss\n        # self.val_loss.clear()\n        for idx, data_batch in enumerate(self.dataloader):\n            self.run_iter(idx, data_batch)\n\n        # compute metrics\n        metrics = self.evaluator.evaluate(len(self.dataloader.dataset))\n\n        # if self.val_loss:\n        #     loss_dict = _parse_losses(self.val_loss, 'val')\n        #     metrics.update(loss_dict)\n\n        self.runner.call_hook('after_val_epoch', metrics=metrics)\n        self.runner.call_hook('after_val')\n        return metrics\n\n    def run_iter(self, idx, data_batch: Sequence[dict]):\n        \"\"\"Iterate one mini-batch.\n\n        Args:\n            data_batch (Sequence[dict]): Batch of data\n                from dataloader.\n        \"\"\"\n        self.runner.call_hook(\n            'before_val_iter', batch_idx=idx, data_batch=data_batch)\n        # outputs should be sequence of BaseDataElement\n        # with autocast(enabled=self.fp16):\n        #     outputs = self.runner.model.val_step(data_batch)\n        outputs = data_batch\n\n        # outputs, self.val_loss = _update_losses(outputs, self.val_loss)\n\n        self.evaluator.process(data_batch=data_batch, data_samples=outputs)\n        self.runner.call_hook(\n            'after_val_iter',\n            batch_idx=idx,\n            data_batch=data_batch,\n            outputs=outputs)\n</code></pre>"},{"location":"documentations/core/#aigve.core.VQALoop.run","title":"<code>run()</code>","text":"<p>Launch validation.</p> Source code in <code>aigve/core/loops.py</code> <pre><code>def run(self) -&gt; dict:\n    \"\"\"Launch validation.\"\"\"\n    self.runner.call_hook('before_val')\n    self.runner.call_hook('before_val_epoch')\n    self.runner.model.eval()\n\n    # clear val loss\n    # self.val_loss.clear()\n    for idx, data_batch in enumerate(self.dataloader):\n        self.run_iter(idx, data_batch)\n\n    # compute metrics\n    metrics = self.evaluator.evaluate(len(self.dataloader.dataset))\n\n    # if self.val_loss:\n    #     loss_dict = _parse_losses(self.val_loss, 'val')\n    #     metrics.update(loss_dict)\n\n    self.runner.call_hook('after_val_epoch', metrics=metrics)\n    self.runner.call_hook('after_val')\n    return metrics\n</code></pre>"},{"location":"documentations/core/#aigve.core.VQALoop.run_iter","title":"<code>run_iter(idx, data_batch)</code>","text":"<p>Iterate one mini-batch.</p> <p>Parameters:</p> Name Type Description Default <code>data_batch</code> <code>Sequence[dict]</code> <p>Batch of data from dataloader.</p> required Source code in <code>aigve/core/loops.py</code> <pre><code>def run_iter(self, idx, data_batch: Sequence[dict]):\n    \"\"\"Iterate one mini-batch.\n\n    Args:\n        data_batch (Sequence[dict]): Batch of data\n            from dataloader.\n    \"\"\"\n    self.runner.call_hook(\n        'before_val_iter', batch_idx=idx, data_batch=data_batch)\n    # outputs should be sequence of BaseDataElement\n    # with autocast(enabled=self.fp16):\n    #     outputs = self.runner.model.val_step(data_batch)\n    outputs = data_batch\n\n    # outputs, self.val_loss = _update_losses(outputs, self.val_loss)\n\n    self.evaluator.process(data_batch=data_batch, data_samples=outputs)\n    self.runner.call_hook(\n        'after_val_iter',\n        batch_idx=idx,\n        data_batch=data_batch,\n        outputs=outputs)\n</code></pre>"},{"location":"documentations/datasets/","title":"aigve.datasets","text":""},{"location":"documentations/datasets/#aigve.datasets.CLIPTempDataset","title":"<code>CLIPTempDataset</code>","text":"<p>               Bases: <code>Dataset</code></p> Source code in <code>aigve/datasets/cliptemp_dataset.py</code> <pre><code>@DATASETS.register_module()\nclass CLIPTempDataset(Dataset):\n    def __init__(self, processor_name, prompt_dir, video_dir):\n        super(CLIPTempDataset, self).__init__()\n        self.prompt_dir = prompt_dir\n        self.video_dir = video_dir\n        self.processor_name = processor_name\n\n        self.processor = AutoProcessor.from_pretrained(self.processor_name)\n        self.video_names = self._read_videoname()\n\n    def _read_videoname(self):\n        with open(self.prompt_dir, 'r') as reader:\n            read_data = json.load(reader)\n\n        video_name_list = []\n        for item in read_data[\"datset_list\"]:\n            video_name = item['video_path_pd'].strip()\n            video_name_list.append(video_name)\n\n        return video_name_list\n\n    def __len__(self):\n        return len(self.video_names)-1\n\n    def __getitem__(self, index):\n        '''return video frame pairs\n        '''\n        video_name = self.video_names[index]\n        video_path = self.video_dir + video_name\n        frames = []\n        cap = cv2.VideoCapture(video_path)\n        while cap.isOpened():\n            ret, frame = cap.read()\n            if not ret:\n                break\n            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n            resized_frame = cv2.resize(frame,(224,224))  # Resize the frame to match the expected input size\n            frames.append(resized_frame)\n\n        input_frame_tensor = self.processor(\n            images=frames,\n            padding=True,\n            truncation=True,\n            max_length=77,\n            return_tensors=\"pt\",\n        )['pixel_values']\n\n        return input_frame_tensor\n</code></pre>"},{"location":"documentations/datasets/#aigve.datasets.CLIPTempDataset.__getitem__","title":"<code>__getitem__(index)</code>","text":"<p>return video frame pairs</p> Source code in <code>aigve/datasets/cliptemp_dataset.py</code> <pre><code>def __getitem__(self, index):\n    '''return video frame pairs\n    '''\n    video_name = self.video_names[index]\n    video_path = self.video_dir + video_name\n    frames = []\n    cap = cv2.VideoCapture(video_path)\n    while cap.isOpened():\n        ret, frame = cap.read()\n        if not ret:\n            break\n        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n        resized_frame = cv2.resize(frame,(224,224))  # Resize the frame to match the expected input size\n        frames.append(resized_frame)\n\n    input_frame_tensor = self.processor(\n        images=frames,\n        padding=True,\n        truncation=True,\n        max_length=77,\n        return_tensors=\"pt\",\n    )['pixel_values']\n\n    return input_frame_tensor\n</code></pre>"},{"location":"documentations/datasets/#aigve.datasets.GSTVQADataset","title":"<code>GSTVQADataset</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>Dataset for GSTVQA metric, supports feature extraction using VGG16 or ResNet.</p> Source code in <code>aigve/datasets/gstvqa_dataset.py</code> <pre><code>@DATASETS.register_module()\nclass GSTVQADataset(Dataset):\n    \"\"\"Dataset for GSTVQA metric, supports feature extraction using VGG16 or ResNet.\"\"\"\n\n    def __init__(self, video_dir, prompt_dir, model_name='vgg16', max_len=500):\n        super(GSTVQADataset, self).__init__()\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self.video_dir = video_dir\n        self.prompt_dir = prompt_dir\n        self.model_name = model_name\n        self.max_len = max_len\n        self.feature_extractor = FeatureExtractor(model_name=model_name)\n\n        self.prompts, self.video_names = self._read_prompt_videoname()\n\n    def _read_prompt_videoname(self):\n        with open(self.prompt_dir, 'r') as reader:\n            read_data = json.load(reader)\n\n        prompt_data_list, video_name_list = [], []\n        for item in read_data[\"data_list\"]:\n            prompt = item['prompt_gt'].strip()\n            video_name = item['video_path_pd'].strip()\n            prompt_data_list.append(prompt)\n            video_name_list.append(video_name)\n\n        return prompt_data_list, video_name_list\n\n    def __len__(self):\n        return len(self.prompts)\n\n    def __getitem__(self, index):\n        \"\"\"\n        Returns a tuple of:\n            deep_features (torch.Tensor): Shape [max_len, 2944]\n                Mean and std features extracted from input frames using the chosen model (VGG16 or ResNet).\n                Padded to self.max_len if the number of frames is less.\n            num_frames (int): The number of frames in the video.\n            video_name (str): The file name for the video.\n        \"\"\"\n        video_name = self.video_names[index]\n        video_path = os.path.join(self.video_dir, video_name)\n        input_frames = []\n\n        cap = cv2.VideoCapture(video_path)\n        frame_count = 0\n\n        while cap.isOpened() and frame_count &lt; self.max_len:\n            ret, frame = cap.read()\n            if not ret:\n                break\n            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n            # frame = cv2.resize(frame, self.frame_size)\n            input_frames.append(torch.tensor(frame).float())\n            frame_count += 1\n\n        cap.release()\n\n        # Pad or truncate frames to max_len\n        num_frames = len(input_frames)\n        # print('num_frames: ', num_frames)\n        if num_frames &lt; 30:\n            pad_frames = torch.zeros((30 - num_frames, *input_frames[0].shape))\n            input_frames_tensor = torch.cat((torch.stack(input_frames), pad_frames), dim=0)\n            num_frames = 30 # Force min frames to be 30 (since two att_frams=15(kernel_size) used in GSTVQA)\n        elif num_frames &lt; self.max_len:\n            pad_frames = torch.zeros((self.max_len - num_frames, *input_frames[0].shape))\n            input_frames_tensor = torch.cat((torch.stack(input_frames), pad_frames), dim=0)\n        else:\n            input_frames_tensor = torch.stack(input_frames[:self.max_len])\n        # print('input_frames_tensor: ', input_frames_tensor.shape) # shape: toy data [max_len, H(512), W(512), C(3)]\n\n        # Convert from [T, H, W, C] to [T, C, H, W]\n        input_frames_tensor = input_frames_tensor.permute(0, 3, 1, 2) \n\n        # Extract features using the chosen model (VGG16 or ResNet)\n        with torch.no_grad():\n            mean_features, std_features = self.feature_extractor(input_frames_tensor) # Shape: [T, 1472]: [10, 1472]\n\n        # Concatenate to match GSTVQA expected 2944-dim features\n        deep_features = torch.cat((mean_features, std_features), dim=1)  # Shape: [T, 2944]\n\n        # Ensure output shape [max_len, 2944] (pad if needed)\n        if deep_features.shape[0] &lt; self.max_len:\n            pad_size = self.max_len - deep_features.shape[0]\n            padding = torch.zeros((pad_size, 2944), device=deep_features.device)\n            deep_features = torch.cat((deep_features, padding), dim=0)\n\n\n        return deep_features, num_frames, video_name\n</code></pre>"},{"location":"documentations/datasets/#aigve.datasets.GSTVQADataset.__getitem__","title":"<code>__getitem__(index)</code>","text":"Returns a tuple of <p>deep_features (torch.Tensor): Shape [max_len, 2944]     Mean and std features extracted from input frames using the chosen model (VGG16 or ResNet).     Padded to self.max_len if the number of frames is less. num_frames (int): The number of frames in the video. video_name (str): The file name for the video.</p> Source code in <code>aigve/datasets/gstvqa_dataset.py</code> <pre><code>def __getitem__(self, index):\n    \"\"\"\n    Returns a tuple of:\n        deep_features (torch.Tensor): Shape [max_len, 2944]\n            Mean and std features extracted from input frames using the chosen model (VGG16 or ResNet).\n            Padded to self.max_len if the number of frames is less.\n        num_frames (int): The number of frames in the video.\n        video_name (str): The file name for the video.\n    \"\"\"\n    video_name = self.video_names[index]\n    video_path = os.path.join(self.video_dir, video_name)\n    input_frames = []\n\n    cap = cv2.VideoCapture(video_path)\n    frame_count = 0\n\n    while cap.isOpened() and frame_count &lt; self.max_len:\n        ret, frame = cap.read()\n        if not ret:\n            break\n        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n        # frame = cv2.resize(frame, self.frame_size)\n        input_frames.append(torch.tensor(frame).float())\n        frame_count += 1\n\n    cap.release()\n\n    # Pad or truncate frames to max_len\n    num_frames = len(input_frames)\n    # print('num_frames: ', num_frames)\n    if num_frames &lt; 30:\n        pad_frames = torch.zeros((30 - num_frames, *input_frames[0].shape))\n        input_frames_tensor = torch.cat((torch.stack(input_frames), pad_frames), dim=0)\n        num_frames = 30 # Force min frames to be 30 (since two att_frams=15(kernel_size) used in GSTVQA)\n    elif num_frames &lt; self.max_len:\n        pad_frames = torch.zeros((self.max_len - num_frames, *input_frames[0].shape))\n        input_frames_tensor = torch.cat((torch.stack(input_frames), pad_frames), dim=0)\n    else:\n        input_frames_tensor = torch.stack(input_frames[:self.max_len])\n    # print('input_frames_tensor: ', input_frames_tensor.shape) # shape: toy data [max_len, H(512), W(512), C(3)]\n\n    # Convert from [T, H, W, C] to [T, C, H, W]\n    input_frames_tensor = input_frames_tensor.permute(0, 3, 1, 2) \n\n    # Extract features using the chosen model (VGG16 or ResNet)\n    with torch.no_grad():\n        mean_features, std_features = self.feature_extractor(input_frames_tensor) # Shape: [T, 1472]: [10, 1472]\n\n    # Concatenate to match GSTVQA expected 2944-dim features\n    deep_features = torch.cat((mean_features, std_features), dim=1)  # Shape: [T, 2944]\n\n    # Ensure output shape [max_len, 2944] (pad if needed)\n    if deep_features.shape[0] &lt; self.max_len:\n        pad_size = self.max_len - deep_features.shape[0]\n        padding = torch.zeros((pad_size, 2944), device=deep_features.device)\n        deep_features = torch.cat((deep_features, padding), dim=0)\n\n\n    return deep_features, num_frames, video_name\n</code></pre>"},{"location":"documentations/datasets/#aigve.datasets.LightVQAPlusDataset","title":"<code>LightVQAPlusDataset</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>Dataset for LightVQA+. Extracts:     - spatial_features (torch.Tensor): Extracted key frames.     - temporal_features (torch.Tensor): SlowFast motion features.     - BNS_features (torch.Tensor): Brightness &amp; Noise features.     - BC_features (torch.Tensor): Temporal CLIP-based brightness contrast features.     - video_name (str): Video filename.</p> Source code in <code>aigve/datasets/lightvqa_plus_dataset.py</code> <pre><code>@DATASETS.register_module()\nclass LightVQAPlusDataset(Dataset):\n    \"\"\"\n    Dataset for LightVQA+.\n    Extracts:\n        - spatial_features (torch.Tensor): Extracted key frames.\n        - temporal_features (torch.Tensor): SlowFast motion features.\n        - BNS_features (torch.Tensor): Brightness &amp; Noise features.\n        - BC_features (torch.Tensor): Temporal CLIP-based brightness contrast features.\n        - video_name (str): Video filename.\n    \"\"\"\n\n    def __init__(self, video_dir, prompt_dir, feature_dir=\"extracted_features\", min_video_seconds=8):\n        super(LightVQAPlusDataset, self).__init__()\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self.video_dir = video_dir\n        self.prompt_dir = prompt_dir\n        self.feature_dir = feature_dir\n        self.min_video_seconds = min_video_seconds\n        os.makedirs(self.feature_dir, exist_ok=True)\n\n        self.video_names = self._read_video_names()\n\n        # Load CLIP model for BNS and BC features\n        self.clip_model, _ = clip.load(\"ViT-B/32\", device=\"cpu\")\n        self.preprocess = transforms.Normalize(\n            (0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711)\n        )\n        self.to_tensor = transforms.ToTensor()\n\n        # CLIP text prompts\n        self.text_B = clip.tokenize([  # brightness (B)\n            \"an underexposed photo\", \"a slightly underexposed photo\",\n            \"a well-exposed photo\", \"a slightly overexposed photo\", \"an overexposed photo\"\n        ])\n\n        self.text_N = clip.tokenize([ # noise (N)\n            \"a photo with no noise\", \"a photo with little noise\",\n            \"a photo with considerable noise\", \"a photo with serious noise\", \"a photo with extreme noise\"\n        ])\n\n        self.submodel_path = os.path.join(os.getcwd(), 'metrics/video_quality_assessment/nn_based/lightvqa_plus')\n        if not submodule_exists(self.submodel_path):\n            add_git_submodule(\n                repo_url='https://github.com/SaMMyCHoo/Light-VQA-plus.git', \n                submodule_path=self.submodel_path\n            )\n        # original_path = os.path.join(self.submodel_path, \"Light-VQA-plus\")\n        lightvqa_path = os.path.join(self.submodel_path, \"Light_VQA_plus\")\n        # if os.path.exists(original_path) and not os.path.exists(lightvqa_path):\n        #     os.rename(original_path, lightvqa_path)\n        if lightvqa_path not in sys.path:\n            sys.path.insert(0, lightvqa_path)\n        # print(sys.path)\n\n        # Load SlowFast model\n\n        slowfast, _ = lazy_import()\n        self.slowfast_model = slowfast()\n\n    def _read_video_names(self):\n        \"\"\"Reads video names from the dataset JSON file.\"\"\"\n        with open(self.prompt_dir, 'r') as reader:\n            read_data = json.load(reader)\n        return [item['video_path_pd'].strip() for item in read_data[\"data_list\"]]\n\n    def __len__(self):\n        return len(self.video_names)\n\n    def extract_key_frames(self, video_path):\n        \"\"\"\n        Extracts 8 evenly spaced key frames across the entire video duration.\n\n        Args:\n            video_path (str): Path to the video file.\n\n        Returns:\n            spatial_features (torch.Tensor): Shape [8, 3, 672, 1120] containing 8 key frames.\n        \"\"\"\n        cap = cv2.VideoCapture(video_path)\n        video_name = os.path.basename(video_path).split('.')[0]\n\n        video_length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n\n        if video_length &gt;= 8:\n            # Select 8 unique frame indices evenly spaced across the entire video\n            frame_indices = np.round(np.linspace(0, video_length - 1, num=8)).astype(int)\n        else:\n            # Select all available frames and repeat the last one to reach 8\n            frame_indices = list(range(video_length)) + [video_length - 1] * (8 - video_length)\n\n        spatial_features = torch.zeros([8, 3, 672, 1120])  # Ensure exactly 8 frames\n        transform = transforms.Compose([\n            transforms.Resize([672, 1120]),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n        ])\n\n        last_valid_frame = None\n        for idx, frame_idx in enumerate(frame_indices):\n            cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)\n            ret, frame = cap.read()\n            if ret:\n                frame = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n                spatial_features[idx] = transform(frame)\n                last_valid_frame = spatial_features[idx]\n            elif last_valid_frame is not None:  # If total frames are less than 8, repeat the last valid frame\n                spatial_features[idx] = last_valid_frame\n\n        cap.release()\n        # print('spatial_features: ', spatial_features.shape) # torch.Size([8, 3, 672, 1120])\n        return spatial_features\n\n    def get_global_sf(self, video_path) -&gt; torch.Tensor:\n        \"\"\"Extracts global brightness &amp; noise features across full video.\n\n        Args:\n            video_path (str): Path to video file.\n\n        Returns:\n            torch.Tensor: Extracted global features (Shape: [8, 150]).\n        \"\"\"\n        cap = cv2.VideoCapture(video_path)\n        video_length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n        # print('video_length: ', video_length)  # 16\n\n        frames = []\n        for _ in range(video_length):\n            ret, frame = cap.read()\n            if ret:\n                frame = cv2.resize(frame, (1120, 672))\n                frames.append(frame)\n        cap.release()\n\n        if not frames:\n            raise ValueError(f\"Failed to extract frames from {video_path}\")\n\n        res = []\n        length = len(frames)\n        now = 0\n        interval = 10  # Process 10 frames at a time\n        while now + interval - 1 &lt; length:\n            final = [self.to_tensor(Image.fromarray(cv2.cvtColor(frames[i + now], cv2.COLOR_BGR2RGB)))\n                    for i in range(interval)]\n\n            # Step 1: Convert to tensor batch\n            images = torch.stack(final, dim=0)  # Shape: [10, 3, 672, 1120]\n\n            # Step 2: Unfold into patches (Strictly following GET_SF)\n            images = images.unfold(2, 224, 224).unfold(3, 224, 224)  # Shape: [10, 3, 3, 5, 224, 224]\n            images = images.permute(0, 3, 2, 1, 4, 5).contiguous()  # Shape: [10, 5, 3, 3, 224, 224]\n            images = images.reshape(-1, 15, 3, 224, 224)  # Shape: [10, 15, 3, 224, 224]\n            images = images.view(-1, 3, 224, 224)  # Shape: [150, 3, 224, 224]\n            images = self.preprocess(images)  # Normalize for CLIP\n            # print('images get_global_sf: ', images.shape) # torch.Size([10*15, 3, 224, 224])\n\n            # Step 3: Extract features using CLIP\n            with torch.no_grad():\n                logits_N, _ = self.clip_model(images, self.text_N)\n                logits_B, _ = self.clip_model(images, self.text_B)\n\n            tmp_N = logits_N.softmax(dim=-1).view(interval, -1) * 10\n            tmp_B = logits_B.softmax(dim=-1).view(interval, -1) * 10\n            # print('tmp_N get_global_sf', tmp_N.shape) # torch.Size([10, 75])\n            # print('tmp_B get_global_sf', tmp_B.shape) # torch.Size([10, 75])\n            res.append(torch.cat([tmp_N, tmp_B], dim=1))\n            now += interval\n\n        # Handle remaining frames\n        if length &gt; now:\n            final = [self.to_tensor(Image.fromarray(cv2.cvtColor(frames[i], cv2.COLOR_BGR2RGB)))\n                    for i in range(now, length)]\n\n            images = torch.stack(final, dim=0)  # Shape: [remaining(6), 3, 672, 1120]\n            images = images.unfold(2, 224, 224).unfold(3, 224, 224)  # Shape: [remaining, 3, 3, 5, 224, 224]\n            images = images.permute(0, 3, 2, 1, 4, 5).contiguous()  # Shape: [remaining, 5, 3, 3, 224, 224]\n            images = images.reshape(-1, 15, 3, 224, 224)  # Shape: [remaining, 15, 3, 224, 224]\n            images = images.view(-1, 3, 224, 224)  # Shape: [remaining*15, 3, 224, 224]\n            images = self.preprocess(images)\n\n            with torch.no_grad():\n                logits_N, _ = self.clip_model(images, self.text_N) # Shape: [remaining, 5(num_text_prompts)]\n                logits_B, _ = self.clip_model(images, self.text_B) # Shape: [remaining, 5]\n                # print('logits_N last get_global_sf', logits_N.shape) # torch.Size([6*15, 5])\n                # print('logits_B last get_global_sf', logits_B.shape) #torch.Size([6*15, 5])\n\n            tmp_N = logits_N.softmax(dim=-1).view(length - now, -1) * 10 # Shape: [remaining, 75]\n            tmp_B = logits_B.softmax(dim=-1).view(length - now, -1) * 10 # Shape: [remaining, 75]\n            # print('tmp_N last get_global_sf', tmp_N.shape)  # torch.Size([6, 75])\n            # print('tmp_B last get_global_sf', tmp_B.shape)  # torch.Size([6, 75])\n\n            res.append(torch.cat([tmp_N, tmp_B], dim=1))\n\n        res = torch.cat(res, dim=0)  # Shape: [length, 150]\n        # print('res: ', res.shape)  # torch.Size([16, 150]) for toy dataset\n\n        # Step 4: Aggregate into 8 time slots\n        chunk_size = length // 8\n        final_res = [\n            torch.mean(res[i * chunk_size: (i + 1) * chunk_size], dim=0) if i &lt; 7 else torch.mean(res[7 * chunk_size:], dim=0)\n            for i in range(8)\n        ]\n\n        return torch.stack(final_res, dim=0)  # Shape: [8, 150]\n\n    def extract_bns_features(self, video_path):\n        \"\"\"Extracts Brightness &amp; Noise Sensitivity (BNS) features using CLIP.\n        Local Feature Extraction (res1) \u2192 Uses 8 key frames\n        Global Feature Extraction (res2) \u2192 Uses all frames\n\n        Args:\n            video_path (str): Path to the video file.\n\n        Returns:\n            spatial_features (torch.Tensor): Extracted 8 evenly spaced key frames across the entire video duration.\n                Shape [8, 3, 672, 1120] containing 8 key frames.\n            final_res (torch.Tensor): Extracted BNS feature (Shape: [8, 300]).\n        \"\"\"\n        # Local Feature Extraction Step 1: Extract key frames\n        spatial_features = self.extract_key_frames(video_path) # Shape: [8, 3, 672, 1120]\n\n        # Step 2: Apply unfolding transformation (Strictly following GET_S_F)\n        images = spatial_features.unfold(2, 224, 224).unfold(3, 224, 224)  # Break into patches. Shape: [8, 3, 3, 5, 224, 224]\n        images = images.permute(0, 3, 2, 1, 4, 5).contiguous()  # Shape: [8, 5, 3, 3, 224, 224]\n        images = images.reshape(-1, 15, 3, 224, 224)  # Shape: [8, 15, 3, 224, 224]\n        images = images.view(-1, 3, 224, 224)  # Shape: [120, 3, 224, 224]\n        images = self.preprocess(images)  # Normalize for CLIP\n        # print('images: ', images.shape) # torch.Size([120, 3, 224, 224])\n        # print(images.device)\n        # print(self.text_N.device)\n\n        # Step 3: Pass through CLIP\n        with torch.no_grad():\n            logits_N, _ = self.clip_model(images, self.text_N)\n            logits_B, _ = self.clip_model(images, self.text_B)\n\n        res_N = logits_N.softmax(dim=-1).view(8, -1) * 10\n        # print('res_N: ', res_N.shape) # torch.Size([8, 75])\n        res_B = logits_B.softmax(dim=-1).view(8, -1) * 10\n        # print('res_B: ', res_N.shape) # torch.Size([8, 75])\n        res1 = torch.cat((res_N, res_B), dim=1)\n        # print('res1: ', res1.shape) # torch.Size([8, 150])\n\n        # Global Feature Extraction (GET_SF Equivalent)\n        res2 = self.get_global_sf(video_path)\n        # print('res2: ', res2.shape) # res2:  torch.Size([8, 150])\n\n        # Split &amp; Combine Features\n        Nl, Bl = torch.split(res1, 75, dim=1)\n        Ng, Bg = torch.split(res2, 75, dim=1)\n        final_res = torch.cat([Nl, Ng, Bl, Bg], dim=1)\n        # print('final_res: ', final_res.shape)\n\n        return spatial_features, final_res  # Shape: [8, 300]\n\n    def extract_bc_features(self, video_path) -&gt; torch.Tensor:\n        \"\"\"\n        Extracts Brightness Consistency features using CLIP-based temporal processing.\n\n        Returns:\n            torch.Tensor: Extracted BC feature (Shape: [8, final_dim]).\n        \"\"\"\n\n        cap = cv2.VideoCapture(video_path)\n        video_length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n\n        frames = []\n        for _ in range(video_length):\n            ret, frame = cap.read()\n            if ret:\n                frame = cv2.resize(frame, (1120, 672))\n                frames.append(frame)\n        cap.release()\n\n        if not frames:\n            raise ValueError(f\"Failed to extract frames from {video_path}\")\n\n        res = []\n        now = 0\n        interval = 10  # Process 10 frames at a time\n        length = len(frames)\n\n        # Step 1: Extract CLIP Features at Fixed Intervals\n        while now + interval - 1 &lt; length:\n            batch = [self.to_tensor(Image.fromarray(cv2.cvtColor(frames[i + now], cv2.COLOR_BGR2RGB)))\n                    for i in range(interval)]\n            images = torch.stack(batch, dim=0)\n            images = images.unfold(2, 224, 224).unfold(3, 224, 224)  # Shape: [10, 3, 3, 5, 224, 224]\n            images = images.permute(0, 3, 2, 1, 4, 5).contiguous()  # Shape: [10, 5, 3, 3, 224, 224]\n            images = images.reshape(-1, 15, 3, 224, 224)  # Shape: [10, 15, 3, 224, 224]\n            images = images.view(-1, 3, 224, 224)  # Shape: [10*15, 3, 224, 224]\n            images = self.preprocess(images)\n            # print('images extract_bc_features', images.shape) # torch.Size([150, 3, 224, 224])\n\n            with torch.no_grad():\n                logits, _ = self.clip_model(images, self.text_B)\n\n            tmp = logits.softmax(dim=-1) * 10\n            res.append(tmp)\n            now += interval\n\n        # Handle Remaining Frames\n        if length &gt; now:\n            batch = [self.to_tensor(Image.fromarray(cv2.cvtColor(frames[i], cv2.COLOR_BGR2RGB)))\n                    for i in range(now, length)]\n            images = torch.stack(batch, dim=0)\n            images = images.unfold(2, 224, 224).unfold(3, 224, 224)  # Shape: [remaining(6), 3, 3, 5, 224, 224]\n            images = images.permute(0, 3, 2, 1, 4, 5).contiguous()  # Shape: [remaining, 5, 3, 3, 224, 224]\n            images = images.reshape(-1, 15, 3, 224, 224)  # Shape: [remaining, 15, 3, 224, 224]\n            images = images.view(-1, 3, 224, 224)  # Shape: [remaining, 15, 3, 224, 224]\n            images = self.preprocess(images)\n            # print('images: ', images.shape) #  torch.Size([6*15, 3, 224, 224])\n\n            with torch.no_grad():\n                logits, _ = self.clip_model(images, self.text_B)\n\n            tmp = logits.softmax(dim=-1) * 10\n            res.append(tmp)\n\n        res = torch.cat(res, dim=0)  # Shape: [length, 5]\n        # print('res extract_bc_features: ', res.shape) # torch.Size([150+90, 5])\n\n        # Step 2: Multi-Scale Variance Computation: downsample frames steps\n        # smaller step: Captures fast, fine-grained changes.\n        # larger step:  Captures slow, long-term trends.\n        final_res = []\n        for step in [1, 2, 4, 8]:  # Multi-scale temporal steps\n            chunk_number = 8 // step\n            chunk_size = length // chunk_number\n            chunks = []\n            for i in range(chunk_number):\n                if i &lt; chunk_number - 1:\n                    chunk = res[i * chunk_size : (i + 1) * chunk_size, :]\n                else:\n                    chunk = res[(chunk_number - 1) * chunk_size:, :]  # Handle remaining frames\n                tmp = []\n                for j in range(step):\n                    temp = chunk[j::step, :]  \n                    tmp.append(torch.var(temp.float(), dim=0))  # Variance computation\n                chunks.append(tmp)  # final chunks len: 8; 4; 2; 1 \n            final_res.append(chunks) # final final_res len: 4\n\n        # Step 3: Aggregate Multi-Scale Features\n        temp = []\n        for i in range(8):  # Aggregate temporal information across 8 time slots\n            temp.append(torch.cat(final_res[0][i]                                                # variance for step size = 1\n                                + [torch.mean(torch.stack(final_res[1][i // 2], dim=0), dim=0)]  # for step size = 2\n                                + [torch.mean(torch.stack(final_res[2][i // 4], dim=0), dim=0)]  # Every 4 slots share the same value.\n                                + [torch.mean(torch.stack(final_res[3][i // 8], dim=0), dim=0)]  # for step size = 8\n                                , dim=0))\n\n        final_res = torch.stack(temp, dim=0)  # Shape: [8, final_dim]  \n        # print('final_res extract_bc_featuresx: ', final_res.shape) # torch.Size([8, 20])\n\n        return final_res\n\n    def extract_temporal_features(self, video_path) -&gt; torch.Tensor:\n        \"\"\"Extracts SlowFast motion features on the entire video segment.\n\n        Args:\n            video_path (str): Path to the video file.\n\n        Returns:\n            torch.Tensor: Extracted motion features (Shape: [1, feature_dim(2304)]).\n        \"\"\"\n        cap = cv2.VideoCapture(video_path)\n        video_length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n        frame_indices = np.round(np.linspace(0, video_length - 1, num=8)).astype(int)\n\n        transform = transforms.Compose([\n            transforms.Resize([224, 224]),  # Match SlowFast input size\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.45, 0.45, 0.45], std=[0.225, 0.225, 0.225])  # Original normalization\n        ])\n\n        frames = []\n        for idx in frame_indices:\n            cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n            ret, frame = cap.read()\n            if ret:\n                frame = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n                frames.append(transform(frame))  # Resize &amp; normalize\n        cap.release()\n\n        if len(frames) &lt; 8:\n            raise ValueError(f\"Insufficient frames in {video_path}, expected 8.\")\n\n        video_tensor = torch.stack(frames, dim=0)  # Shape: [8, 3, 224, 224]\n\n        # Prepare for SlowFast input\n        video_tensor = video_tensor.unsqueeze(0)  # Add batch dimension: [1, 8, 3, 224, 224]\n        video_tensor = video_tensor.permute(0, 2, 1, 3, 4)  # Shape: [1, 3, 8, 224, 224]\n\n        # Pack pathways for SlowFast model\n        _, pack_pathway_output = lazy_import()\n        inputs = pack_pathway_output(video_tensor, device='cpu')\n        # print('inputs len: ', len(inputs))\n        # print('inputs[0]: ', inputs[0].shape) # torch.Size([1, 3, 2, 224, 224])\n        # print('inputs[1]: ', inputs[1].shape) # torch.Size([1, 3, 8, 224, 224])\n\n        # Extract features using SlowFast\n        with torch.no_grad():\n            slow_feature, fast_feature = self.slowfast_model(inputs)\n\n        # print('slow_feature extract_temporal_features: ', slow_feature.shape) # torch.Size([1, 2048, 1, 1, 1])\n        # print('fast_feature extract_temporal_features: ', fast_feature.shape) # torch.Size([1, 256, 1, 1, 1])\n\n        # Concatenate slow and fast features\n        features = torch.cat([slow_feature, fast_feature], dim=1).squeeze(-1).squeeze(-1).squeeze(-1)\n        # print('features extract_temporal_features: ', features.shape) # torch.Size([1, 2304])\n\n        return features\n\n    def __getitem__(self, index):\n        \"\"\"\n        Returns:\n            spatial_features (torch.Tensor): Spatial features. Shape: [8, 3, 672, 1120].\n            bns_features (torch.Tensor): Brightness &amp; Noise features. Shape: [8, 300].\n            (bc_features (torch.Tensor): Temporal brightness contrast features. Shape: [8, final_dim].)\n            temporal_features (torch.Tensor): SlowFast motion features. Shape: [1, feature_dim(2304)]\n            video_name (str): Video filename.\n        \"\"\"\n        video_name = self.video_names[index]\n        video_path = os.path.join(self.video_dir, video_name)\n\n        spatial_features, bns_features = self.extract_bns_features(video_path)\n        bc_features = self.extract_bc_features(video_path)\n        temporal_features = self.extract_temporal_features(video_path)\n\n        # # Save extracted features\n        # feature_path = os.path.join(self.feature_dir, f\"{video_name}_features.pth\")\n        # torch.save({\n        #     \"spatial\": spatial_features,\n        #     \"temporal\": temporal_features,\n        #     \"bns\": bns_features,\n        #     \"bc\": bc_features\n        # }, feature_path)\n\n        return spatial_features, temporal_features, bns_features, bc_features, video_name\n</code></pre>"},{"location":"documentations/datasets/#aigve.datasets.LightVQAPlusDataset.__getitem__","title":"<code>__getitem__(index)</code>","text":"<p>Returns:</p> Name Type Description <code>spatial_features</code> <code>Tensor</code> <p>Spatial features. Shape: [8, 3, 672, 1120].</p> <code>bns_features</code> <code>Tensor</code> <p>Brightness &amp; Noise features. Shape: [8, 300].</p> <code>bc_features (torch.Tensor</code> <p>Temporal brightness contrast features. Shape: [8, final_dim].)</p> <code>temporal_features</code> <code>Tensor</code> <p>SlowFast motion features. Shape: [1, feature_dim(2304)]</p> <code>video_name</code> <code>str</code> <p>Video filename.</p> Source code in <code>aigve/datasets/lightvqa_plus_dataset.py</code> <pre><code>def __getitem__(self, index):\n    \"\"\"\n    Returns:\n        spatial_features (torch.Tensor): Spatial features. Shape: [8, 3, 672, 1120].\n        bns_features (torch.Tensor): Brightness &amp; Noise features. Shape: [8, 300].\n        (bc_features (torch.Tensor): Temporal brightness contrast features. Shape: [8, final_dim].)\n        temporal_features (torch.Tensor): SlowFast motion features. Shape: [1, feature_dim(2304)]\n        video_name (str): Video filename.\n    \"\"\"\n    video_name = self.video_names[index]\n    video_path = os.path.join(self.video_dir, video_name)\n\n    spatial_features, bns_features = self.extract_bns_features(video_path)\n    bc_features = self.extract_bc_features(video_path)\n    temporal_features = self.extract_temporal_features(video_path)\n\n    # # Save extracted features\n    # feature_path = os.path.join(self.feature_dir, f\"{video_name}_features.pth\")\n    # torch.save({\n    #     \"spatial\": spatial_features,\n    #     \"temporal\": temporal_features,\n    #     \"bns\": bns_features,\n    #     \"bc\": bc_features\n    # }, feature_path)\n\n    return spatial_features, temporal_features, bns_features, bc_features, video_name\n</code></pre>"},{"location":"documentations/datasets/#aigve.datasets.LightVQAPlusDataset._read_video_names","title":"<code>_read_video_names()</code>","text":"<p>Reads video names from the dataset JSON file.</p> Source code in <code>aigve/datasets/lightvqa_plus_dataset.py</code> <pre><code>def _read_video_names(self):\n    \"\"\"Reads video names from the dataset JSON file.\"\"\"\n    with open(self.prompt_dir, 'r') as reader:\n        read_data = json.load(reader)\n    return [item['video_path_pd'].strip() for item in read_data[\"data_list\"]]\n</code></pre>"},{"location":"documentations/datasets/#aigve.datasets.LightVQAPlusDataset.extract_bc_features","title":"<code>extract_bc_features(video_path)</code>","text":"<p>Extracts Brightness Consistency features using CLIP-based temporal processing.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: Extracted BC feature (Shape: [8, final_dim]).</p> Source code in <code>aigve/datasets/lightvqa_plus_dataset.py</code> <pre><code>def extract_bc_features(self, video_path) -&gt; torch.Tensor:\n    \"\"\"\n    Extracts Brightness Consistency features using CLIP-based temporal processing.\n\n    Returns:\n        torch.Tensor: Extracted BC feature (Shape: [8, final_dim]).\n    \"\"\"\n\n    cap = cv2.VideoCapture(video_path)\n    video_length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n\n    frames = []\n    for _ in range(video_length):\n        ret, frame = cap.read()\n        if ret:\n            frame = cv2.resize(frame, (1120, 672))\n            frames.append(frame)\n    cap.release()\n\n    if not frames:\n        raise ValueError(f\"Failed to extract frames from {video_path}\")\n\n    res = []\n    now = 0\n    interval = 10  # Process 10 frames at a time\n    length = len(frames)\n\n    # Step 1: Extract CLIP Features at Fixed Intervals\n    while now + interval - 1 &lt; length:\n        batch = [self.to_tensor(Image.fromarray(cv2.cvtColor(frames[i + now], cv2.COLOR_BGR2RGB)))\n                for i in range(interval)]\n        images = torch.stack(batch, dim=0)\n        images = images.unfold(2, 224, 224).unfold(3, 224, 224)  # Shape: [10, 3, 3, 5, 224, 224]\n        images = images.permute(0, 3, 2, 1, 4, 5).contiguous()  # Shape: [10, 5, 3, 3, 224, 224]\n        images = images.reshape(-1, 15, 3, 224, 224)  # Shape: [10, 15, 3, 224, 224]\n        images = images.view(-1, 3, 224, 224)  # Shape: [10*15, 3, 224, 224]\n        images = self.preprocess(images)\n        # print('images extract_bc_features', images.shape) # torch.Size([150, 3, 224, 224])\n\n        with torch.no_grad():\n            logits, _ = self.clip_model(images, self.text_B)\n\n        tmp = logits.softmax(dim=-1) * 10\n        res.append(tmp)\n        now += interval\n\n    # Handle Remaining Frames\n    if length &gt; now:\n        batch = [self.to_tensor(Image.fromarray(cv2.cvtColor(frames[i], cv2.COLOR_BGR2RGB)))\n                for i in range(now, length)]\n        images = torch.stack(batch, dim=0)\n        images = images.unfold(2, 224, 224).unfold(3, 224, 224)  # Shape: [remaining(6), 3, 3, 5, 224, 224]\n        images = images.permute(0, 3, 2, 1, 4, 5).contiguous()  # Shape: [remaining, 5, 3, 3, 224, 224]\n        images = images.reshape(-1, 15, 3, 224, 224)  # Shape: [remaining, 15, 3, 224, 224]\n        images = images.view(-1, 3, 224, 224)  # Shape: [remaining, 15, 3, 224, 224]\n        images = self.preprocess(images)\n        # print('images: ', images.shape) #  torch.Size([6*15, 3, 224, 224])\n\n        with torch.no_grad():\n            logits, _ = self.clip_model(images, self.text_B)\n\n        tmp = logits.softmax(dim=-1) * 10\n        res.append(tmp)\n\n    res = torch.cat(res, dim=0)  # Shape: [length, 5]\n    # print('res extract_bc_features: ', res.shape) # torch.Size([150+90, 5])\n\n    # Step 2: Multi-Scale Variance Computation: downsample frames steps\n    # smaller step: Captures fast, fine-grained changes.\n    # larger step:  Captures slow, long-term trends.\n    final_res = []\n    for step in [1, 2, 4, 8]:  # Multi-scale temporal steps\n        chunk_number = 8 // step\n        chunk_size = length // chunk_number\n        chunks = []\n        for i in range(chunk_number):\n            if i &lt; chunk_number - 1:\n                chunk = res[i * chunk_size : (i + 1) * chunk_size, :]\n            else:\n                chunk = res[(chunk_number - 1) * chunk_size:, :]  # Handle remaining frames\n            tmp = []\n            for j in range(step):\n                temp = chunk[j::step, :]  \n                tmp.append(torch.var(temp.float(), dim=0))  # Variance computation\n            chunks.append(tmp)  # final chunks len: 8; 4; 2; 1 \n        final_res.append(chunks) # final final_res len: 4\n\n    # Step 3: Aggregate Multi-Scale Features\n    temp = []\n    for i in range(8):  # Aggregate temporal information across 8 time slots\n        temp.append(torch.cat(final_res[0][i]                                                # variance for step size = 1\n                            + [torch.mean(torch.stack(final_res[1][i // 2], dim=0), dim=0)]  # for step size = 2\n                            + [torch.mean(torch.stack(final_res[2][i // 4], dim=0), dim=0)]  # Every 4 slots share the same value.\n                            + [torch.mean(torch.stack(final_res[3][i // 8], dim=0), dim=0)]  # for step size = 8\n                            , dim=0))\n\n    final_res = torch.stack(temp, dim=0)  # Shape: [8, final_dim]  \n    # print('final_res extract_bc_featuresx: ', final_res.shape) # torch.Size([8, 20])\n\n    return final_res\n</code></pre>"},{"location":"documentations/datasets/#aigve.datasets.LightVQAPlusDataset.extract_bns_features","title":"<code>extract_bns_features(video_path)</code>","text":"<p>Extracts Brightness &amp; Noise Sensitivity (BNS) features using CLIP. Local Feature Extraction (res1) \u2192 Uses 8 key frames Global Feature Extraction (res2) \u2192 Uses all frames</p> <p>Parameters:</p> Name Type Description Default <code>video_path</code> <code>str</code> <p>Path to the video file.</p> required <p>Returns:</p> Name Type Description <code>spatial_features</code> <code>Tensor</code> <p>Extracted 8 evenly spaced key frames across the entire video duration. Shape [8, 3, 672, 1120] containing 8 key frames.</p> <code>final_res</code> <code>Tensor</code> <p>Extracted BNS feature (Shape: [8, 300]).</p> Source code in <code>aigve/datasets/lightvqa_plus_dataset.py</code> <pre><code>def extract_bns_features(self, video_path):\n    \"\"\"Extracts Brightness &amp; Noise Sensitivity (BNS) features using CLIP.\n    Local Feature Extraction (res1) \u2192 Uses 8 key frames\n    Global Feature Extraction (res2) \u2192 Uses all frames\n\n    Args:\n        video_path (str): Path to the video file.\n\n    Returns:\n        spatial_features (torch.Tensor): Extracted 8 evenly spaced key frames across the entire video duration.\n            Shape [8, 3, 672, 1120] containing 8 key frames.\n        final_res (torch.Tensor): Extracted BNS feature (Shape: [8, 300]).\n    \"\"\"\n    # Local Feature Extraction Step 1: Extract key frames\n    spatial_features = self.extract_key_frames(video_path) # Shape: [8, 3, 672, 1120]\n\n    # Step 2: Apply unfolding transformation (Strictly following GET_S_F)\n    images = spatial_features.unfold(2, 224, 224).unfold(3, 224, 224)  # Break into patches. Shape: [8, 3, 3, 5, 224, 224]\n    images = images.permute(0, 3, 2, 1, 4, 5).contiguous()  # Shape: [8, 5, 3, 3, 224, 224]\n    images = images.reshape(-1, 15, 3, 224, 224)  # Shape: [8, 15, 3, 224, 224]\n    images = images.view(-1, 3, 224, 224)  # Shape: [120, 3, 224, 224]\n    images = self.preprocess(images)  # Normalize for CLIP\n    # print('images: ', images.shape) # torch.Size([120, 3, 224, 224])\n    # print(images.device)\n    # print(self.text_N.device)\n\n    # Step 3: Pass through CLIP\n    with torch.no_grad():\n        logits_N, _ = self.clip_model(images, self.text_N)\n        logits_B, _ = self.clip_model(images, self.text_B)\n\n    res_N = logits_N.softmax(dim=-1).view(8, -1) * 10\n    # print('res_N: ', res_N.shape) # torch.Size([8, 75])\n    res_B = logits_B.softmax(dim=-1).view(8, -1) * 10\n    # print('res_B: ', res_N.shape) # torch.Size([8, 75])\n    res1 = torch.cat((res_N, res_B), dim=1)\n    # print('res1: ', res1.shape) # torch.Size([8, 150])\n\n    # Global Feature Extraction (GET_SF Equivalent)\n    res2 = self.get_global_sf(video_path)\n    # print('res2: ', res2.shape) # res2:  torch.Size([8, 150])\n\n    # Split &amp; Combine Features\n    Nl, Bl = torch.split(res1, 75, dim=1)\n    Ng, Bg = torch.split(res2, 75, dim=1)\n    final_res = torch.cat([Nl, Ng, Bl, Bg], dim=1)\n    # print('final_res: ', final_res.shape)\n\n    return spatial_features, final_res  # Shape: [8, 300]\n</code></pre>"},{"location":"documentations/datasets/#aigve.datasets.LightVQAPlusDataset.extract_key_frames","title":"<code>extract_key_frames(video_path)</code>","text":"<p>Extracts 8 evenly spaced key frames across the entire video duration.</p> <p>Parameters:</p> Name Type Description Default <code>video_path</code> <code>str</code> <p>Path to the video file.</p> required <p>Returns:</p> Name Type Description <code>spatial_features</code> <code>Tensor</code> <p>Shape [8, 3, 672, 1120] containing 8 key frames.</p> Source code in <code>aigve/datasets/lightvqa_plus_dataset.py</code> <pre><code>def extract_key_frames(self, video_path):\n    \"\"\"\n    Extracts 8 evenly spaced key frames across the entire video duration.\n\n    Args:\n        video_path (str): Path to the video file.\n\n    Returns:\n        spatial_features (torch.Tensor): Shape [8, 3, 672, 1120] containing 8 key frames.\n    \"\"\"\n    cap = cv2.VideoCapture(video_path)\n    video_name = os.path.basename(video_path).split('.')[0]\n\n    video_length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n\n    if video_length &gt;= 8:\n        # Select 8 unique frame indices evenly spaced across the entire video\n        frame_indices = np.round(np.linspace(0, video_length - 1, num=8)).astype(int)\n    else:\n        # Select all available frames and repeat the last one to reach 8\n        frame_indices = list(range(video_length)) + [video_length - 1] * (8 - video_length)\n\n    spatial_features = torch.zeros([8, 3, 672, 1120])  # Ensure exactly 8 frames\n    transform = transforms.Compose([\n        transforms.Resize([672, 1120]),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    ])\n\n    last_valid_frame = None\n    for idx, frame_idx in enumerate(frame_indices):\n        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)\n        ret, frame = cap.read()\n        if ret:\n            frame = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n            spatial_features[idx] = transform(frame)\n            last_valid_frame = spatial_features[idx]\n        elif last_valid_frame is not None:  # If total frames are less than 8, repeat the last valid frame\n            spatial_features[idx] = last_valid_frame\n\n    cap.release()\n    # print('spatial_features: ', spatial_features.shape) # torch.Size([8, 3, 672, 1120])\n    return spatial_features\n</code></pre>"},{"location":"documentations/datasets/#aigve.datasets.LightVQAPlusDataset.extract_temporal_features","title":"<code>extract_temporal_features(video_path)</code>","text":"<p>Extracts SlowFast motion features on the entire video segment.</p> <p>Parameters:</p> Name Type Description Default <code>video_path</code> <code>str</code> <p>Path to the video file.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: Extracted motion features (Shape: [1, feature_dim(2304)]).</p> Source code in <code>aigve/datasets/lightvqa_plus_dataset.py</code> <pre><code>def extract_temporal_features(self, video_path) -&gt; torch.Tensor:\n    \"\"\"Extracts SlowFast motion features on the entire video segment.\n\n    Args:\n        video_path (str): Path to the video file.\n\n    Returns:\n        torch.Tensor: Extracted motion features (Shape: [1, feature_dim(2304)]).\n    \"\"\"\n    cap = cv2.VideoCapture(video_path)\n    video_length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n    frame_indices = np.round(np.linspace(0, video_length - 1, num=8)).astype(int)\n\n    transform = transforms.Compose([\n        transforms.Resize([224, 224]),  # Match SlowFast input size\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.45, 0.45, 0.45], std=[0.225, 0.225, 0.225])  # Original normalization\n    ])\n\n    frames = []\n    for idx in frame_indices:\n        cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n        ret, frame = cap.read()\n        if ret:\n            frame = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n            frames.append(transform(frame))  # Resize &amp; normalize\n    cap.release()\n\n    if len(frames) &lt; 8:\n        raise ValueError(f\"Insufficient frames in {video_path}, expected 8.\")\n\n    video_tensor = torch.stack(frames, dim=0)  # Shape: [8, 3, 224, 224]\n\n    # Prepare for SlowFast input\n    video_tensor = video_tensor.unsqueeze(0)  # Add batch dimension: [1, 8, 3, 224, 224]\n    video_tensor = video_tensor.permute(0, 2, 1, 3, 4)  # Shape: [1, 3, 8, 224, 224]\n\n    # Pack pathways for SlowFast model\n    _, pack_pathway_output = lazy_import()\n    inputs = pack_pathway_output(video_tensor, device='cpu')\n    # print('inputs len: ', len(inputs))\n    # print('inputs[0]: ', inputs[0].shape) # torch.Size([1, 3, 2, 224, 224])\n    # print('inputs[1]: ', inputs[1].shape) # torch.Size([1, 3, 8, 224, 224])\n\n    # Extract features using SlowFast\n    with torch.no_grad():\n        slow_feature, fast_feature = self.slowfast_model(inputs)\n\n    # print('slow_feature extract_temporal_features: ', slow_feature.shape) # torch.Size([1, 2048, 1, 1, 1])\n    # print('fast_feature extract_temporal_features: ', fast_feature.shape) # torch.Size([1, 256, 1, 1, 1])\n\n    # Concatenate slow and fast features\n    features = torch.cat([slow_feature, fast_feature], dim=1).squeeze(-1).squeeze(-1).squeeze(-1)\n    # print('features extract_temporal_features: ', features.shape) # torch.Size([1, 2304])\n\n    return features\n</code></pre>"},{"location":"documentations/datasets/#aigve.datasets.LightVQAPlusDataset.get_global_sf","title":"<code>get_global_sf(video_path)</code>","text":"<p>Extracts global brightness &amp; noise features across full video.</p> <p>Parameters:</p> Name Type Description Default <code>video_path</code> <code>str</code> <p>Path to video file.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: Extracted global features (Shape: [8, 150]).</p> Source code in <code>aigve/datasets/lightvqa_plus_dataset.py</code> <pre><code>def get_global_sf(self, video_path) -&gt; torch.Tensor:\n    \"\"\"Extracts global brightness &amp; noise features across full video.\n\n    Args:\n        video_path (str): Path to video file.\n\n    Returns:\n        torch.Tensor: Extracted global features (Shape: [8, 150]).\n    \"\"\"\n    cap = cv2.VideoCapture(video_path)\n    video_length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n    # print('video_length: ', video_length)  # 16\n\n    frames = []\n    for _ in range(video_length):\n        ret, frame = cap.read()\n        if ret:\n            frame = cv2.resize(frame, (1120, 672))\n            frames.append(frame)\n    cap.release()\n\n    if not frames:\n        raise ValueError(f\"Failed to extract frames from {video_path}\")\n\n    res = []\n    length = len(frames)\n    now = 0\n    interval = 10  # Process 10 frames at a time\n    while now + interval - 1 &lt; length:\n        final = [self.to_tensor(Image.fromarray(cv2.cvtColor(frames[i + now], cv2.COLOR_BGR2RGB)))\n                for i in range(interval)]\n\n        # Step 1: Convert to tensor batch\n        images = torch.stack(final, dim=0)  # Shape: [10, 3, 672, 1120]\n\n        # Step 2: Unfold into patches (Strictly following GET_SF)\n        images = images.unfold(2, 224, 224).unfold(3, 224, 224)  # Shape: [10, 3, 3, 5, 224, 224]\n        images = images.permute(0, 3, 2, 1, 4, 5).contiguous()  # Shape: [10, 5, 3, 3, 224, 224]\n        images = images.reshape(-1, 15, 3, 224, 224)  # Shape: [10, 15, 3, 224, 224]\n        images = images.view(-1, 3, 224, 224)  # Shape: [150, 3, 224, 224]\n        images = self.preprocess(images)  # Normalize for CLIP\n        # print('images get_global_sf: ', images.shape) # torch.Size([10*15, 3, 224, 224])\n\n        # Step 3: Extract features using CLIP\n        with torch.no_grad():\n            logits_N, _ = self.clip_model(images, self.text_N)\n            logits_B, _ = self.clip_model(images, self.text_B)\n\n        tmp_N = logits_N.softmax(dim=-1).view(interval, -1) * 10\n        tmp_B = logits_B.softmax(dim=-1).view(interval, -1) * 10\n        # print('tmp_N get_global_sf', tmp_N.shape) # torch.Size([10, 75])\n        # print('tmp_B get_global_sf', tmp_B.shape) # torch.Size([10, 75])\n        res.append(torch.cat([tmp_N, tmp_B], dim=1))\n        now += interval\n\n    # Handle remaining frames\n    if length &gt; now:\n        final = [self.to_tensor(Image.fromarray(cv2.cvtColor(frames[i], cv2.COLOR_BGR2RGB)))\n                for i in range(now, length)]\n\n        images = torch.stack(final, dim=0)  # Shape: [remaining(6), 3, 672, 1120]\n        images = images.unfold(2, 224, 224).unfold(3, 224, 224)  # Shape: [remaining, 3, 3, 5, 224, 224]\n        images = images.permute(0, 3, 2, 1, 4, 5).contiguous()  # Shape: [remaining, 5, 3, 3, 224, 224]\n        images = images.reshape(-1, 15, 3, 224, 224)  # Shape: [remaining, 15, 3, 224, 224]\n        images = images.view(-1, 3, 224, 224)  # Shape: [remaining*15, 3, 224, 224]\n        images = self.preprocess(images)\n\n        with torch.no_grad():\n            logits_N, _ = self.clip_model(images, self.text_N) # Shape: [remaining, 5(num_text_prompts)]\n            logits_B, _ = self.clip_model(images, self.text_B) # Shape: [remaining, 5]\n            # print('logits_N last get_global_sf', logits_N.shape) # torch.Size([6*15, 5])\n            # print('logits_B last get_global_sf', logits_B.shape) #torch.Size([6*15, 5])\n\n        tmp_N = logits_N.softmax(dim=-1).view(length - now, -1) * 10 # Shape: [remaining, 75]\n        tmp_B = logits_B.softmax(dim=-1).view(length - now, -1) * 10 # Shape: [remaining, 75]\n        # print('tmp_N last get_global_sf', tmp_N.shape)  # torch.Size([6, 75])\n        # print('tmp_B last get_global_sf', tmp_B.shape)  # torch.Size([6, 75])\n\n        res.append(torch.cat([tmp_N, tmp_B], dim=1))\n\n    res = torch.cat(res, dim=0)  # Shape: [length, 150]\n    # print('res: ', res.shape)  # torch.Size([16, 150]) for toy dataset\n\n    # Step 4: Aggregate into 8 time slots\n    chunk_size = length // 8\n    final_res = [\n        torch.mean(res[i * chunk_size: (i + 1) * chunk_size], dim=0) if i &lt; 7 else torch.mean(res[7 * chunk_size:], dim=0)\n        for i in range(8)\n    ]\n\n    return torch.stack(final_res, dim=0)  # Shape: [8, 150]\n</code></pre>"},{"location":"documentations/datasets/#aigve.datasets.SimpleVQADataset","title":"<code>SimpleVQADataset</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>Dataset for SimpleVQA. Each sample returns:     - spatial_features (torch.Tensor): Extracted spatial frames.     - motion_features (torch.Tensor): Extracted motion-based clips.     - video_name (str): Video filename.</p> Source code in <code>aigve/datasets/simplevqa_dataset.py</code> <pre><code>@DATASETS.register_module()\nclass SimpleVQADataset(Dataset):\n    \"\"\"\n    Dataset for SimpleVQA.\n    Each sample returns:\n        - spatial_features (torch.Tensor): Extracted spatial frames.\n        - motion_features (torch.Tensor): Extracted motion-based clips.\n        - video_name (str): Video filename.\n    \"\"\"\n\n    def __init__(self, video_dir, prompt_dir, min_video_seconds=8):\n        super(SimpleVQADataset, self).__init__()\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self.video_dir = video_dir\n        self.prompt_dir = prompt_dir\n        self.min_video_seconds = min_video_seconds\n\n        self.prompts, self.video_names = self._read_prompt_videoname()\n\n    def _read_prompt_videoname(self):\n        with open(self.prompt_dir, 'r') as reader:\n            read_data = json.load(reader)\n\n        prompt_data_list, video_name_list = [], []\n        for item in read_data[\"data_list\"]:\n            prompt = item['prompt_gt'].strip()\n            video_name = item['video_path_pd'].strip()\n            prompt_data_list.append(prompt)\n            video_name_list.append(video_name)\n\n        return prompt_data_list, video_name_list\n\n    def __len__(self):\n        return len(self.prompts)\n\n    def video_processing_spatial(self, video_path):\n        \"\"\"\n        Extracts spatial frames with proper resizing and normalization.\n            - Key frame extraction: It selects 1 frame per second.\n            - Standard input size: It resizes frames to 448 * 448 (after an initial resize to 520).\n        Return:\n            transformed_video (torch.Tensor): shape[video_length_read, 3, 448, 448]. \n                `video_length_read` is total seconds of the video (though 2 for toy dataset) with minium 8 (i.e. min_video_seconds).\n            video_name (str)\n        \"\"\"\n        video_capture = cv2.VideoCapture(video_path)\n        video_name = os.path.basename(video_path)\n        video_length = int(video_capture.get(cv2.CAP_PROP_FRAME_COUNT))\n        video_frame_rate = int(round(video_capture.get(cv2.CAP_PROP_FPS)))\n\n        # Compute the number of total seconds of the video\n        video_length_read = int(video_length/video_frame_rate) # math.ceil()\n        # print('video_length_read (s): ', video_length_read)\n        transformations = transforms.Compose([\n            transforms.Resize(520),\n            transforms.CenterCrop(448),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) # Standard ImageNet normalization\n        ])\n        transformed_video = torch.zeros([max(video_length_read, self.min_video_seconds), 3, 448, 448])\n\n        video_read_index = 0\n        frame_idx = 0\n        for i in range(video_length):\n            has_frames, frame = video_capture.read()\n            if has_frames:\n                # Key frames extraction\n                if (video_read_index &lt; video_length_read) and (frame_idx % video_frame_rate == 0):\n                    read_frame = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n                    read_frame = transformations(read_frame)\n                    transformed_video[video_read_index] = read_frame\n                    video_read_index += 1\n                frame_idx += 1\n\n        # Pads remaining frames by repeating the last available frame.\n        if video_read_index &lt; self.min_video_seconds:\n            for i in range(video_read_index, self.min_video_seconds):\n                transformed_video[i] = transformed_video[video_read_index - 1]\n\n        video_capture.release()\n        return transformed_video, video_name\n\n    def video_processing_motion(self, video_path):\n        \"\"\"\n        Extracts motion-based clips suitable for SlowFast.\n            - Standard input size: It resizes frames to 224 * 224.\n            - Motion-based clips: Processes at leaset 8-second clips, select 32 consecutive frames from each second.\n        Return:\n            transformed_video_all (List[torch.Tensor]): Tensor shape[video_length_clip(32), 3, 224, 224]. \n                Len(List) is total seconds of the video, with minium 8.\n            video_name (str)\n        \"\"\"\n        video_capture = cv2.VideoCapture(video_path)\n        video_name = os.path.basename(video_path)\n        video_length = int(video_capture.get(cv2.CAP_PROP_FRAME_COUNT))\n        video_frame_rate = int(round(video_capture.get(cv2.CAP_PROP_FPS)))\n\n        transform = transforms.Compose([\n            transforms.Resize([224, 224]),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.45, 0.45, 0.45], std=[0.225, 0.225, 0.225]) # General purpose\n        ])\n        transformed_frame_all = torch.zeros([video_length, 3, 224, 224])\n        video_read_index = 0\n        for i in range(video_length): # All frames extraction\n            has_frames, frame = video_capture.read()\n            if has_frames:\n                read_frame = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n                read_frame = transform(read_frame)\n                transformed_frame_all[video_read_index] = read_frame\n                video_read_index += 1\n\n        # Pads remaining frames by repeating the last available frame.\n        if video_read_index &lt; video_length: \n            for i in range(video_read_index, video_length):\n                transformed_frame_all[i] = transformed_frame_all[video_read_index - 1]\n\n        video_capture.release()\n\n        # Compute the number of total seconds of the video\n        video_clip = int(video_length/video_frame_rate)\n        # print('video_clip (s): ', video_clip)\n        video_length_clip = 32\n        transformed_video_all = []\n\n        # Extract motion-based clips: select 32 consecutive frames from each second\n        for i in range(video_clip):\n            transformed_video = torch.zeros([video_length_clip, 3, 224, 224])\n            if (i * video_frame_rate + video_length_clip) &lt;= video_length: # if the clip can be fully extracted, select 32 consecutive frames starting at i*video_frame_rate\n                transformed_video = transformed_frame_all[i * video_frame_rate:(i * video_frame_rate + video_length_clip)]\n            else: # Copy all rest available frames. Pads remaining frames by repeating the last available frame.\n                transformed_video[:(video_length - i * video_frame_rate)] = transformed_frame_all[i * video_frame_rate:]\n                for j in range((video_length - i * video_frame_rate), video_length_clip):\n                    transformed_video[j] = transformed_video[video_length - i * video_frame_rate - 1]\n            transformed_video_all.append(transformed_video)\n\n        if video_clip &lt; self.min_video_seconds:\n            for i in range(video_clip, self.min_video_seconds):\n                transformed_video_all.append(transformed_video_all[video_clip - 1])\n\n        return transformed_video_all, video_name\n\n    def __getitem__(self, index):\n        \"\"\"\n        Returns:\n            spatial_features (torch.Tensor): Shape [v_len_second, 3, 448, 448]\n                `v_len_second` is total seconds of the video (though 2 for toy dataset) with minium 8 (i.e. min_video_seconds).\n            motion_features (List[torch.Tensor]): List of motion feature tensors.\n                Each tensor has shape [32, 3, 224, 224].\n                Len(List) is total seconds of the video (i.e. v_len_second), with minium 8 (i.e. min_video_seconds).\n            video_name (str): Video filename\n        \"\"\"\n        video_name = self.video_names[index]\n        video_path = os.path.join(self.video_dir, video_name)\n\n        spatial_features, video_name = self.video_processing_spatial(video_path)\n        motion_features, video_name = self.video_processing_motion(video_path)\n        # print('spatial_features: ', spatial_features.shape) # torch.Size([8, 3, 448, 448]) for toy dataset\n        # print('motion_features len: ', len(motion_features)) # 8\n        # print('motion_features[0]: ', motion_features[0].shape) # torch.Size([32, 3, 224, 224])\n\n        return spatial_features, motion_features, video_name\n</code></pre>"},{"location":"documentations/datasets/#aigve.datasets.SimpleVQADataset.__getitem__","title":"<code>__getitem__(index)</code>","text":"<p>Returns:</p> Name Type Description <code>spatial_features</code> <code>Tensor</code> <p>Shape [v_len_second, 3, 448, 448] <code>v_len_second</code> is total seconds of the video (though 2 for toy dataset) with minium 8 (i.e. min_video_seconds).</p> <code>motion_features</code> <code>List[Tensor]</code> <p>List of motion feature tensors. Each tensor has shape [32, 3, 224, 224]. Len(List) is total seconds of the video (i.e. v_len_second), with minium 8 (i.e. min_video_seconds).</p> <code>video_name</code> <code>str</code> <p>Video filename</p> Source code in <code>aigve/datasets/simplevqa_dataset.py</code> <pre><code>def __getitem__(self, index):\n    \"\"\"\n    Returns:\n        spatial_features (torch.Tensor): Shape [v_len_second, 3, 448, 448]\n            `v_len_second` is total seconds of the video (though 2 for toy dataset) with minium 8 (i.e. min_video_seconds).\n        motion_features (List[torch.Tensor]): List of motion feature tensors.\n            Each tensor has shape [32, 3, 224, 224].\n            Len(List) is total seconds of the video (i.e. v_len_second), with minium 8 (i.e. min_video_seconds).\n        video_name (str): Video filename\n    \"\"\"\n    video_name = self.video_names[index]\n    video_path = os.path.join(self.video_dir, video_name)\n\n    spatial_features, video_name = self.video_processing_spatial(video_path)\n    motion_features, video_name = self.video_processing_motion(video_path)\n    # print('spatial_features: ', spatial_features.shape) # torch.Size([8, 3, 448, 448]) for toy dataset\n    # print('motion_features len: ', len(motion_features)) # 8\n    # print('motion_features[0]: ', motion_features[0].shape) # torch.Size([32, 3, 224, 224])\n\n    return spatial_features, motion_features, video_name\n</code></pre>"},{"location":"documentations/datasets/#aigve.datasets.SimpleVQADataset.video_processing_motion","title":"<code>video_processing_motion(video_path)</code>","text":"<p>Extracts motion-based clips suitable for SlowFast.     - Standard input size: It resizes frames to 224 * 224.     - Motion-based clips: Processes at leaset 8-second clips, select 32 consecutive frames from each second. Return:     transformed_video_all (List[torch.Tensor]): Tensor shape[video_length_clip(32), 3, 224, 224].          Len(List) is total seconds of the video, with minium 8.     video_name (str)</p> Source code in <code>aigve/datasets/simplevqa_dataset.py</code> <pre><code>def video_processing_motion(self, video_path):\n    \"\"\"\n    Extracts motion-based clips suitable for SlowFast.\n        - Standard input size: It resizes frames to 224 * 224.\n        - Motion-based clips: Processes at leaset 8-second clips, select 32 consecutive frames from each second.\n    Return:\n        transformed_video_all (List[torch.Tensor]): Tensor shape[video_length_clip(32), 3, 224, 224]. \n            Len(List) is total seconds of the video, with minium 8.\n        video_name (str)\n    \"\"\"\n    video_capture = cv2.VideoCapture(video_path)\n    video_name = os.path.basename(video_path)\n    video_length = int(video_capture.get(cv2.CAP_PROP_FRAME_COUNT))\n    video_frame_rate = int(round(video_capture.get(cv2.CAP_PROP_FPS)))\n\n    transform = transforms.Compose([\n        transforms.Resize([224, 224]),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.45, 0.45, 0.45], std=[0.225, 0.225, 0.225]) # General purpose\n    ])\n    transformed_frame_all = torch.zeros([video_length, 3, 224, 224])\n    video_read_index = 0\n    for i in range(video_length): # All frames extraction\n        has_frames, frame = video_capture.read()\n        if has_frames:\n            read_frame = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n            read_frame = transform(read_frame)\n            transformed_frame_all[video_read_index] = read_frame\n            video_read_index += 1\n\n    # Pads remaining frames by repeating the last available frame.\n    if video_read_index &lt; video_length: \n        for i in range(video_read_index, video_length):\n            transformed_frame_all[i] = transformed_frame_all[video_read_index - 1]\n\n    video_capture.release()\n\n    # Compute the number of total seconds of the video\n    video_clip = int(video_length/video_frame_rate)\n    # print('video_clip (s): ', video_clip)\n    video_length_clip = 32\n    transformed_video_all = []\n\n    # Extract motion-based clips: select 32 consecutive frames from each second\n    for i in range(video_clip):\n        transformed_video = torch.zeros([video_length_clip, 3, 224, 224])\n        if (i * video_frame_rate + video_length_clip) &lt;= video_length: # if the clip can be fully extracted, select 32 consecutive frames starting at i*video_frame_rate\n            transformed_video = transformed_frame_all[i * video_frame_rate:(i * video_frame_rate + video_length_clip)]\n        else: # Copy all rest available frames. Pads remaining frames by repeating the last available frame.\n            transformed_video[:(video_length - i * video_frame_rate)] = transformed_frame_all[i * video_frame_rate:]\n            for j in range((video_length - i * video_frame_rate), video_length_clip):\n                transformed_video[j] = transformed_video[video_length - i * video_frame_rate - 1]\n        transformed_video_all.append(transformed_video)\n\n    if video_clip &lt; self.min_video_seconds:\n        for i in range(video_clip, self.min_video_seconds):\n            transformed_video_all.append(transformed_video_all[video_clip - 1])\n\n    return transformed_video_all, video_name\n</code></pre>"},{"location":"documentations/datasets/#aigve.datasets.SimpleVQADataset.video_processing_spatial","title":"<code>video_processing_spatial(video_path)</code>","text":"<p>Extracts spatial frames with proper resizing and normalization.     - Key frame extraction: It selects 1 frame per second.     - Standard input size: It resizes frames to 448 * 448 (after an initial resize to 520). Return:     transformed_video (torch.Tensor): shape[video_length_read, 3, 448, 448].          <code>video_length_read</code> is total seconds of the video (though 2 for toy dataset) with minium 8 (i.e. min_video_seconds).     video_name (str)</p> Source code in <code>aigve/datasets/simplevqa_dataset.py</code> <pre><code>def video_processing_spatial(self, video_path):\n    \"\"\"\n    Extracts spatial frames with proper resizing and normalization.\n        - Key frame extraction: It selects 1 frame per second.\n        - Standard input size: It resizes frames to 448 * 448 (after an initial resize to 520).\n    Return:\n        transformed_video (torch.Tensor): shape[video_length_read, 3, 448, 448]. \n            `video_length_read` is total seconds of the video (though 2 for toy dataset) with minium 8 (i.e. min_video_seconds).\n        video_name (str)\n    \"\"\"\n    video_capture = cv2.VideoCapture(video_path)\n    video_name = os.path.basename(video_path)\n    video_length = int(video_capture.get(cv2.CAP_PROP_FRAME_COUNT))\n    video_frame_rate = int(round(video_capture.get(cv2.CAP_PROP_FPS)))\n\n    # Compute the number of total seconds of the video\n    video_length_read = int(video_length/video_frame_rate) # math.ceil()\n    # print('video_length_read (s): ', video_length_read)\n    transformations = transforms.Compose([\n        transforms.Resize(520),\n        transforms.CenterCrop(448),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) # Standard ImageNet normalization\n    ])\n    transformed_video = torch.zeros([max(video_length_read, self.min_video_seconds), 3, 448, 448])\n\n    video_read_index = 0\n    frame_idx = 0\n    for i in range(video_length):\n        has_frames, frame = video_capture.read()\n        if has_frames:\n            # Key frames extraction\n            if (video_read_index &lt; video_length_read) and (frame_idx % video_frame_rate == 0):\n                read_frame = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n                read_frame = transformations(read_frame)\n                transformed_video[video_read_index] = read_frame\n                video_read_index += 1\n            frame_idx += 1\n\n    # Pads remaining frames by repeating the last available frame.\n    if video_read_index &lt; self.min_video_seconds:\n        for i in range(video_read_index, self.min_video_seconds):\n            transformed_video[i] = transformed_video[video_read_index - 1]\n\n    video_capture.release()\n    return transformed_video, video_name\n</code></pre>"},{"location":"documentations/datasets/#aigve.datasets.ToyDataset","title":"<code>ToyDataset</code>","text":"<p>               Bases: <code>BaseDataset</code></p> <p>ToyDataset for testing.</p> <p>Parameters:</p> Name Type Description Default <code>data_root</code> <code>str</code> <p>Root directory for data.</p> <code>None</code> <code>ann_file</code> <code>str</code> <p>Annotation file path.</p> <code>''</code> <code>metainfo</code> <code>dict</code> <p>Metadata information.</p> <code>None</code> <code>data_prefix</code> <code>dict</code> <p>Prefix paths for different modalities.</p> <code>None</code> <code>pipeline</code> <code>List[Union[Callable, dict]]</code> <p>Data transformation pipeline.</p> <code>[]</code> <code>modality</code> <code>dict</code> <p>Specifies which modalities are used (video, text, image).</p> <code>dict(use_video=True, use_text=True, use_image=False)</code> <code>image_frame</code> <code>int</code> <p>Number of frames for images.</p> <code>None</code> Source code in <code>aigve/datasets/toy_dataset.py</code> <pre><code>@DATASETS.register_module()\nclass ToyDataset(BaseDataset):\n    \"\"\"ToyDataset for testing.\n\n    Args:\n        data_root (str, optional): Root directory for data.\n        ann_file (str): Annotation file path.\n        metainfo (dict, optional): Metadata information.\n        data_prefix (dict): Prefix paths for different modalities.\n        pipeline (List[Union[Callable, dict]]): Data transformation pipeline.\n        modality (dict): Specifies which modalities are used (video, text, image).\n        image_frame (int, optional): Number of frames for images.\n    \"\"\"\n\n    def __init__(self,\n                 data_root: Optional[str] = None,\n                 ann_file: str = '',\n                 metainfo: Optional[dict] = None,\n                 data_prefix: dict = None,\n                 pipeline: List[Union[Callable, dict]] = [],\n                 modality: dict = dict(use_video=True, use_text=True, use_image=False),\n                 image_frame: int = None,\n                 **kwargs) -&gt; None:\n        super().__init__(\n            data_root=data_root,\n            ann_file=ann_file,\n            metainfo=metainfo,\n            data_prefix=data_prefix,\n            pipeline=pipeline,\n            **kwargs\n        )\n        self.modality = modality\n        self.image_frame = image_frame\n        assert self.modality['use_video'] or self.modality['use_text'], (\n            'Please specify the `modality` (`use_video` '\n            f', `use_text`) for {self.__class__.__name__}')\n\n    def parse_data_info(self, raw_data_info: dict) -&gt; dict:\n        \"\"\"Parse raw data info.\"\"\"\n        info = {}\n        info['img_frame'] = None\n        if self.modality['use_text']:\n            info['prompt_gt'] = osp.join(self.data_prefix.get('video', ''), \n                                         raw_data_info['prompt_gt'])\n\n        if self.modality['use_video'] or self.modality['use_image']:\n            info['video_path_pd'] = osp.join(self.data_prefix.get('video', ''), \n                                     raw_data_info['video_path_pd'])\n            if self.modality['use_image']:\n                info['img_frame'] = self.image_frame\n\n        return info\n</code></pre>"},{"location":"documentations/datasets/#aigve.datasets.ToyDataset.parse_data_info","title":"<code>parse_data_info(raw_data_info)</code>","text":"<p>Parse raw data info.</p> Source code in <code>aigve/datasets/toy_dataset.py</code> <pre><code>def parse_data_info(self, raw_data_info: dict) -&gt; dict:\n    \"\"\"Parse raw data info.\"\"\"\n    info = {}\n    info['img_frame'] = None\n    if self.modality['use_text']:\n        info['prompt_gt'] = osp.join(self.data_prefix.get('video', ''), \n                                     raw_data_info['prompt_gt'])\n\n    if self.modality['use_video'] or self.modality['use_image']:\n        info['video_path_pd'] = osp.join(self.data_prefix.get('video', ''), \n                                 raw_data_info['video_path_pd'])\n        if self.modality['use_image']:\n            info['img_frame'] = self.image_frame\n\n    return info\n</code></pre>"},{"location":"documentations/datasets/#aigve.datasets.VideoPhyDataset","title":"<code>VideoPhyDataset</code>","text":"<p>               Bases: <code>Dataset</code></p> Source code in <code>aigve/datasets/videophy_dataset.py</code> <pre><code>@DATASETS.register_module()\nclass VideoPhyDataset(Dataset):\n    def __init__(self, data_path, video_root_path, hf_token, tokenizer=None, processor=None, max_length=2048, media_tokens=['&lt;image&gt;', '&lt;|video|&gt;'], hf_checkpoint='videophysics/videocon_physics'):\n        \"\"\"\n        Args:\n            data_path (str): Path to the data folder, it should be a json file\n            tokenizer (Tokenizer): Tokenizer object\n            processor (Processor): Processor object\n            max_length (int): Maximum length of the input sequence\n            media_tokens (list): List of media tokens\n        \"\"\"\n        self.dataset = json.load(open(data_path))\n        self.video_root_path = video_root_path\n\n        self.hf_token = hf_token\n        self.hf_checkpoint = hf_checkpoint\n        self.max_length = max_length\n        self.media_tokens = {k: -int(i + 1) for i, k in enumerate(media_tokens)}\n        self.media_lengths = {'&lt;image&gt;': 1 + 64, '&lt;|video|&gt;': 1 + 64}\n        self.bucket = {}\n\n\n        # initialize tokenizer\n        if tokenizer is not None:\n            self.tokenizer = tokenizer\n        else:\n            self.tokenizer = LlamaTokenizer.from_pretrained(self.hf_checkpoint, token=self.hf_token)\n\n        MplugOwlImageProcessor, MplugOwlProcessor = lazy_import_mplug_owl()\n        self.image_processor = MplugOwlImageProcessor.from_pretrained(self.hf_checkpoint)\n        # initialize processor\n        if processor is not None:\n            self.processor = processor\n        else:\n            self.processor = MplugOwlProcessor(self.image_processor, self.tokenizer)\n\n    def __len__(self) -&gt; int:\n        \"\"\"\n        Returns:\n            int: Length of the dataset\n        \"\"\"\n        return self.dataset['metainfo']['length']\n\n    def __getitem__(self, idx):\n        \"\"\"\n        Args:\n            idx (int): Index of the dataset\n        Returns:\n            dict: Dictionary containing the video, text, video path and caption\n        \"\"\"\n        data = self.dataset['dataset_list'][idx]\n        videopath = os.path.join(self.video_root_path, data['video_path_pd'])\n        caption = data['prompt_gt']\n        # video_input = self.processor(videos=[videopath], num_frames=16, return_tensors='pt') # video_pixel_values\n        video_input = self.processor(videos=[videopath], num_frames=32, return_tensors='pt')  # video_pixel_values\n        text_input = self._extract_text_token_from_conversation(caption, self.max_length, idx)\n        item = {'video': video_input, 'text': text_input, 'videopath': videopath, 'caption': caption}\n        return item\n\n    def _extract_text_token_from_conversation(self, data, max_length, index):\n        \"\"\"\n        Extracts the text tokens from the conversation\n        Args:\n            data (str): Conversation\n            max_length (int): Maximum length of the input sequence\n            index (int): Index of the dataset\n        \"\"\"\n        # output enc_chunk\n        enc_chunk = []\n\n        if self.tokenizer.bos_token_id &gt; 0:\n            prompt_chunk = [self.tokenizer.bos_token_id]\n        else:\n            prompt_chunk = []\n\n        # conversation = data[\"completion\"]\n        conversation = data\n\n        # For Text only data\n        if all([media_token not in conversation for media_token in self.media_tokens.keys()]):\n            pattern = '|'.join(map(re.escape, ['AI: ', '\\nHuman: ']))\n            chunk_strs = re.split(f'({pattern})', conversation)\n            prompt_length = -1\n            stop_flag = False\n            for idx, chunk_str in enumerate(chunk_strs):\n                if idx == 0:\n                    enc_chunk = prompt_chunk + \\\n                                self.tokenizer(chunk_str, add_special_tokens=False)[\n                                    'input_ids']\n                    enc_length = len(enc_chunk)\n                    label_chunk = [0] * enc_length\n                else:\n                    if chunk_strs[idx - 1] == 'AI: ':\n                        curr_chunk = self.tokenizer(\n                            chunk_str, add_special_tokens=False)['input_ids']\n                        if enc_length + len(curr_chunk) &gt;= max_length:\n                            curr_chunk = curr_chunk[:max_length - enc_length]\n                            stop_flag = True\n                        curr_chunk += [self.tokenizer.eos_token_id]\n                        enc_length += len(curr_chunk)\n                        enc_chunk += curr_chunk\n                        label_chunk += [1] * len(curr_chunk)\n                    else:\n                        curr_chunk = self.tokenizer(\n                            chunk_str, add_special_tokens=False)['input_ids']\n                        if enc_length + len(curr_chunk) &gt;= max_length + 1:\n                            curr_chunk = curr_chunk[:max_length + 1 - enc_length]\n                            stop_flag = True\n                        enc_length += len(curr_chunk)\n                        enc_chunk += curr_chunk\n                        label_chunk += [0] * len(curr_chunk)\n                    if stop_flag:\n                        break\n\n        # For Image-Text Data\n        else:\n            enc_length = 0\n            prompt_length = -2\n            pattern = '|'.join(\n                map(re.escape, list(self.media_tokens.keys()) + ['AI: ', '\\nHuman: ']))\n            chunk_strs = re.split(f'({pattern})', conversation)\n            chunk_strs = [x for x in chunk_strs if len(x) &gt; 0]\n            for idx, chunk_str in enumerate(chunk_strs):\n                if enc_length &gt;= max_length + 1:\n                    break\n\n                if idx == 0:\n                    enc_chunk = prompt_chunk + \\\n                                self.tokenizer(chunk_str, add_special_tokens=False)[\n                                    'input_ids']\n                    enc_length = len(enc_chunk)\n                    label_chunk = [0] * enc_length\n                else:\n                    if chunk_str in self.media_tokens:\n                        # [CLS] + 256 + [EOS]\n                        if enc_length + self.media_lengths[chunk_str] &gt; max_length + 1:\n                            break\n                        else:\n                            enc_chunk += [self.media_tokens[chunk_str]\n                                          ] * self.media_lengths[chunk_str]\n                            enc_length += self.media_lengths[chunk_str]\n                            label_chunk += [0] * self.media_lengths[chunk_str]\n                    else:\n\n                        if chunk_strs[idx - 1] == 'AI: ':\n                            curr_chunk = self.tokenizer(\n                                chunk_str, add_special_tokens=False)['input_ids']\n                            if enc_length + len(curr_chunk) &gt;= max_length:\n                                curr_chunk = curr_chunk[:max_length - enc_length]\n                            curr_chunk += [self.tokenizer.eos_token_id]\n                            enc_length += len(curr_chunk)\n                            enc_chunk += curr_chunk\n                            label_chunk += [1] * len(curr_chunk)\n                        else:\n                            curr_chunk = self.tokenizer(\n                                chunk_str, add_special_tokens=False)['input_ids']\n                            if enc_length + len(curr_chunk) &gt;= max_length + 1:\n                                curr_chunk = curr_chunk[:max_length +\n                                                         1 - enc_length]\n                            enc_length += len(curr_chunk)\n                            enc_chunk += curr_chunk\n                            label_chunk += [0] * len(curr_chunk)\n\n        if enc_length &lt; max_length + 1:\n            padding_chunk = [self.tokenizer.pad_token_id] * \\\n                            (max_length + 1 - enc_length)\n            padding_length = len(padding_chunk)\n            label_chunk += [0] * (max_length + 1 - enc_length)\n            enc_chunk = enc_chunk + padding_chunk\n        else:\n            padding_length = 0\n\n        assert enc_length + padding_length == max_length + \\\n               1, (index, prompt_length, enc_length,\n                   padding_length, max_length + 1)\n        assert len(label_chunk) == max_length + \\\n               1, (len(label_chunk), max_length + 1)\n        non_padding_mask = [1 if i &lt; enc_length -\n                                 1 else 0 for i in range(max_length)]\n\n        enc_chunk = torch.tensor(enc_chunk).long()\n        non_padding_mask = torch.tensor(non_padding_mask).long()\n        prompt_mask = torch.tensor(label_chunk)[1:].long()\n        prompt_length = torch.tensor([prompt_length]).long()\n\n        # Create loss mask\n        if all([media_token not in conversation for media_token in self.media_tokens.keys()]):\n            non_media_mask = torch.ones_like(non_padding_mask).long()\n        else:\n            tmp_enc_chunk = enc_chunk.clone()\n            tmp_enc_chunk[tmp_enc_chunk &gt;= 0] = 1\n            tmp_enc_chunk[tmp_enc_chunk &lt; 0] = 0\n            non_media_mask = torch.tensor(tmp_enc_chunk).long()\n            non_media_mask = non_media_mask[1:].long()\n        return {'input_ids': enc_chunk, \"prompt_length\": prompt_length, 'seq_length': enc_length,\n                \"non_padding_mask\": non_padding_mask, 'non_media_mask': non_media_mask, 'prompt_mask': prompt_mask}\n</code></pre>"},{"location":"documentations/datasets/#aigve.datasets.VideoPhyDataset.__getitem__","title":"<code>__getitem__(idx)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>idx</code> <code>int</code> <p>Index of the dataset</p> required <p>Returns:     dict: Dictionary containing the video, text, video path and caption</p> Source code in <code>aigve/datasets/videophy_dataset.py</code> <pre><code>def __getitem__(self, idx):\n    \"\"\"\n    Args:\n        idx (int): Index of the dataset\n    Returns:\n        dict: Dictionary containing the video, text, video path and caption\n    \"\"\"\n    data = self.dataset['dataset_list'][idx]\n    videopath = os.path.join(self.video_root_path, data['video_path_pd'])\n    caption = data['prompt_gt']\n    # video_input = self.processor(videos=[videopath], num_frames=16, return_tensors='pt') # video_pixel_values\n    video_input = self.processor(videos=[videopath], num_frames=32, return_tensors='pt')  # video_pixel_values\n    text_input = self._extract_text_token_from_conversation(caption, self.max_length, idx)\n    item = {'video': video_input, 'text': text_input, 'videopath': videopath, 'caption': caption}\n    return item\n</code></pre>"},{"location":"documentations/datasets/#aigve.datasets.VideoPhyDataset.__init__","title":"<code>__init__(data_path, video_root_path, hf_token, tokenizer=None, processor=None, max_length=2048, media_tokens=['&lt;image&gt;', '&lt;|video|&gt;'], hf_checkpoint='videophysics/videocon_physics')</code>","text":"<p>Parameters:</p> Name Type Description Default <code>data_path</code> <code>str</code> <p>Path to the data folder, it should be a json file</p> required <code>tokenizer</code> <code>Tokenizer</code> <p>Tokenizer object</p> <code>None</code> <code>processor</code> <code>Processor</code> <p>Processor object</p> <code>None</code> <code>max_length</code> <code>int</code> <p>Maximum length of the input sequence</p> <code>2048</code> <code>media_tokens</code> <code>list</code> <p>List of media tokens</p> <code>['&lt;image&gt;', '&lt;|video|&gt;']</code> Source code in <code>aigve/datasets/videophy_dataset.py</code> <pre><code>def __init__(self, data_path, video_root_path, hf_token, tokenizer=None, processor=None, max_length=2048, media_tokens=['&lt;image&gt;', '&lt;|video|&gt;'], hf_checkpoint='videophysics/videocon_physics'):\n    \"\"\"\n    Args:\n        data_path (str): Path to the data folder, it should be a json file\n        tokenizer (Tokenizer): Tokenizer object\n        processor (Processor): Processor object\n        max_length (int): Maximum length of the input sequence\n        media_tokens (list): List of media tokens\n    \"\"\"\n    self.dataset = json.load(open(data_path))\n    self.video_root_path = video_root_path\n\n    self.hf_token = hf_token\n    self.hf_checkpoint = hf_checkpoint\n    self.max_length = max_length\n    self.media_tokens = {k: -int(i + 1) for i, k in enumerate(media_tokens)}\n    self.media_lengths = {'&lt;image&gt;': 1 + 64, '&lt;|video|&gt;': 1 + 64}\n    self.bucket = {}\n\n\n    # initialize tokenizer\n    if tokenizer is not None:\n        self.tokenizer = tokenizer\n    else:\n        self.tokenizer = LlamaTokenizer.from_pretrained(self.hf_checkpoint, token=self.hf_token)\n\n    MplugOwlImageProcessor, MplugOwlProcessor = lazy_import_mplug_owl()\n    self.image_processor = MplugOwlImageProcessor.from_pretrained(self.hf_checkpoint)\n    # initialize processor\n    if processor is not None:\n        self.processor = processor\n    else:\n        self.processor = MplugOwlProcessor(self.image_processor, self.tokenizer)\n</code></pre>"},{"location":"documentations/datasets/#aigve.datasets.VideoPhyDataset.__len__","title":"<code>__len__()</code>","text":"<p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>Length of the dataset</p> Source code in <code>aigve/datasets/videophy_dataset.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"\n    Returns:\n        int: Length of the dataset\n    \"\"\"\n    return self.dataset['metainfo']['length']\n</code></pre>"},{"location":"documentations/datasets/#aigve.datasets.VideoPhyDataset._extract_text_token_from_conversation","title":"<code>_extract_text_token_from_conversation(data, max_length, index)</code>","text":"<p>Extracts the text tokens from the conversation Args:     data (str): Conversation     max_length (int): Maximum length of the input sequence     index (int): Index of the dataset</p> Source code in <code>aigve/datasets/videophy_dataset.py</code> <pre><code>def _extract_text_token_from_conversation(self, data, max_length, index):\n    \"\"\"\n    Extracts the text tokens from the conversation\n    Args:\n        data (str): Conversation\n        max_length (int): Maximum length of the input sequence\n        index (int): Index of the dataset\n    \"\"\"\n    # output enc_chunk\n    enc_chunk = []\n\n    if self.tokenizer.bos_token_id &gt; 0:\n        prompt_chunk = [self.tokenizer.bos_token_id]\n    else:\n        prompt_chunk = []\n\n    # conversation = data[\"completion\"]\n    conversation = data\n\n    # For Text only data\n    if all([media_token not in conversation for media_token in self.media_tokens.keys()]):\n        pattern = '|'.join(map(re.escape, ['AI: ', '\\nHuman: ']))\n        chunk_strs = re.split(f'({pattern})', conversation)\n        prompt_length = -1\n        stop_flag = False\n        for idx, chunk_str in enumerate(chunk_strs):\n            if idx == 0:\n                enc_chunk = prompt_chunk + \\\n                            self.tokenizer(chunk_str, add_special_tokens=False)[\n                                'input_ids']\n                enc_length = len(enc_chunk)\n                label_chunk = [0] * enc_length\n            else:\n                if chunk_strs[idx - 1] == 'AI: ':\n                    curr_chunk = self.tokenizer(\n                        chunk_str, add_special_tokens=False)['input_ids']\n                    if enc_length + len(curr_chunk) &gt;= max_length:\n                        curr_chunk = curr_chunk[:max_length - enc_length]\n                        stop_flag = True\n                    curr_chunk += [self.tokenizer.eos_token_id]\n                    enc_length += len(curr_chunk)\n                    enc_chunk += curr_chunk\n                    label_chunk += [1] * len(curr_chunk)\n                else:\n                    curr_chunk = self.tokenizer(\n                        chunk_str, add_special_tokens=False)['input_ids']\n                    if enc_length + len(curr_chunk) &gt;= max_length + 1:\n                        curr_chunk = curr_chunk[:max_length + 1 - enc_length]\n                        stop_flag = True\n                    enc_length += len(curr_chunk)\n                    enc_chunk += curr_chunk\n                    label_chunk += [0] * len(curr_chunk)\n                if stop_flag:\n                    break\n\n    # For Image-Text Data\n    else:\n        enc_length = 0\n        prompt_length = -2\n        pattern = '|'.join(\n            map(re.escape, list(self.media_tokens.keys()) + ['AI: ', '\\nHuman: ']))\n        chunk_strs = re.split(f'({pattern})', conversation)\n        chunk_strs = [x for x in chunk_strs if len(x) &gt; 0]\n        for idx, chunk_str in enumerate(chunk_strs):\n            if enc_length &gt;= max_length + 1:\n                break\n\n            if idx == 0:\n                enc_chunk = prompt_chunk + \\\n                            self.tokenizer(chunk_str, add_special_tokens=False)[\n                                'input_ids']\n                enc_length = len(enc_chunk)\n                label_chunk = [0] * enc_length\n            else:\n                if chunk_str in self.media_tokens:\n                    # [CLS] + 256 + [EOS]\n                    if enc_length + self.media_lengths[chunk_str] &gt; max_length + 1:\n                        break\n                    else:\n                        enc_chunk += [self.media_tokens[chunk_str]\n                                      ] * self.media_lengths[chunk_str]\n                        enc_length += self.media_lengths[chunk_str]\n                        label_chunk += [0] * self.media_lengths[chunk_str]\n                else:\n\n                    if chunk_strs[idx - 1] == 'AI: ':\n                        curr_chunk = self.tokenizer(\n                            chunk_str, add_special_tokens=False)['input_ids']\n                        if enc_length + len(curr_chunk) &gt;= max_length:\n                            curr_chunk = curr_chunk[:max_length - enc_length]\n                        curr_chunk += [self.tokenizer.eos_token_id]\n                        enc_length += len(curr_chunk)\n                        enc_chunk += curr_chunk\n                        label_chunk += [1] * len(curr_chunk)\n                    else:\n                        curr_chunk = self.tokenizer(\n                            chunk_str, add_special_tokens=False)['input_ids']\n                        if enc_length + len(curr_chunk) &gt;= max_length + 1:\n                            curr_chunk = curr_chunk[:max_length +\n                                                     1 - enc_length]\n                        enc_length += len(curr_chunk)\n                        enc_chunk += curr_chunk\n                        label_chunk += [0] * len(curr_chunk)\n\n    if enc_length &lt; max_length + 1:\n        padding_chunk = [self.tokenizer.pad_token_id] * \\\n                        (max_length + 1 - enc_length)\n        padding_length = len(padding_chunk)\n        label_chunk += [0] * (max_length + 1 - enc_length)\n        enc_chunk = enc_chunk + padding_chunk\n    else:\n        padding_length = 0\n\n    assert enc_length + padding_length == max_length + \\\n           1, (index, prompt_length, enc_length,\n               padding_length, max_length + 1)\n    assert len(label_chunk) == max_length + \\\n           1, (len(label_chunk), max_length + 1)\n    non_padding_mask = [1 if i &lt; enc_length -\n                             1 else 0 for i in range(max_length)]\n\n    enc_chunk = torch.tensor(enc_chunk).long()\n    non_padding_mask = torch.tensor(non_padding_mask).long()\n    prompt_mask = torch.tensor(label_chunk)[1:].long()\n    prompt_length = torch.tensor([prompt_length]).long()\n\n    # Create loss mask\n    if all([media_token not in conversation for media_token in self.media_tokens.keys()]):\n        non_media_mask = torch.ones_like(non_padding_mask).long()\n    else:\n        tmp_enc_chunk = enc_chunk.clone()\n        tmp_enc_chunk[tmp_enc_chunk &gt;= 0] = 1\n        tmp_enc_chunk[tmp_enc_chunk &lt; 0] = 0\n        non_media_mask = torch.tensor(tmp_enc_chunk).long()\n        non_media_mask = non_media_mask[1:].long()\n    return {'input_ids': enc_chunk, \"prompt_length\": prompt_length, 'seq_length': enc_length,\n            \"non_padding_mask\": non_padding_mask, 'non_media_mask': non_media_mask, 'prompt_mask': prompt_mask}\n</code></pre>"},{"location":"documentations/datasets/#aigve.datasets.VideoScoreDataset","title":"<code>VideoScoreDataset</code>","text":"<p>               Bases: <code>BaseDataset</code></p> Source code in <code>aigve/datasets/videoscore_dataset.py</code> <pre><code>@DATASETS.register_module()\nclass VideoScoreDataset(BaseDataset):\n    def __init__(self, ann_file='', metainfo=None, data_root='', data_prefix={'video_path_pd': ''}, filter_cfg=None, indices=None,\n                 serialize_data=True, pipeline=[], test_mode=False, lazy_init=False, max_refetch=1000, model_name = None, regression_query_prompt: str = None,\n                max_num_frames: int = None):\n        \"\"\"\n        Args:\n            ann_file (str): annotation file path\n            metainfo (dict): meta information about the dataset\n            data_root (str): the root path of the data\n            data_prefix (dict): the prefix of the data, for example, the prefix of the image path\n            filter_cfg (dict): the filter configuration\n            indices (list): the indices of the data\n            serialize_data (bool): whether to serialize the data\n            pipeline (list): the pipeline of the data\n            test_mode (bool): whether in test mode\n            lazy_init (bool): whether to lazy initialize the dataset\n            max_refetch (int): the maximum number of refetching data\n            model_name (str): the name of the model\n            regression_query_prompt (str): the prompt for the regression query\n            max_num_frames (int): the maximum number of frames\n        \"\"\"\n        super(VideoScoreDataset, self).__init__(ann_file, metainfo, data_root, data_prefix, filter_cfg, indices, serialize_data, pipeline, test_mode, lazy_init, max_refetch)\n        if model_name is None:\n            self.model_name = 'TIGER-Lab/VideoScore-v1.1'\n        else:\n            self.model_name = model_name\n\n        self.processor = AutoProcessor.from_pretrained(self.model_name,torch_dtype=torch.bfloat16)\n\n        if regression_query_prompt is not None:\n            self.regression_query_prompt = regression_query_prompt\n        else:\n            self.regression_query_prompt = '''\n                Suppose you are an expert in judging and evaluating the quality of AI-generated videos,\n                please watch the following frames of a given video and see the text prompt for generating the video,\n                then give scores from 5 different dimensions:\n                (1) visual quality: the quality of the video in terms of clearness, resolution, brightness, and color\n                (2) temporal consistency, both the consistency of objects or humans and the smoothness of motion or movements\n                (3) dynamic degree, the degree of dynamic changes\n                (4) text-to-video alignment, the alignment between the text prompt and the video content\n                (5) factual consistency, the consistency of the video content with the common-sense and factual knowledge\n                for each dimension, output a float number from 1.0 to 4.0,\n                the higher the number is, the better the video performs in that sub-score, \n                the lowest 1.0 means Bad, the highest 4.0 means Perfect/Real (the video is like a real video)\n                Here is an output example:\n                visual quality: 3.2\n                temporal consistency: 2.7\n                dynamic degree: 4.0\n                text-to-video alignment: 2.3\n                factual consistency: 1.8\n                For this video, the text prompt is \"{text_prompt}\",\n                all the frames of video are as follows:\n            '''\n        if max_num_frames is not None:\n            self.max_num_frames = max_num_frames\n        else:\n            self.max_num_frames = 48\n\n    def __len__(self):\n        \"\"\"\n        Returns:\n            int: the length of the dataset\n        \"\"\"\n        return self.metainfo['length']\n\n\n    def __getitem__(self, idx):\n        \"\"\"\n        Args:\n            idx (int): the index of the data\n        \"\"\"\n        anno_info = self.get_data_info(idx)\n        video_path = os.path.join(self.data_root, anno_info['video_path_pd'])\n\n        container = av.open(video_path)\n\n        total_frames = container.streams.video[0].frames\n        if total_frames &gt; self.max_num_frames:\n            indices = np.arange(0, total_frames, total_frames / self.max_num_frames).astype(int)\n        else:\n            indices = np.arange(total_frames)\n\n        frames = [Image.fromarray(x) for x in _read_video_pyav(container, indices)]\n        eval_prompt = self.regression_query_prompt.format(text_prompt=anno_info['prompt_gt'])\n        num_image_token = eval_prompt.count(\"&lt;image&gt;\")\n        if num_image_token &lt; len(frames):\n            eval_prompt += \"&lt;image&gt; \" * (len(frames) - num_image_token)\n\n        flatten_images = []\n        for x in [frames]:\n            if isinstance(x, list):\n                flatten_images.extend(x)\n            else:\n                flatten_images.append(x)\n        flatten_images = [Image.open(x) if isinstance(x, str) else x for x in flatten_images]\n        inputs = self.processor(text=eval_prompt, images=flatten_images, return_tensors=\"pt\")\n        return inputs\n</code></pre>"},{"location":"documentations/datasets/#aigve.datasets.VideoScoreDataset.__getitem__","title":"<code>__getitem__(idx)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>idx</code> <code>int</code> <p>the index of the data</p> required Source code in <code>aigve/datasets/videoscore_dataset.py</code> <pre><code>def __getitem__(self, idx):\n    \"\"\"\n    Args:\n        idx (int): the index of the data\n    \"\"\"\n    anno_info = self.get_data_info(idx)\n    video_path = os.path.join(self.data_root, anno_info['video_path_pd'])\n\n    container = av.open(video_path)\n\n    total_frames = container.streams.video[0].frames\n    if total_frames &gt; self.max_num_frames:\n        indices = np.arange(0, total_frames, total_frames / self.max_num_frames).astype(int)\n    else:\n        indices = np.arange(total_frames)\n\n    frames = [Image.fromarray(x) for x in _read_video_pyav(container, indices)]\n    eval_prompt = self.regression_query_prompt.format(text_prompt=anno_info['prompt_gt'])\n    num_image_token = eval_prompt.count(\"&lt;image&gt;\")\n    if num_image_token &lt; len(frames):\n        eval_prompt += \"&lt;image&gt; \" * (len(frames) - num_image_token)\n\n    flatten_images = []\n    for x in [frames]:\n        if isinstance(x, list):\n            flatten_images.extend(x)\n        else:\n            flatten_images.append(x)\n    flatten_images = [Image.open(x) if isinstance(x, str) else x for x in flatten_images]\n    inputs = self.processor(text=eval_prompt, images=flatten_images, return_tensors=\"pt\")\n    return inputs\n</code></pre>"},{"location":"documentations/datasets/#aigve.datasets.VideoScoreDataset.__init__","title":"<code>__init__(ann_file='', metainfo=None, data_root='', data_prefix={'video_path_pd': ''}, filter_cfg=None, indices=None, serialize_data=True, pipeline=[], test_mode=False, lazy_init=False, max_refetch=1000, model_name=None, regression_query_prompt=None, max_num_frames=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>ann_file</code> <code>str</code> <p>annotation file path</p> <code>''</code> <code>metainfo</code> <code>dict</code> <p>meta information about the dataset</p> <code>None</code> <code>data_root</code> <code>str</code> <p>the root path of the data</p> <code>''</code> <code>data_prefix</code> <code>dict</code> <p>the prefix of the data, for example, the prefix of the image path</p> <code>{'video_path_pd': ''}</code> <code>filter_cfg</code> <code>dict</code> <p>the filter configuration</p> <code>None</code> <code>indices</code> <code>list</code> <p>the indices of the data</p> <code>None</code> <code>serialize_data</code> <code>bool</code> <p>whether to serialize the data</p> <code>True</code> <code>pipeline</code> <code>list</code> <p>the pipeline of the data</p> <code>[]</code> <code>test_mode</code> <code>bool</code> <p>whether in test mode</p> <code>False</code> <code>lazy_init</code> <code>bool</code> <p>whether to lazy initialize the dataset</p> <code>False</code> <code>max_refetch</code> <code>int</code> <p>the maximum number of refetching data</p> <code>1000</code> <code>model_name</code> <code>str</code> <p>the name of the model</p> <code>None</code> <code>regression_query_prompt</code> <code>str</code> <p>the prompt for the regression query</p> <code>None</code> <code>max_num_frames</code> <code>int</code> <p>the maximum number of frames</p> <code>None</code> Source code in <code>aigve/datasets/videoscore_dataset.py</code> <pre><code>def __init__(self, ann_file='', metainfo=None, data_root='', data_prefix={'video_path_pd': ''}, filter_cfg=None, indices=None,\n             serialize_data=True, pipeline=[], test_mode=False, lazy_init=False, max_refetch=1000, model_name = None, regression_query_prompt: str = None,\n            max_num_frames: int = None):\n    \"\"\"\n    Args:\n        ann_file (str): annotation file path\n        metainfo (dict): meta information about the dataset\n        data_root (str): the root path of the data\n        data_prefix (dict): the prefix of the data, for example, the prefix of the image path\n        filter_cfg (dict): the filter configuration\n        indices (list): the indices of the data\n        serialize_data (bool): whether to serialize the data\n        pipeline (list): the pipeline of the data\n        test_mode (bool): whether in test mode\n        lazy_init (bool): whether to lazy initialize the dataset\n        max_refetch (int): the maximum number of refetching data\n        model_name (str): the name of the model\n        regression_query_prompt (str): the prompt for the regression query\n        max_num_frames (int): the maximum number of frames\n    \"\"\"\n    super(VideoScoreDataset, self).__init__(ann_file, metainfo, data_root, data_prefix, filter_cfg, indices, serialize_data, pipeline, test_mode, lazy_init, max_refetch)\n    if model_name is None:\n        self.model_name = 'TIGER-Lab/VideoScore-v1.1'\n    else:\n        self.model_name = model_name\n\n    self.processor = AutoProcessor.from_pretrained(self.model_name,torch_dtype=torch.bfloat16)\n\n    if regression_query_prompt is not None:\n        self.regression_query_prompt = regression_query_prompt\n    else:\n        self.regression_query_prompt = '''\n            Suppose you are an expert in judging and evaluating the quality of AI-generated videos,\n            please watch the following frames of a given video and see the text prompt for generating the video,\n            then give scores from 5 different dimensions:\n            (1) visual quality: the quality of the video in terms of clearness, resolution, brightness, and color\n            (2) temporal consistency, both the consistency of objects or humans and the smoothness of motion or movements\n            (3) dynamic degree, the degree of dynamic changes\n            (4) text-to-video alignment, the alignment between the text prompt and the video content\n            (5) factual consistency, the consistency of the video content with the common-sense and factual knowledge\n            for each dimension, output a float number from 1.0 to 4.0,\n            the higher the number is, the better the video performs in that sub-score, \n            the lowest 1.0 means Bad, the highest 4.0 means Perfect/Real (the video is like a real video)\n            Here is an output example:\n            visual quality: 3.2\n            temporal consistency: 2.7\n            dynamic degree: 4.0\n            text-to-video alignment: 2.3\n            factual consistency: 1.8\n            For this video, the text prompt is \"{text_prompt}\",\n            all the frames of video are as follows:\n        '''\n    if max_num_frames is not None:\n        self.max_num_frames = max_num_frames\n    else:\n        self.max_num_frames = 48\n</code></pre>"},{"location":"documentations/datasets/#aigve.datasets.VideoScoreDataset.__len__","title":"<code>__len__()</code>","text":"<p>Returns:</p> Name Type Description <code>int</code> <p>the length of the dataset</p> Source code in <code>aigve/datasets/videoscore_dataset.py</code> <pre><code>def __len__(self):\n    \"\"\"\n    Returns:\n        int: the length of the dataset\n    \"\"\"\n    return self.metainfo['length']\n</code></pre>"},{"location":"documentations/metrics/","title":"aigve.metrics","text":"<p>This module provides the videos evaluation metrics that can be used within the AIGVE toolkit.</p>"},{"location":"documentations/metrics/#aigve.metrics.BlipSimScore","title":"<code>BlipSimScore</code>","text":"<p>               Bases: <code>BaseMetric</code></p> <p>Initialize the <code>BLIPSimScore</code> evaluator.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>The name of the BLIP model. Defaults to <code>Salesforce/blip-itm-base-coco</code>.</p> <code>'Salesforce/blip-itm-base-coco'</code> <code>logit_scale</code> <code>bool</code> <p>Whether to calcualte the cosine similarity as logits. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <p>None</p> Source code in <code>aigve/metrics/text_video_alignment/similarity_based/blipscore/blipsim.py</code> <pre><code>@METRICS.register_module()\nclass BlipSimScore(BaseMetric):\n    \"\"\" Initialize the ``BLIPSimScore`` evaluator.\n\n    Args:\n            model_name (str): The name of the BLIP model. Defaults to ``Salesforce/blip-itm-base-coco``.\n            logit_scale (bool): Whether to calcualte the cosine similarity as logits. Defaults to False.\n\n    Returns:\n            None\n    \"\"\"\n    def __init__(self,\n                 model_name: str = \"Salesforce/blip-itm-base-coco\",\n                 logit_scale: bool = False,\n                 ) -&gt; None:\n        super().__init__()\n        self.model_name = model_name\n        self.logit_scale = logit_scale\n\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self.model = BlipForImageTextRetrieval.from_pretrained(self.model_name).to(self.device)\n        self.model.eval()\n\n\n# def process(self, data_batch: dict, data_samples: Sequence[dict]) -&gt; None:\n    def process(self, data_batch: Sequence, data_samples: Sequence) -&gt; None:\n        \"\"\"BLIPSimScore process\n        Process one batch of data samples and predictions. The processed\n        results should be stored in ``self.results``, which will be used to\n        compute the metrics when all batches have been processed.\n\n        Args:\n            data_batch (Sequence): A batch of data from the dataloader.\n            data_samples (Sequence): A batch of data samples that\n                contain annotations and predictions.\n        \"\"\"\n\n        result = dict()\n\n        input_prompts, input_videos = data_samples  \n        bsz = len(input_prompts)\n\n        # Ensure prompt_input is a tensor\n        if isinstance(input_prompts, tuple):\n            input_prompts = list(input_prompts)\n\n        if isinstance(input_videos, tuple):\n            input_videos = list(input_videos)\n\n\n        # Initialize an empty tensor to store the concatenated features\n        blip_score_sum, blip_score_cnt = 0, 0\n        logit_scale = self.model.logit_scale.exp() if self.logit_scale else 1\n        with torch.no_grad():\n            for input_prompt, input_frames in zip(input_prompts, input_videos):\n                # If frame is a tuple, extract the tensor. Assume tensor is the first element.\n                # if isinstance(input_prompt_frame_pair, tuple):\n                #     input_prompt_frame_pair = input_prompt_frame_pair[0]\n\n                # for key, value in input_prompt_frame_pair.items():\n                #     if isinstance(value, list):\n                #         input_prompt_frame_pair[key] = value[0]\n\n                # input_prompt_frame_pair = input_prompt_frame_pair.to(\"cuda\")  # Add batch dimension and move the frame to the device\n                # blip_cosine_sim_score = self.model(**input_prompt_frame_pair, use_itm_head=False)[0].item()\n                # blip_scores.append(blip_cosine_sim_score)\n                input_prompt = input_prompt.to(self.device)\n                input_frames = input_frames.to(self.device)\n                blip_cosine_sim_score = self.model(input_ids=input_prompt, pixel_values=input_frames, use_itm_head=False)[0].mean().item()\n                blip_cosine_sim_score *= logit_scale\n                print('current blip cosine similarity score', blip_cosine_sim_score)\n                blip_score_sum += blip_cosine_sim_score\n                blip_score_cnt += 1\n\n        # Calculate the average BLIP score across all frames\n        blip_score_frames_avg = blip_score_sum/blip_score_cnt\n\n        result['blip_sim_score'] = blip_score_frames_avg\n\n        self.results.append(result)\n\n\n    def compute_metrics(self, results: list) -&gt; Dict[str, float]:\n        \"\"\"Compute the metrics from processed results.\n\n        Args:\n            results (list): The processed results of each batch.\n\n        Returns:\n            Dict[str, float]: The computed metrics. The keys are the names of\n            the metrics, and the values are corresponding results.\n        \"\"\"\n        logger: MMLogger = MMLogger.get_current_instance()\n\n        blip_score_np = np.zeros(len(results))\n        for i, result in enumerate(results):\n            blip_score_np[i] = result['blip_sim_score']\n\n        blip_sim_mean = np.mean(blip_score_np) \n\n        print(\"Test results: blip similarity score={:.4f}\"\n              .format(blip_sim_mean))\n\n        return result\n</code></pre>"},{"location":"documentations/metrics/#aigve.metrics.BlipSimScore.compute_metrics","title":"<code>compute_metrics(results)</code>","text":"<p>Compute the metrics from processed results.</p> <p>Parameters:</p> Name Type Description Default <code>results</code> <code>list</code> <p>The processed results of each batch.</p> required <p>Returns:</p> Type Description <code>Dict[str, float]</code> <p>Dict[str, float]: The computed metrics. The keys are the names of</p> <code>Dict[str, float]</code> <p>the metrics, and the values are corresponding results.</p> Source code in <code>aigve/metrics/text_video_alignment/similarity_based/blipscore/blipsim.py</code> <pre><code>def compute_metrics(self, results: list) -&gt; Dict[str, float]:\n    \"\"\"Compute the metrics from processed results.\n\n    Args:\n        results (list): The processed results of each batch.\n\n    Returns:\n        Dict[str, float]: The computed metrics. The keys are the names of\n        the metrics, and the values are corresponding results.\n    \"\"\"\n    logger: MMLogger = MMLogger.get_current_instance()\n\n    blip_score_np = np.zeros(len(results))\n    for i, result in enumerate(results):\n        blip_score_np[i] = result['blip_sim_score']\n\n    blip_sim_mean = np.mean(blip_score_np) \n\n    print(\"Test results: blip similarity score={:.4f}\"\n          .format(blip_sim_mean))\n\n    return result\n</code></pre>"},{"location":"documentations/metrics/#aigve.metrics.BlipSimScore.process","title":"<code>process(data_batch, data_samples)</code>","text":"<p>BLIPSimScore process Process one batch of data samples and predictions. The processed results should be stored in <code>self.results</code>, which will be used to compute the metrics when all batches have been processed.</p> <p>Parameters:</p> Name Type Description Default <code>data_batch</code> <code>Sequence</code> <p>A batch of data from the dataloader.</p> required <code>data_samples</code> <code>Sequence</code> <p>A batch of data samples that contain annotations and predictions.</p> required Source code in <code>aigve/metrics/text_video_alignment/similarity_based/blipscore/blipsim.py</code> <pre><code>def process(self, data_batch: Sequence, data_samples: Sequence) -&gt; None:\n    \"\"\"BLIPSimScore process\n    Process one batch of data samples and predictions. The processed\n    results should be stored in ``self.results``, which will be used to\n    compute the metrics when all batches have been processed.\n\n    Args:\n        data_batch (Sequence): A batch of data from the dataloader.\n        data_samples (Sequence): A batch of data samples that\n            contain annotations and predictions.\n    \"\"\"\n\n    result = dict()\n\n    input_prompts, input_videos = data_samples  \n    bsz = len(input_prompts)\n\n    # Ensure prompt_input is a tensor\n    if isinstance(input_prompts, tuple):\n        input_prompts = list(input_prompts)\n\n    if isinstance(input_videos, tuple):\n        input_videos = list(input_videos)\n\n\n    # Initialize an empty tensor to store the concatenated features\n    blip_score_sum, blip_score_cnt = 0, 0\n    logit_scale = self.model.logit_scale.exp() if self.logit_scale else 1\n    with torch.no_grad():\n        for input_prompt, input_frames in zip(input_prompts, input_videos):\n            # If frame is a tuple, extract the tensor. Assume tensor is the first element.\n            # if isinstance(input_prompt_frame_pair, tuple):\n            #     input_prompt_frame_pair = input_prompt_frame_pair[0]\n\n            # for key, value in input_prompt_frame_pair.items():\n            #     if isinstance(value, list):\n            #         input_prompt_frame_pair[key] = value[0]\n\n            # input_prompt_frame_pair = input_prompt_frame_pair.to(\"cuda\")  # Add batch dimension and move the frame to the device\n            # blip_cosine_sim_score = self.model(**input_prompt_frame_pair, use_itm_head=False)[0].item()\n            # blip_scores.append(blip_cosine_sim_score)\n            input_prompt = input_prompt.to(self.device)\n            input_frames = input_frames.to(self.device)\n            blip_cosine_sim_score = self.model(input_ids=input_prompt, pixel_values=input_frames, use_itm_head=False)[0].mean().item()\n            blip_cosine_sim_score *= logit_scale\n            print('current blip cosine similarity score', blip_cosine_sim_score)\n            blip_score_sum += blip_cosine_sim_score\n            blip_score_cnt += 1\n\n    # Calculate the average BLIP score across all frames\n    blip_score_frames_avg = blip_score_sum/blip_score_cnt\n\n    result['blip_sim_score'] = blip_score_frames_avg\n\n    self.results.append(result)\n</code></pre>"},{"location":"documentations/metrics/#aigve.metrics.CLIPSimScore","title":"<code>CLIPSimScore</code>","text":"<p>               Bases: <code>BaseMetric</code></p> <p>Initialize the <code>CLIPSimScore</code> evaluator.</p> <p>Parameters:</p> Name Type Description Default <code>processor_name</code> <code>str</code> <p>The name of the CLIP processor, which wraps a CLIP feature extractor and a CLIP tokenizer into this single procesor.                Defaults to <code>openai/clip-vit-base-patch32</code>.</p> <code>'openai/clip-vit-base-patch32'</code> <code>model_name</code> <code>str</code> <p>The name of the CLIP model. Defaults to <code>openai/clip-vit-base-patch32</code>.</p> <code>'openai/clip-vit-base-patch32'</code> <code>logit_scale</code> <code>bool</code> <p>Whether to calcualte the cosine similarity as logits. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <p>None</p> Source code in <code>aigve/metrics/text_video_alignment/similarity_based/clipscore/clipsim.py</code> <pre><code>@METRICS.register_module()\nclass CLIPSimScore(BaseMetric):\n    \"\"\" Initialize the ``CLIPSimScore`` evaluator.\n\n    Args:\n            processor_name (str): The name of the CLIP processor, which wraps a CLIP feature extractor and a CLIP tokenizer into this single procesor. \n                                  Defaults to ``openai/clip-vit-base-patch32``.\n            model_name (str): The name of the CLIP model. Defaults to ``openai/clip-vit-base-patch32``.\n            logit_scale (bool): Whether to calcualte the cosine similarity as logits. Defaults to False.\n\n    Returns:\n            None\n    \"\"\"\n    def __init__(self,\n                 processor_name: str = \"openai/clip-vit-base-patch32\",\n                 model_name: str = \"openai/clip-vit-base-patch32\",\n                 logit_scale: bool = False,\n                #  train_index: int = 4\n                 ) -&gt; None:\n        super().__init__()\n        self.processor_name = processor_name\n        self.model_name = model_name\n        self.logit_scale = logit_scale\n\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self.processor = AutoProcessor.from_pretrained(self.processor_name)\n        self.model = CLIPModel.from_pretrained(self.model_name).to(self.device)\n        self.model.eval()\n\n\n# def process(self, data_batch: dict, data_samples: Sequence[dict]) -&gt; None:\n    def process(self, data_batch: Sequence, data_samples: Sequence) -&gt; None:\n        \"\"\"CLIPSimScore process\n        Process one batch of data samples and predictions. The processed\n        results should be stored in ``self.results``, which will be used to\n        compute the metrics when all batches have been processed.\n\n        Args:\n            data_batch (Sequence): A batch of data from the dataloader.\n            data_samples (Sequence): A batch of data samples that\n                contain annotations and predictions.\n        \"\"\"\n\n        result = dict()\n\n        input_prompts, input_videos = data_samples\n        bsz = len(input_prompts)\n\n        # Ensure prompt_input is a tensor\n        if isinstance(input_prompts, tuple):\n            input_prompts = list(input_prompts)\n\n        if isinstance(input_videos, tuple):\n            input_videos = list(input_videos)\n\n        # Initialize an empty list to store each similarity score\n        clip_score_sum, clip_score_cnt = 0, 0\n        logit_scale = self.model.logit_scale.exp() if self.logit_scale else 1\n        with torch.no_grad():\n            for input_prompt, input_frames in zip(input_prompts, input_videos):\n                input_prompt = input_prompt.to(self.device)\n                text_feature = self.model.get_text_features(input_prompt) # [bsz, hid_dim]\n                text_feature = text_feature / torch.norm(text_feature, dim=-1, keepdim=True)\n\n                input_frames = input_frames.to(self.device)  # Add batch dimension and move the frame to the device\n                frame_feature = self.model.get_image_features(input_frames)\n                frame_feature = frame_feature / torch.norm(frame_feature, dim=-1, keepdim=True)\n\n                clip_score = logit_scale * (frame_feature @ text_feature.T).mean().item()\n                print('current clip similarity score', clip_score)\n                clip_score_sum += clip_score\n                clip_score_cnt += 1\n\n        # Calculate the average CLIP score across all frames\n        clip_score_videos_avg = clip_score_sum/clip_score_cnt\n\n        result['clip_sim_score'] = clip_score_videos_avg\n\n        self.results.append(result)\n\n\n    def compute_metrics(self, results: list) -&gt; Dict[str, float]:\n        \"\"\"Compute the metrics from processed results.\n\n        Args:\n            results (list): The processed results of each batch.\n\n        Returns:\n            Dict[str, float]: The computed metrics. The keys are the names of\n            the metrics, and the values are corresponding results.\n        \"\"\"\n        logger: MMLogger = MMLogger.get_current_instance()\n\n        clip_score_np = np.zeros(len(results))\n        for i, result in enumerate(results):\n            clip_score_np[i] = result['clip_sim_score']\n\n        clip_sim_mean = np.mean(clip_score_np) \n\n        print(\"Test results: clip similarity score={:.4f}\"\n              .format(clip_sim_mean))\n\n        return result\n</code></pre>"},{"location":"documentations/metrics/#aigve.metrics.CLIPSimScore.compute_metrics","title":"<code>compute_metrics(results)</code>","text":"<p>Compute the metrics from processed results.</p> <p>Parameters:</p> Name Type Description Default <code>results</code> <code>list</code> <p>The processed results of each batch.</p> required <p>Returns:</p> Type Description <code>Dict[str, float]</code> <p>Dict[str, float]: The computed metrics. The keys are the names of</p> <code>Dict[str, float]</code> <p>the metrics, and the values are corresponding results.</p> Source code in <code>aigve/metrics/text_video_alignment/similarity_based/clipscore/clipsim.py</code> <pre><code>def compute_metrics(self, results: list) -&gt; Dict[str, float]:\n    \"\"\"Compute the metrics from processed results.\n\n    Args:\n        results (list): The processed results of each batch.\n\n    Returns:\n        Dict[str, float]: The computed metrics. The keys are the names of\n        the metrics, and the values are corresponding results.\n    \"\"\"\n    logger: MMLogger = MMLogger.get_current_instance()\n\n    clip_score_np = np.zeros(len(results))\n    for i, result in enumerate(results):\n        clip_score_np[i] = result['clip_sim_score']\n\n    clip_sim_mean = np.mean(clip_score_np) \n\n    print(\"Test results: clip similarity score={:.4f}\"\n          .format(clip_sim_mean))\n\n    return result\n</code></pre>"},{"location":"documentations/metrics/#aigve.metrics.CLIPSimScore.process","title":"<code>process(data_batch, data_samples)</code>","text":"<p>CLIPSimScore process Process one batch of data samples and predictions. The processed results should be stored in <code>self.results</code>, which will be used to compute the metrics when all batches have been processed.</p> <p>Parameters:</p> Name Type Description Default <code>data_batch</code> <code>Sequence</code> <p>A batch of data from the dataloader.</p> required <code>data_samples</code> <code>Sequence</code> <p>A batch of data samples that contain annotations and predictions.</p> required Source code in <code>aigve/metrics/text_video_alignment/similarity_based/clipscore/clipsim.py</code> <pre><code>def process(self, data_batch: Sequence, data_samples: Sequence) -&gt; None:\n    \"\"\"CLIPSimScore process\n    Process one batch of data samples and predictions. The processed\n    results should be stored in ``self.results``, which will be used to\n    compute the metrics when all batches have been processed.\n\n    Args:\n        data_batch (Sequence): A batch of data from the dataloader.\n        data_samples (Sequence): A batch of data samples that\n            contain annotations and predictions.\n    \"\"\"\n\n    result = dict()\n\n    input_prompts, input_videos = data_samples\n    bsz = len(input_prompts)\n\n    # Ensure prompt_input is a tensor\n    if isinstance(input_prompts, tuple):\n        input_prompts = list(input_prompts)\n\n    if isinstance(input_videos, tuple):\n        input_videos = list(input_videos)\n\n    # Initialize an empty list to store each similarity score\n    clip_score_sum, clip_score_cnt = 0, 0\n    logit_scale = self.model.logit_scale.exp() if self.logit_scale else 1\n    with torch.no_grad():\n        for input_prompt, input_frames in zip(input_prompts, input_videos):\n            input_prompt = input_prompt.to(self.device)\n            text_feature = self.model.get_text_features(input_prompt) # [bsz, hid_dim]\n            text_feature = text_feature / torch.norm(text_feature, dim=-1, keepdim=True)\n\n            input_frames = input_frames.to(self.device)  # Add batch dimension and move the frame to the device\n            frame_feature = self.model.get_image_features(input_frames)\n            frame_feature = frame_feature / torch.norm(frame_feature, dim=-1, keepdim=True)\n\n            clip_score = logit_scale * (frame_feature @ text_feature.T).mean().item()\n            print('current clip similarity score', clip_score)\n            clip_score_sum += clip_score\n            clip_score_cnt += 1\n\n    # Calculate the average CLIP score across all frames\n    clip_score_videos_avg = clip_score_sum/clip_score_cnt\n\n    result['clip_sim_score'] = clip_score_videos_avg\n\n    self.results.append(result)\n</code></pre>"},{"location":"documentations/metrics/#aigve.metrics.CLIPTempScore","title":"<code>CLIPTempScore</code>","text":"<p>               Bases: <code>BaseMetric</code></p> <p>Initialize the <code>CLIPTempScore</code> evaluator.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>The name of the CLIP encoder model. Defaults to <code>openai/clip-vit-base-patch32</code>.</p> <code>'openai/clip-vit-base-patch32'</code> <code>logit_scale</code> <code>bool</code> <p>Whether to calcualte the cosine similarity as logits. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <p>None</p> Source code in <code>aigve/metrics/text_video_alignment/similarity_based/clipscore/cliptemp.py</code> <pre><code>@METRICS.register_module()\nclass CLIPTempScore(BaseMetric):\n    \"\"\" Initialize the ``CLIPTempScore`` evaluator.\n\n    Args:\n            model_name (str): The name of the CLIP encoder model. Defaults to ``openai/clip-vit-base-patch32``.\n            logit_scale (bool): Whether to calcualte the cosine similarity as logits. Defaults to False.\n\n    Returns:\n            None\n    \"\"\"\n    def __init__(self,\n                 model_name: str = \"openai/clip-vit-base-patch32\",\n                 logit_scale: bool = False,\n                #  train_index: int = 4\n                 ) -&gt; None:\n        super().__init__()\n        self.model_name = model_name\n        self.logit_scale = logit_scale\n\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self.model = CLIPModel.from_pretrained(self.model_name).to(self.device)\n        self.model.eval()\n\n\n# def process(self, data_batch: dict, data_samples: Sequence[dict]) -&gt; None:\n    def process(self, data_batch: Sequence, data_samples: Sequence) -&gt; None:\n        \"\"\"CLIPTempScore process\n        Process one batch of data samples and predictions. The processed\n        results should be stored in ``self.results``, which will be used to\n        compute the metrics when all batches have been processed.\n\n        Args:\n            data_batch (Sequence): A batch of data from the dataloader.\n            data_samples (Sequence): A batch of data samples that\n                contain annotations and predictions.\n        \"\"\"\n\n        result = dict()\n\n        input_videos = data_samples\n        # bsz = len(input_videos)\n\n\n        # Ensure prompt_input is a tensor        \n        if isinstance(input_videos, tuple):\n            input_videos = list(input_videos)\n\n        # Generate embeddings for each frame and concatenate the features\n        clip_temp_score_sum, clip_temp_score_cnt = 0, 0\n        logit_scale = self.model.logit_scale.exp() if self.logit_scale else 1\n        with torch.no_grad():  \n            for input_frames in input_videos: # Too many frames in a video, must split before CLIP embedding, limited by the memory\n                input_frames = input_frames.to(self.device)\n                frame_feature = self.model.get_image_features(input_frames)\n                frame_feature = frame_feature / torch.norm(frame_feature, dim=-1, keepdim=True)\n                # print(frame_feature.shape)\n\n                clip_temp_score_list = []\n                for i in range(frame_feature.shape[0]-1):\n                    clip_temp_score = logit_scale * frame_feature[i].unsqueeze(0) @ frame_feature[i+1].unsqueeze(0).T\n                    clip_temp_score = clip_temp_score.item()\n                    # print(clip_temp_score)\n                    clip_temp_score_list.append(clip_temp_score)\n                clip_temp_cur_avg_score = sum(clip_temp_score_list)/len(clip_temp_score_list)\n                clip_temp_score_sum += clip_temp_cur_avg_score\n                clip_temp_score_cnt += 1\n                print('current clip temp similarity score', clip_temp_cur_avg_score)\n\n        clip_temp_score_avg = clip_temp_score_sum/clip_temp_score_cnt\n\n        result['clip_temp_score'] = clip_temp_score_avg\n\n        self.results.append(result)\n\n\n    def compute_metrics(self, results: list) -&gt; Dict[str, float]:\n        \"\"\"Compute the metrics from processed results.\n\n        Args:\n            results (list): The processed results of each batch.\n\n        Returns:\n            Dict[str, float]: The computed metrics. The keys are the names of\n            the metrics, and the values are corresponding results.\n        \"\"\"\n        logger: MMLogger = MMLogger.get_current_instance()\n\n        clip_score_np = np.zeros(len(results))\n        for i, result in enumerate(results):\n            clip_score_np[i] = result['clip_temp_score']\n\n        clip_temp_mean = np.mean(clip_score_np) \n\n        print(\"Test results: clip temporal consistency score={:.4f}\"\n              .format(clip_temp_mean))\n\n        return result\n</code></pre>"},{"location":"documentations/metrics/#aigve.metrics.CLIPTempScore.compute_metrics","title":"<code>compute_metrics(results)</code>","text":"<p>Compute the metrics from processed results.</p> <p>Parameters:</p> Name Type Description Default <code>results</code> <code>list</code> <p>The processed results of each batch.</p> required <p>Returns:</p> Type Description <code>Dict[str, float]</code> <p>Dict[str, float]: The computed metrics. The keys are the names of</p> <code>Dict[str, float]</code> <p>the metrics, and the values are corresponding results.</p> Source code in <code>aigve/metrics/text_video_alignment/similarity_based/clipscore/cliptemp.py</code> <pre><code>def compute_metrics(self, results: list) -&gt; Dict[str, float]:\n    \"\"\"Compute the metrics from processed results.\n\n    Args:\n        results (list): The processed results of each batch.\n\n    Returns:\n        Dict[str, float]: The computed metrics. The keys are the names of\n        the metrics, and the values are corresponding results.\n    \"\"\"\n    logger: MMLogger = MMLogger.get_current_instance()\n\n    clip_score_np = np.zeros(len(results))\n    for i, result in enumerate(results):\n        clip_score_np[i] = result['clip_temp_score']\n\n    clip_temp_mean = np.mean(clip_score_np) \n\n    print(\"Test results: clip temporal consistency score={:.4f}\"\n          .format(clip_temp_mean))\n\n    return result\n</code></pre>"},{"location":"documentations/metrics/#aigve.metrics.CLIPTempScore.process","title":"<code>process(data_batch, data_samples)</code>","text":"<p>CLIPTempScore process Process one batch of data samples and predictions. The processed results should be stored in <code>self.results</code>, which will be used to compute the metrics when all batches have been processed.</p> <p>Parameters:</p> Name Type Description Default <code>data_batch</code> <code>Sequence</code> <p>A batch of data from the dataloader.</p> required <code>data_samples</code> <code>Sequence</code> <p>A batch of data samples that contain annotations and predictions.</p> required Source code in <code>aigve/metrics/text_video_alignment/similarity_based/clipscore/cliptemp.py</code> <pre><code>def process(self, data_batch: Sequence, data_samples: Sequence) -&gt; None:\n    \"\"\"CLIPTempScore process\n    Process one batch of data samples and predictions. The processed\n    results should be stored in ``self.results``, which will be used to\n    compute the metrics when all batches have been processed.\n\n    Args:\n        data_batch (Sequence): A batch of data from the dataloader.\n        data_samples (Sequence): A batch of data samples that\n            contain annotations and predictions.\n    \"\"\"\n\n    result = dict()\n\n    input_videos = data_samples\n    # bsz = len(input_videos)\n\n\n    # Ensure prompt_input is a tensor        \n    if isinstance(input_videos, tuple):\n        input_videos = list(input_videos)\n\n    # Generate embeddings for each frame and concatenate the features\n    clip_temp_score_sum, clip_temp_score_cnt = 0, 0\n    logit_scale = self.model.logit_scale.exp() if self.logit_scale else 1\n    with torch.no_grad():  \n        for input_frames in input_videos: # Too many frames in a video, must split before CLIP embedding, limited by the memory\n            input_frames = input_frames.to(self.device)\n            frame_feature = self.model.get_image_features(input_frames)\n            frame_feature = frame_feature / torch.norm(frame_feature, dim=-1, keepdim=True)\n            # print(frame_feature.shape)\n\n            clip_temp_score_list = []\n            for i in range(frame_feature.shape[0]-1):\n                clip_temp_score = logit_scale * frame_feature[i].unsqueeze(0) @ frame_feature[i+1].unsqueeze(0).T\n                clip_temp_score = clip_temp_score.item()\n                # print(clip_temp_score)\n                clip_temp_score_list.append(clip_temp_score)\n            clip_temp_cur_avg_score = sum(clip_temp_score_list)/len(clip_temp_score_list)\n            clip_temp_score_sum += clip_temp_cur_avg_score\n            clip_temp_score_cnt += 1\n            print('current clip temp similarity score', clip_temp_cur_avg_score)\n\n    clip_temp_score_avg = clip_temp_score_sum/clip_temp_score_cnt\n\n    result['clip_temp_score'] = clip_temp_score_avg\n\n    self.results.append(result)\n</code></pre>"},{"location":"documentations/metrics/#aigve.metrics.DSGScore","title":"<code>DSGScore</code>","text":"<p>               Bases: <code>BaseMetric</code></p> <p>Initialize the <code>DSGScore</code> evaluator.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>The name of the VQA model used in the DSGScore evaluator. Defaults to <code>InstructBLIP</code>, you can also choose the \"MPLUG\" as the VQA model.</p> required <code>verbose</code> <code>bool</code> <p>Whether the intermediate output processes is required. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <p>None</p> Source code in <code>aigve/metrics/text_video_alignment/gpt_based/dsg/dsg_eval.py</code> <pre><code>@METRICS.register_module()\nclass DSGScore(BaseMetric):\n    \"\"\" Initialize the ``DSGScore`` evaluator.\n\n    Args:\n            model_name (str): The name of the VQA model used in the DSGScore evaluator. Defaults to ``InstructBLIP``, you can also choose the \"MPLUG\" as the VQA model.\n            verbose (bool): Whether the intermediate output processes is required. Defaults to False.\n\n    Returns:\n            None\n    \"\"\"\n    def __init__(self, \n                 vqa_model_name: str = \"InstructBLIP\",\n                 verbose: bool = False):\n        super().__init__()\n\n        self.submodel_path = 'metrics/text_video_alignment/gpt_based/dsg'\n        if not submodule_exists(self.submodel_path):\n            add_git_submodule(\n                repo_url='https://github.com/j-min/DSG.git', \n                submodule_path=self.submodel_path\n            )     \n        from .DSG.dsg.vqa_utils import MPLUG, InstructBLIP\n\n        self.vqa_model_name = vqa_model_name\n        assert self.vqa_model_name in [\"InstructBLIP\", \"MPLUG\"]\n        if self.vqa_model_name == 'InstructBLIP':\n            self.vqa_model = InstructBLIP()\n        else:\n            self.vqa_model = MPLUG()\n\n        self.verbose = verbose\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\n    def evaluate_image_dsg(self, qid_list, frame_index, frame):\n        \"\"\" Evaluate a generated image with DSG evaluator; this is the intermediate process of the ``process`` function. \n\n        Args:\n                qid_list (List[str]): The list of DSG parse question generation results.\n                frame_index (int): The index number of the currently evaluated frame.\n                frame (List[List[float]): The current evaluated frame.\n\n        Returns:\n                None\n        \"\"\"\n        if self.verbose:\n            print(\"#\"*50)\n            print(\"2) Answer questions given the generated image, with VQA\")\n            print(\"#\"*50)\n\n        # 2) answer questions with the generated image\n        qid2answer = {}\n        qid2scores = {}\n\n        qid2tuple, qid2dependency, qid2question = qid_list\n        for id, question in qid2question.items():\n            answer = self.vqa_model.vqa(image=frame, question=question)\n            print(answer)\n            qid2answer[id] = answer\n            qid2scores[id] = float('yes' in answer)\n\n        average_score_without_dep = sum(qid2scores.values()) / len(qid2scores)\n        print(average_score_without_dep, qid2answer, qid2scores)\n\n        if self.verbose:\n            print(\"#\"*50)\n            print(\"3) Zero-out scores from invalid questions\")\n            print(\"#\"*50)\n\n        # 3) zero-out scores from invalid questions \n        qid2validity = {}\n        qid2scores_after_filtering = deepcopy(qid2scores)\n\n        # print('qid2scores', qid2scores)\n        # print('qid2dependency', qid2dependency)\n        for id, parent_ids in qid2dependency.items():\n            # zero-out scores if parent questions are answered 'no'\n            any_parent_answered_no = False\n            for parent_id in parent_ids:\n                parent_id = list(parent_id)[0]\n                if parent_id == 0:\n                    continue\n                if qid2scores[parent_id] == 0:\n                    any_parent_answered_no = True\n                    break\n            if any_parent_answered_no:\n                qid2scores_after_filtering[id] = 0.0\n                qid2validity[id] = False\n            else:\n                qid2validity[id] = True\n\n        if self.verbose:\n            print(\"Per-quesiton eval results (after using dependency)\")\n            for id in qid2question:\n                print(\"ID\", id)\n                print(\"question\", qid2question[id])\n                print(\"answer\", qid2answer[id])\n                print(\"validity\", qid2validity[id])\n                print(\"score (before filtering)\", qid2scores[id])\n                print(\"score (after filtering)\", qid2scores_after_filtering[id])\n                print()\n\n        if self.verbose:\n            print(\"#\"*50)\n            print(\"4) Calculate the final score by averaging\")\n            print(\"#\"*50)\n\n        average_score_with_dep = sum(qid2scores_after_filtering.values()) / len(qid2scores)\n\n        return {\n            'frame_index': frame_index,\n            'qid2tuple': qid2tuple,\n            'qid2dependency': qid2dependency,\n            'qid2question': qid2question,\n            'qid2answer': qid2answer,\n            'qid2scores': qid2scores,\n            'qid2validity': qid2validity,\n            'average_score_with_dependency': average_score_with_dep,\n            'average_score_without_dependency': average_score_without_dep\n        }\n\n\n    def process(self, data_batch: Sequence, data_samples: Sequence) -&gt; None:\n        \"\"\"DSGScore process\n\n        Process one batch of data samples and predictions. The processed\n        results should be stored in ``self.results``, which will be used to\n        compute the metrics when all batches have been processed.\n\n        Args:\n            data_batch (Sequence): A batch of data from the dataloader.\n            data_samples (Sequence): A batch of data samples that\n                contain annotations and predictions.\n        \"\"\"\n\n        result = dict()\n\n        input_qid_lists, input_videos = data_samples\n        bsz = len(input_qid_lists)\n        # print('input_qid_lists: ', input_qid_lists)\n\n        # Ensure prompt_input is a tensor\n        if isinstance(input_qid_lists, tuple):\n            input_qid_lists = list(input_qid_lists)\n\n        if isinstance(input_videos, tuple):\n            input_videos = list(input_videos)\n\n        average_dep_score_list, average_wo_dep_score_list = [], []\n        for input_qid_list, input_video in zip([input_qid_lists], input_videos):\n            evaluate_dict_list = []\n            dep_score, wo_dep_score = [], []\n            for index, frame in enumerate(input_video):\n                # print('input_qid_list: ', input_qid_list)\n                evaluate_dict = self.evaluate_image_dsg(qid_list=input_qid_list, \n                                                        frame_index=index, \n                                                        frame=frame)\n                evaluate_dict_list.append(evaluate_dict)\n                frame_average_score_with_dependency = evaluate_dict['average_score_with_dependency']\n                dep_score.append(frame_average_score_with_dependency)\n                frame_average_score_without_dependency = evaluate_dict['average_score_without_dependency']\n                wo_dep_score.append(frame_average_score_without_dependency)\n            avg_dep_score, avg_wo_dep_score = sum(dep_score)/len(dep_score), sum(wo_dep_score)/len(dep_score)\n            average_dep_score_list.append(avg_dep_score)\n            average_wo_dep_score_list.append(avg_wo_dep_score)\n\n\n        result['average_dep_dgs_score'] = sum(average_dep_score_list)/len(average_dep_score_list)\n        result['average_wo_dep_dgs_score'] = sum(average_wo_dep_score_list)/len(average_wo_dep_score_list)\n\n        self.results.append(result)\n\n\n    def compute_metrics(self, results: list) -&gt; Dict[str, float]:\n        \"\"\"Compute the metrics from processed results.\n\n        Args:\n            results (list): The processed results of each batch.\n\n        Returns:\n            Dict[str, float]: The computed metrics. The keys are the names of\n            the metrics, and the values are corresponding results.\n        \"\"\"\n        logger: MMLogger = MMLogger.get_current_instance()\n\n        dep_dsg_score_np = np.zeros(len(results))\n        wo_dep_dsg_score_np = np.zeros(len(results))\n        for i, result in enumerate(results):\n            dep_dsg_score_np[i] = result['average_dep_dgs_score']\n            wo_dep_dsg_score_np[i] = result['average_wo_dep_dgs_score']\n\n        dep_dsg_score_np_mean = np.mean(dep_dsg_score_np) \n        wo_dep_dsg_score_np_mean = np.mean(wo_dep_dsg_score_np)\n\n        print(\"Test results: dsg score with dependency={:.4f}\"\n              .format(dep_dsg_score_np_mean))\n        print(\"Test results: dsg score without dependency={:.4f}\"\n              .format(wo_dep_dsg_score_np_mean))\n\n        return result\n</code></pre>"},{"location":"documentations/metrics/#aigve.metrics.DSGScore.compute_metrics","title":"<code>compute_metrics(results)</code>","text":"<p>Compute the metrics from processed results.</p> <p>Parameters:</p> Name Type Description Default <code>results</code> <code>list</code> <p>The processed results of each batch.</p> required <p>Returns:</p> Type Description <code>Dict[str, float]</code> <p>Dict[str, float]: The computed metrics. The keys are the names of</p> <code>Dict[str, float]</code> <p>the metrics, and the values are corresponding results.</p> Source code in <code>aigve/metrics/text_video_alignment/gpt_based/dsg/dsg_eval.py</code> <pre><code>def compute_metrics(self, results: list) -&gt; Dict[str, float]:\n    \"\"\"Compute the metrics from processed results.\n\n    Args:\n        results (list): The processed results of each batch.\n\n    Returns:\n        Dict[str, float]: The computed metrics. The keys are the names of\n        the metrics, and the values are corresponding results.\n    \"\"\"\n    logger: MMLogger = MMLogger.get_current_instance()\n\n    dep_dsg_score_np = np.zeros(len(results))\n    wo_dep_dsg_score_np = np.zeros(len(results))\n    for i, result in enumerate(results):\n        dep_dsg_score_np[i] = result['average_dep_dgs_score']\n        wo_dep_dsg_score_np[i] = result['average_wo_dep_dgs_score']\n\n    dep_dsg_score_np_mean = np.mean(dep_dsg_score_np) \n    wo_dep_dsg_score_np_mean = np.mean(wo_dep_dsg_score_np)\n\n    print(\"Test results: dsg score with dependency={:.4f}\"\n          .format(dep_dsg_score_np_mean))\n    print(\"Test results: dsg score without dependency={:.4f}\"\n          .format(wo_dep_dsg_score_np_mean))\n\n    return result\n</code></pre>"},{"location":"documentations/metrics/#aigve.metrics.DSGScore.evaluate_image_dsg","title":"<code>evaluate_image_dsg(qid_list, frame_index, frame)</code>","text":"<p>Evaluate a generated image with DSG evaluator; this is the intermediate process of the <code>process</code> function. </p> <p>Parameters:</p> Name Type Description Default <code>qid_list</code> <code>List[str]</code> <p>The list of DSG parse question generation results.</p> required <code>frame_index</code> <code>int</code> <p>The index number of the currently evaluated frame.</p> required <code>frame</code> <code>List[List[float]</code> <p>The current evaluated frame.</p> required <p>Returns:</p> Type Description <p>None</p> Source code in <code>aigve/metrics/text_video_alignment/gpt_based/dsg/dsg_eval.py</code> <pre><code>def evaluate_image_dsg(self, qid_list, frame_index, frame):\n    \"\"\" Evaluate a generated image with DSG evaluator; this is the intermediate process of the ``process`` function. \n\n    Args:\n            qid_list (List[str]): The list of DSG parse question generation results.\n            frame_index (int): The index number of the currently evaluated frame.\n            frame (List[List[float]): The current evaluated frame.\n\n    Returns:\n            None\n    \"\"\"\n    if self.verbose:\n        print(\"#\"*50)\n        print(\"2) Answer questions given the generated image, with VQA\")\n        print(\"#\"*50)\n\n    # 2) answer questions with the generated image\n    qid2answer = {}\n    qid2scores = {}\n\n    qid2tuple, qid2dependency, qid2question = qid_list\n    for id, question in qid2question.items():\n        answer = self.vqa_model.vqa(image=frame, question=question)\n        print(answer)\n        qid2answer[id] = answer\n        qid2scores[id] = float('yes' in answer)\n\n    average_score_without_dep = sum(qid2scores.values()) / len(qid2scores)\n    print(average_score_without_dep, qid2answer, qid2scores)\n\n    if self.verbose:\n        print(\"#\"*50)\n        print(\"3) Zero-out scores from invalid questions\")\n        print(\"#\"*50)\n\n    # 3) zero-out scores from invalid questions \n    qid2validity = {}\n    qid2scores_after_filtering = deepcopy(qid2scores)\n\n    # print('qid2scores', qid2scores)\n    # print('qid2dependency', qid2dependency)\n    for id, parent_ids in qid2dependency.items():\n        # zero-out scores if parent questions are answered 'no'\n        any_parent_answered_no = False\n        for parent_id in parent_ids:\n            parent_id = list(parent_id)[0]\n            if parent_id == 0:\n                continue\n            if qid2scores[parent_id] == 0:\n                any_parent_answered_no = True\n                break\n        if any_parent_answered_no:\n            qid2scores_after_filtering[id] = 0.0\n            qid2validity[id] = False\n        else:\n            qid2validity[id] = True\n\n    if self.verbose:\n        print(\"Per-quesiton eval results (after using dependency)\")\n        for id in qid2question:\n            print(\"ID\", id)\n            print(\"question\", qid2question[id])\n            print(\"answer\", qid2answer[id])\n            print(\"validity\", qid2validity[id])\n            print(\"score (before filtering)\", qid2scores[id])\n            print(\"score (after filtering)\", qid2scores_after_filtering[id])\n            print()\n\n    if self.verbose:\n        print(\"#\"*50)\n        print(\"4) Calculate the final score by averaging\")\n        print(\"#\"*50)\n\n    average_score_with_dep = sum(qid2scores_after_filtering.values()) / len(qid2scores)\n\n    return {\n        'frame_index': frame_index,\n        'qid2tuple': qid2tuple,\n        'qid2dependency': qid2dependency,\n        'qid2question': qid2question,\n        'qid2answer': qid2answer,\n        'qid2scores': qid2scores,\n        'qid2validity': qid2validity,\n        'average_score_with_dependency': average_score_with_dep,\n        'average_score_without_dependency': average_score_without_dep\n    }\n</code></pre>"},{"location":"documentations/metrics/#aigve.metrics.DSGScore.process","title":"<code>process(data_batch, data_samples)</code>","text":"<p>DSGScore process</p> <p>Process one batch of data samples and predictions. The processed results should be stored in <code>self.results</code>, which will be used to compute the metrics when all batches have been processed.</p> <p>Parameters:</p> Name Type Description Default <code>data_batch</code> <code>Sequence</code> <p>A batch of data from the dataloader.</p> required <code>data_samples</code> <code>Sequence</code> <p>A batch of data samples that contain annotations and predictions.</p> required Source code in <code>aigve/metrics/text_video_alignment/gpt_based/dsg/dsg_eval.py</code> <pre><code>def process(self, data_batch: Sequence, data_samples: Sequence) -&gt; None:\n    \"\"\"DSGScore process\n\n    Process one batch of data samples and predictions. The processed\n    results should be stored in ``self.results``, which will be used to\n    compute the metrics when all batches have been processed.\n\n    Args:\n        data_batch (Sequence): A batch of data from the dataloader.\n        data_samples (Sequence): A batch of data samples that\n            contain annotations and predictions.\n    \"\"\"\n\n    result = dict()\n\n    input_qid_lists, input_videos = data_samples\n    bsz = len(input_qid_lists)\n    # print('input_qid_lists: ', input_qid_lists)\n\n    # Ensure prompt_input is a tensor\n    if isinstance(input_qid_lists, tuple):\n        input_qid_lists = list(input_qid_lists)\n\n    if isinstance(input_videos, tuple):\n        input_videos = list(input_videos)\n\n    average_dep_score_list, average_wo_dep_score_list = [], []\n    for input_qid_list, input_video in zip([input_qid_lists], input_videos):\n        evaluate_dict_list = []\n        dep_score, wo_dep_score = [], []\n        for index, frame in enumerate(input_video):\n            # print('input_qid_list: ', input_qid_list)\n            evaluate_dict = self.evaluate_image_dsg(qid_list=input_qid_list, \n                                                    frame_index=index, \n                                                    frame=frame)\n            evaluate_dict_list.append(evaluate_dict)\n            frame_average_score_with_dependency = evaluate_dict['average_score_with_dependency']\n            dep_score.append(frame_average_score_with_dependency)\n            frame_average_score_without_dependency = evaluate_dict['average_score_without_dependency']\n            wo_dep_score.append(frame_average_score_without_dependency)\n        avg_dep_score, avg_wo_dep_score = sum(dep_score)/len(dep_score), sum(wo_dep_score)/len(dep_score)\n        average_dep_score_list.append(avg_dep_score)\n        average_wo_dep_score_list.append(avg_wo_dep_score)\n\n\n    result['average_dep_dgs_score'] = sum(average_dep_score_list)/len(average_dep_score_list)\n    result['average_wo_dep_dgs_score'] = sum(average_wo_dep_score_list)/len(average_wo_dep_score_list)\n\n    self.results.append(result)\n</code></pre>"},{"location":"documentations/metrics/#aigve.metrics.FIDScore","title":"<code>FIDScore</code>","text":"<p>               Bases: <code>BaseModel</code></p> Source code in <code>aigve/metrics/video_quality_assessment/distribution_based/fid.py</code> <pre><code>@MODELS.register_module()\n@METRICS.register_module()\nclass FIDScore(BaseModel):\n\n    def __init__(self, model_name='inception_v3', input_shape=(299, 299, 3), pooling='avg'):\n        super().__init__()\n        if model_name == 'inception_v3':\n            self.model = InceptionV3(include_top=False, pooling=pooling, input_shape=input_shape)\n        else:\n            raise ValueError(\"Only 'inception_v3' is currently supported.\")\n\n    def preprocess_images(self, images):\n        \"\"\"\n        Preprocess the images for the InceptionV3 model.\n\n        Parameters:\n        images (numpy array): Input images.\n\n        Returns:\n        numpy array: Preprocessed images.\n        \"\"\"\n        return preprocess_input(images)\n\n    def calculate_statistics(self, images):\n        \"\"\"\n        Calculate the feature statistics (mean and covariance) for a set of images.\n\n        Parameters:\n        images (numpy array): Preprocessed images.\n\n        Returns:\n        tuple: Mean and covariance of the features.\n        \"\"\"\n        features = self.model.predict(images)\n        mu = features.mean(axis=0)\n        sigma = np.cov(features, rowvar=False)\n        return mu, sigma\n\n    def process(self, data_batch: dict, data_samples: Sequence[dict]) -&gt; None:\n        \"\"\"\n        Process one batch of data samples and predictions.\n\n        Args:\n            data_batch (dict): A batch of data from the dataloader.\n            data_samples (Sequence[dict]): A batch of data samples that\n                contain annotations and predictions.\n        \"\"\"\n        result = dict()\n        images1, images2 = data_samples\n\n        # Calculate FID score\n        fid_score = self.calculate_fid(images1, images2)\n        result['fid_score'] = fid_score\n\n        self.results.append(result)\n\n    def compute_metrics(self, results: list) -&gt; Dict[str, float]:\n        \"\"\"\n        Compute the metrics from processed results.\n\n        Args:\n            results (list): The processed results of each batch.\n\n        Returns:\n            Dict[str, float]: The computed metrics.\n        \"\"\"\n        logger: MMLogger = MMLogger.get_current_instance()\n\n        fid_scores = np.zeros(len(results))\n        for i, result in enumerate(results):\n            fid_scores[i] = result['fid_score']\n\n        mean_fid = np.mean(fid_scores)\n\n        print(\"Test results: FID Score={:.4f}\".format(mean_fid))\n\n        return {'fid': mean_fid}\n\n    def calculate_fid(self, images1, images2):\n        \"\"\"\n        Calculate the FID score between two sets of images.\n\n        Parameters:\n        images1 (numpy array): First set of images.\n        images2 (numpy array): Second set of images.\n\n        Returns:\n        float: The FID score.\n        \"\"\"\n        # Preprocess images\n        images1 = self.preprocess_images(images1)\n        images2 = self.preprocess_images(images2)\n\n        # Calculate statistics\n        mu1, sigma1 = self.calculate_statistics(images1)\n        mu2, sigma2 = self.calculate_statistics(images2)\n\n        # Compute FID score\n        ssdiff = np.sum((mu1 - mu2) ** 2.0)\n        covmean = sqrtm(sigma1.dot(sigma2))\n\n        # Check and correct for imaginary numbers\n        if np.iscomplexobj(covmean):\n            covmean = covmean.real\n\n        fid = ssdiff + np.trace(sigma1 + sigma2 - 2.0 * covmean)\n        return fid\n</code></pre>"},{"location":"documentations/metrics/#aigve.metrics.FIDScore.calculate_fid","title":"<code>calculate_fid(images1, images2)</code>","text":"<p>Calculate the FID score between two sets of images.</p> <p>Parameters: images1 (numpy array): First set of images. images2 (numpy array): Second set of images.</p> <p>Returns: float: The FID score.</p> Source code in <code>aigve/metrics/video_quality_assessment/distribution_based/fid.py</code> <pre><code>def calculate_fid(self, images1, images2):\n    \"\"\"\n    Calculate the FID score between two sets of images.\n\n    Parameters:\n    images1 (numpy array): First set of images.\n    images2 (numpy array): Second set of images.\n\n    Returns:\n    float: The FID score.\n    \"\"\"\n    # Preprocess images\n    images1 = self.preprocess_images(images1)\n    images2 = self.preprocess_images(images2)\n\n    # Calculate statistics\n    mu1, sigma1 = self.calculate_statistics(images1)\n    mu2, sigma2 = self.calculate_statistics(images2)\n\n    # Compute FID score\n    ssdiff = np.sum((mu1 - mu2) ** 2.0)\n    covmean = sqrtm(sigma1.dot(sigma2))\n\n    # Check and correct for imaginary numbers\n    if np.iscomplexobj(covmean):\n        covmean = covmean.real\n\n    fid = ssdiff + np.trace(sigma1 + sigma2 - 2.0 * covmean)\n    return fid\n</code></pre>"},{"location":"documentations/metrics/#aigve.metrics.FIDScore.calculate_statistics","title":"<code>calculate_statistics(images)</code>","text":"<p>Calculate the feature statistics (mean and covariance) for a set of images.</p> <p>Parameters: images (numpy array): Preprocessed images.</p> <p>Returns: tuple: Mean and covariance of the features.</p> Source code in <code>aigve/metrics/video_quality_assessment/distribution_based/fid.py</code> <pre><code>def calculate_statistics(self, images):\n    \"\"\"\n    Calculate the feature statistics (mean and covariance) for a set of images.\n\n    Parameters:\n    images (numpy array): Preprocessed images.\n\n    Returns:\n    tuple: Mean and covariance of the features.\n    \"\"\"\n    features = self.model.predict(images)\n    mu = features.mean(axis=0)\n    sigma = np.cov(features, rowvar=False)\n    return mu, sigma\n</code></pre>"},{"location":"documentations/metrics/#aigve.metrics.FIDScore.compute_metrics","title":"<code>compute_metrics(results)</code>","text":"<p>Compute the metrics from processed results.</p> <p>Parameters:</p> Name Type Description Default <code>results</code> <code>list</code> <p>The processed results of each batch.</p> required <p>Returns:</p> Type Description <code>Dict[str, float]</code> <p>Dict[str, float]: The computed metrics.</p> Source code in <code>aigve/metrics/video_quality_assessment/distribution_based/fid.py</code> <pre><code>def compute_metrics(self, results: list) -&gt; Dict[str, float]:\n    \"\"\"\n    Compute the metrics from processed results.\n\n    Args:\n        results (list): The processed results of each batch.\n\n    Returns:\n        Dict[str, float]: The computed metrics.\n    \"\"\"\n    logger: MMLogger = MMLogger.get_current_instance()\n\n    fid_scores = np.zeros(len(results))\n    for i, result in enumerate(results):\n        fid_scores[i] = result['fid_score']\n\n    mean_fid = np.mean(fid_scores)\n\n    print(\"Test results: FID Score={:.4f}\".format(mean_fid))\n\n    return {'fid': mean_fid}\n</code></pre>"},{"location":"documentations/metrics/#aigve.metrics.FIDScore.preprocess_images","title":"<code>preprocess_images(images)</code>","text":"<p>Preprocess the images for the InceptionV3 model.</p> <p>Parameters: images (numpy array): Input images.</p> <p>Returns: numpy array: Preprocessed images.</p> Source code in <code>aigve/metrics/video_quality_assessment/distribution_based/fid.py</code> <pre><code>def preprocess_images(self, images):\n    \"\"\"\n    Preprocess the images for the InceptionV3 model.\n\n    Parameters:\n    images (numpy array): Input images.\n\n    Returns:\n    numpy array: Preprocessed images.\n    \"\"\"\n    return preprocess_input(images)\n</code></pre>"},{"location":"documentations/metrics/#aigve.metrics.FIDScore.process","title":"<code>process(data_batch, data_samples)</code>","text":"<p>Process one batch of data samples and predictions.</p> <p>Parameters:</p> Name Type Description Default <code>data_batch</code> <code>dict</code> <p>A batch of data from the dataloader.</p> required <code>data_samples</code> <code>Sequence[dict]</code> <p>A batch of data samples that contain annotations and predictions.</p> required Source code in <code>aigve/metrics/video_quality_assessment/distribution_based/fid.py</code> <pre><code>def process(self, data_batch: dict, data_samples: Sequence[dict]) -&gt; None:\n    \"\"\"\n    Process one batch of data samples and predictions.\n\n    Args:\n        data_batch (dict): A batch of data from the dataloader.\n        data_samples (Sequence[dict]): A batch of data samples that\n            contain annotations and predictions.\n    \"\"\"\n    result = dict()\n    images1, images2 = data_samples\n\n    # Calculate FID score\n    fid_score = self.calculate_fid(images1, images2)\n    result['fid_score'] = fid_score\n\n    self.results.append(result)\n</code></pre>"},{"location":"documentations/metrics/#aigve.metrics.GSTVQACrossData","title":"<code>GSTVQACrossData</code>","text":"<p>               Bases: <code>BaseMetric</code></p> <p>The GSTVQA evaluation metric. https://arxiv.org/pdf/2012.13936</p> <p>Parameters:</p> Name Type Description Default <code>collect_device</code> <code>str</code> <p>Device used for collecting results from workers. Options: 'cpu' and 'gpu'.</p> <code>'cpu'</code> <code>prefix</code> <code>str</code> <p>The prefix that will be added in the metric names to disambiguate homonymous metrics of different evaluators. Default: None.</p> <code>None</code> <code>metric_path</code> <code>str</code> <p>the file path of the metric </p> <code>''</code> <code>train_index</code> <code>(int, Optional)</code> <p>The specific model used. Details on: https://github.com/Baoliang93/GSTVQA/blob/main/TCSVT_Release/GVQA_Release/GVQA_Cross/cross_test.py#L162</p> required Source code in <code>aigve/metrics/video_quality_assessment/nn_based/gstvqa/gstvqa_crossdata_metric.py</code> <pre><code>@METRICS.register_module()\nclass GSTVQACrossData(BaseMetric):\n    \"\"\"The GSTVQA evaluation metric. https://arxiv.org/pdf/2012.13936\n\n    Args:\n        collect_device (str): Device used for collecting results from workers.\n            Options: 'cpu' and 'gpu'.\n        prefix (str, optional): The prefix that will be added in the metric\n            names to disambiguate homonymous metrics of different evaluators.\n            Default: None.\n        metric_path (str): the file path of the metric \n        train_index (int, Optional): The specific model used. Details on: https://github.com/Baoliang93/GSTVQA/blob/main/TCSVT_Release/GVQA_Release/GVQA_Cross/cross_test.py#L162\n    \"\"\"\n\n    default_prefix: Optional[str] = 'llm_score'\n\n    def __init__(self,\n                 collect_device: str = 'cpu',\n                 prefix: Optional[str] = None, \n                 metric_path: str = '',\n                 model_path: str = '',\n                 scale: int = 1,\n                 test_index: int = None,\n                #  train_index: int = 4\n                 ) -&gt; None:\n        super().__init__(collect_device=collect_device, prefix=prefix)\n        # self.train_index = train_index\n        self.metric_path = metric_path\n        self.model_path = model_path\n        self.scale = scale\n        self.test_index = test_index\n        if not submodule_exists(self.metric_path):\n            add_git_submodule(repo_url='https://github.com/Baoliang93/GSTVQA.git', submodule_path=self.metric_path)\n        from .GSTVQA.TCSVT_Release.GVQA_Release.GVQA_Cross.cross_test import GSTVQA as GSTVQA_model\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self.model = GSTVQA_model().to(self.device)\n        self.model.load_state_dict(torch.load(model_path))\n        self.model.eval()\n        self.criterion = nn.L1Loss().to(self.device)\n\n\n    # def process(self, data_batch: dict, data_samples: Sequence[dict]) -&gt; None:\n    def process(self, data_batch: Sequence, data_samples: Sequence) -&gt; None:\n        \"\"\"GSTVQA process\n        Process one batch of data samples and predictions. The processed\n        results should be stored in ``self.results``, which will be used to\n        compute the metrics when all batches have been processed.\n\n        Args:\n            data_batch (Sequence): A batch of data from the dataloader.\n            data_samples (Sequence): A batch of data samples that\n                contain annotations and predictions.\n        \"\"\"\n\n        result = dict()\n\n        features, length, label, mean_var,std_var,mean_mean,std_mean = data_samples\n        # # prompt_gt = data_sample['prompt_gt'] # str\n        # video_pd = data_sample['video_pd'] # torch.uint8(F, C, H, W)\n\n        result['y_test'] = self.scale * label.item()\n\n        features = features.to(self.device).float()\n        label = label.to(self.device).float()\n        mean_var = mean_var.to(self.device).float()\n        std_var = std_var.to(self.device).float()\n        mean_mean = mean_mean.to(self.device).float()\n        std_mean = std_mean.to(self.device).float()\n\n        outputs = self.model(features, length.float(),mean_var,std_var,mean_mean,std_mean)\n        result['y_pred'] = self.scale * outputs.item()\n        result['loss'] = self.criterion(outputs, label).item()\n\n        self.results.append(result)\n\n\n    def compute_metrics(self, results: list) -&gt; Dict[str, float]:\n        \"\"\"Compute the metrics from processed results.\n\n        Args:\n            results (list): The processed results of each batch.\n\n        Returns:\n            Dict[str, float]: The computed metrics. The keys are the names of\n            the metrics, and the values are corresponding results.\n        \"\"\"\n        logger: MMLogger = MMLogger.get_current_instance()\n\n        assert self.test_index == len(results)\n        test_loss = sum(result.get('loss', 0) for result in results) / len(results)\n        y_pred_np = np.zeros(len(self.test_index))\n        y_test_np = np.zeros(len(self.test_index))\n        for i, result in enumerate(results):\n            y_pred_np[i] = result['y_pred']\n            y_test_np[i] = result['y_test']\n\n        PLCC = stats.pearsonr(y_pred_np, y_test_np)[0]\n        SROCC = stats.spearmanr(y_pred_np, y_test_np)[0]\n        RMSE = np.sqrt(((y_pred_np-y_test_np) ** 2).mean())\n        KROCC = stats.stats.kendalltau(y_pred_np, y_test_np)[0]\n        print(\"Test results: test loss={:.4f}, SROCC={:.4f}, KROCC={:.4f}, PLCC={:.4f}, RMSE={:.4f}\"\n                .format(test_loss, SROCC, KROCC, PLCC, RMSE))\n</code></pre>"},{"location":"documentations/metrics/#aigve.metrics.GSTVQACrossData.compute_metrics","title":"<code>compute_metrics(results)</code>","text":"<p>Compute the metrics from processed results.</p> <p>Parameters:</p> Name Type Description Default <code>results</code> <code>list</code> <p>The processed results of each batch.</p> required <p>Returns:</p> Type Description <code>Dict[str, float]</code> <p>Dict[str, float]: The computed metrics. The keys are the names of</p> <code>Dict[str, float]</code> <p>the metrics, and the values are corresponding results.</p> Source code in <code>aigve/metrics/video_quality_assessment/nn_based/gstvqa/gstvqa_crossdata_metric.py</code> <pre><code>def compute_metrics(self, results: list) -&gt; Dict[str, float]:\n    \"\"\"Compute the metrics from processed results.\n\n    Args:\n        results (list): The processed results of each batch.\n\n    Returns:\n        Dict[str, float]: The computed metrics. The keys are the names of\n        the metrics, and the values are corresponding results.\n    \"\"\"\n    logger: MMLogger = MMLogger.get_current_instance()\n\n    assert self.test_index == len(results)\n    test_loss = sum(result.get('loss', 0) for result in results) / len(results)\n    y_pred_np = np.zeros(len(self.test_index))\n    y_test_np = np.zeros(len(self.test_index))\n    for i, result in enumerate(results):\n        y_pred_np[i] = result['y_pred']\n        y_test_np[i] = result['y_test']\n\n    PLCC = stats.pearsonr(y_pred_np, y_test_np)[0]\n    SROCC = stats.spearmanr(y_pred_np, y_test_np)[0]\n    RMSE = np.sqrt(((y_pred_np-y_test_np) ** 2).mean())\n    KROCC = stats.stats.kendalltau(y_pred_np, y_test_np)[0]\n    print(\"Test results: test loss={:.4f}, SROCC={:.4f}, KROCC={:.4f}, PLCC={:.4f}, RMSE={:.4f}\"\n            .format(test_loss, SROCC, KROCC, PLCC, RMSE))\n</code></pre>"},{"location":"documentations/metrics/#aigve.metrics.GSTVQACrossData.process","title":"<code>process(data_batch, data_samples)</code>","text":"<p>GSTVQA process Process one batch of data samples and predictions. The processed results should be stored in <code>self.results</code>, which will be used to compute the metrics when all batches have been processed.</p> <p>Parameters:</p> Name Type Description Default <code>data_batch</code> <code>Sequence</code> <p>A batch of data from the dataloader.</p> required <code>data_samples</code> <code>Sequence</code> <p>A batch of data samples that contain annotations and predictions.</p> required Source code in <code>aigve/metrics/video_quality_assessment/nn_based/gstvqa/gstvqa_crossdata_metric.py</code> <pre><code>def process(self, data_batch: Sequence, data_samples: Sequence) -&gt; None:\n    \"\"\"GSTVQA process\n    Process one batch of data samples and predictions. The processed\n    results should be stored in ``self.results``, which will be used to\n    compute the metrics when all batches have been processed.\n\n    Args:\n        data_batch (Sequence): A batch of data from the dataloader.\n        data_samples (Sequence): A batch of data samples that\n            contain annotations and predictions.\n    \"\"\"\n\n    result = dict()\n\n    features, length, label, mean_var,std_var,mean_mean,std_mean = data_samples\n    # # prompt_gt = data_sample['prompt_gt'] # str\n    # video_pd = data_sample['video_pd'] # torch.uint8(F, C, H, W)\n\n    result['y_test'] = self.scale * label.item()\n\n    features = features.to(self.device).float()\n    label = label.to(self.device).float()\n    mean_var = mean_var.to(self.device).float()\n    std_var = std_var.to(self.device).float()\n    mean_mean = mean_mean.to(self.device).float()\n    std_mean = std_mean.to(self.device).float()\n\n    outputs = self.model(features, length.float(),mean_var,std_var,mean_mean,std_mean)\n    result['y_pred'] = self.scale * outputs.item()\n    result['loss'] = self.criterion(outputs, label).item()\n\n    self.results.append(result)\n</code></pre>"},{"location":"documentations/metrics/#aigve.metrics.GstVqa","title":"<code>GstVqa</code>","text":"<p>               Bases: <code>BaseMetric</code></p> <p>GstVQA metric modified for the toy dataset. (Supporting 2944-dim features).</p> Source code in <code>aigve/metrics/video_quality_assessment/nn_based/gstvqa/gstvqa_metric.py</code> <pre><code>@METRICS.register_module()\nclass GstVqa(BaseMetric):\n    \"\"\"GstVQA metric modified for the toy dataset. (Supporting 2944-dim features).\"\"\"\n\n    def __init__(self, model_path: str):\n        super(GstVqa, self).__init__()\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self.submodel_path = os.path.join(os.getcwd(), 'metrics/video_quality_assessment/nn_based/gstvqa')\n        if not submodule_exists(self.submodel_path):\n            add_git_submodule(\n                repo_url='https://github.com/Baoliang93/GSTVQA.git', \n                submodule_path=self.submodel_path\n            )\n        from .GSTVQA.TCSVT_Release.GVQA_Release.GVQA_Cross.cross_test import GSTVQA as GSTVQA_model\n        self.model = GSTVQA_model().to(self.device)\n        self.model.load_state_dict(torch.load(model_path, map_location=self.device))\n        self.model.eval()\n        # self.criterion = nn.L1Loss().to(self.device)\n\n    def compute_stat_features(self, features: torch.Tensor, num_valid_frames: int) -&gt; Tuple[torch.Tensor]:\n        \"\"\"Compute statistical features mean_var, std_var, mean_mean, std_mean from extracted deep features.\n\n        Args:\n            features (torch.Tensor): Tensor of shape [T, 2944].\n            num_valid_frames (int): Number of valid frames before padding.\n\n        Returns:\n            Tuple[torch.Tensor]: (mean_var, std_var, mean_mean, std_mean), each of shape [1472].\n        \"\"\"\n        # Ignore padded frames\n        features = features[:num_valid_frames]  # Shape: [num_valid_frames, feature_dim]: [10, 1472]\n\n        if num_valid_frames == 0:  # Edge case: all frames were padded\n            return (\n                torch.zeros(1472, device=self.device),\n                torch.zeros(1472, device=self.device),\n                torch.zeros(1472, device=self.device),\n                torch.zeros(1472, device=self.device),\n            )\n\n        # Split into mean and std components\n        mean_features = features[:, :1472]  # First 1472 features are mean-based\n        std_features = features[:, 1472:]   # Last 1472 features are std-based\n\n        # Compute per-feature statistics over frames\n        mean_mean = mean_features.mean(dim=0)  # Shape: [1472]\n        std_mean = std_features.mean(dim=0)    # Shape: [1472]\n        mean_var = mean_features.var(dim=0, unbiased=False)  # Shape: [1472]\n        std_var = std_features.var(dim=0, unbiased=False)    # Shape: [1472]\n\n        return mean_var, std_var, mean_mean, std_mean\n\n    def process(self, data_batch: Sequence, data_samples: Sequence) -&gt; None:\n        \"\"\"\n        Process a batch of extracted deep features for GSTVQA evaluation and store results in a JSON file.\n\n        Args:\n            data_batch (SequencTuplee): A batch of data from the dataloader (not used here).\n            data_samples (List[ [torch.Tensor], Tuple[int], Tuple[str] ]): \n                A list containing three tuples:\n                - A tuple of `deep_features`: Each item is a Tensor of shape [T, 2944]. \n                - A tuple of `num_frames`: Each item is an integer representing the number of valid frames.\n                - A tuple of `video_name`: Each item is a string representing the file name for the video.\n                The len of these three tuples are the batch size.\n        \"\"\"\n        # data_samples an example: [\n        #     (tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n        #              [0., 0., 0.,  ..., 0., 0., 0.],\n        #              ...\n        #              [0., 0., 0.,  ..., 0., 0., 0.]]), \n        #      tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n        #              [0., 0., 0.,  ..., 0., 0., 0.],\n        #              ...\n        #              [0., 0., 0.,  ..., 0., 0., 0.]])), \n        #     (10, 10)\n        # ]\n        results = []\n        deep_features_tuple, num_frames_tuple, video_name_tuple = data_samples\n        with torch.no_grad():\n            for deep_features, num_valid_frames, video_name in zip(deep_features_tuple, num_frames_tuple, video_name_tuple):\n                if not isinstance(deep_features, torch.Tensor) or not isinstance(num_valid_frames, int):\n                    raise TypeError(\"Expected deep_features to be a torch.Tensor and num_valid_frames to be an int.\")\n\n                if num_valid_frames == 0:  # Edge case: No valid frames\n                    results.append({\"video_name\": 'N/A', \"GSTVQA_Score\": 0.0})\n                    continue\n\n                # Remove padded features\n                features = deep_features[:num_valid_frames].to(self.device)\n\n                # Compute statistical features only on valid frames\n                mean_var, std_var, mean_mean, std_mean = self.compute_stat_features(features, num_valid_frames)\n                mean_var, std_var, mean_mean, std_mean = (\n                    mean_var.to(self.device),\n                    std_var.to(self.device),\n                    mean_mean.to(self.device),\n                    std_mean.to(self.device),\n                )\n\n                # Length tensor indicating the number of valid frames\n                length = torch.tensor([num_valid_frames]).to(self.device)\n                # print('features(input) shape', features.unsqueeze(1).shape) # torch.Size([10, 1, 1472])\n                # print('input_length shape', length.shape) # torch.Size([1])\n                # print('input_length', length) # torch.Size([1])\n                # print('mean_mean shape', mean_mean.shape) # torch.Size([1472])\n                # print('std_mean shape', std_mean.shape) # torch.Size([1472])\n                # print('mean_var shape', mean_var.shape) # torch.Size([1472])\n                # print('std_var shape', std_var.shape) # torch.Size([1472])\n\n                # Run GSTVQA model\n                outputs = self.model(features.unsqueeze(1), length, mean_var, std_var, mean_mean, std_mean)\n                score = outputs.item()\n                results.append({\"video_name\": video_name, \"GSTVQA_Score\": score})\n                # print(f\"Processed score {score:.4f} for {video_name}\")\n\n        self.results.extend(results)\n\n\n    def compute_metrics(self, results: list) -&gt; Dict[str, float]:\n        \"\"\"Compute final GSTVQA-based metrics.\"\"\"\n        scores = np.array([res['GSTVQA_Score'] for res in self.results])\n        mean_score = np.mean(scores)\n        print(f\"GSTVQA mean score: {mean_score:.4f}\")\n\n        json_file_path = os.path.join(os.getcwd(), \"gstvqa_results.json\")\n        final_results = {\"video_results\": self.results, \"GSTVQA_Mean_Score\": mean_score}\n        with open(json_file_path, \"w\") as json_file:\n            json.dump(final_results, json_file, indent=4)\n        print(f\"GSTVQA mean score saved to {json_file_path}\")\n\n        return {'GSTVQA_Mean_Score': mean_score}\n</code></pre>"},{"location":"documentations/metrics/#aigve.metrics.GstVqa.compute_metrics","title":"<code>compute_metrics(results)</code>","text":"<p>Compute final GSTVQA-based metrics.</p> Source code in <code>aigve/metrics/video_quality_assessment/nn_based/gstvqa/gstvqa_metric.py</code> <pre><code>def compute_metrics(self, results: list) -&gt; Dict[str, float]:\n    \"\"\"Compute final GSTVQA-based metrics.\"\"\"\n    scores = np.array([res['GSTVQA_Score'] for res in self.results])\n    mean_score = np.mean(scores)\n    print(f\"GSTVQA mean score: {mean_score:.4f}\")\n\n    json_file_path = os.path.join(os.getcwd(), \"gstvqa_results.json\")\n    final_results = {\"video_results\": self.results, \"GSTVQA_Mean_Score\": mean_score}\n    with open(json_file_path, \"w\") as json_file:\n        json.dump(final_results, json_file, indent=4)\n    print(f\"GSTVQA mean score saved to {json_file_path}\")\n\n    return {'GSTVQA_Mean_Score': mean_score}\n</code></pre>"},{"location":"documentations/metrics/#aigve.metrics.GstVqa.compute_stat_features","title":"<code>compute_stat_features(features, num_valid_frames)</code>","text":"<p>Compute statistical features mean_var, std_var, mean_mean, std_mean from extracted deep features.</p> <p>Parameters:</p> Name Type Description Default <code>features</code> <code>Tensor</code> <p>Tensor of shape [T, 2944].</p> required <code>num_valid_frames</code> <code>int</code> <p>Number of valid frames before padding.</p> required <p>Returns:</p> Type Description <code>Tuple[Tensor]</code> <p>Tuple[torch.Tensor]: (mean_var, std_var, mean_mean, std_mean), each of shape [1472].</p> Source code in <code>aigve/metrics/video_quality_assessment/nn_based/gstvqa/gstvqa_metric.py</code> <pre><code>def compute_stat_features(self, features: torch.Tensor, num_valid_frames: int) -&gt; Tuple[torch.Tensor]:\n    \"\"\"Compute statistical features mean_var, std_var, mean_mean, std_mean from extracted deep features.\n\n    Args:\n        features (torch.Tensor): Tensor of shape [T, 2944].\n        num_valid_frames (int): Number of valid frames before padding.\n\n    Returns:\n        Tuple[torch.Tensor]: (mean_var, std_var, mean_mean, std_mean), each of shape [1472].\n    \"\"\"\n    # Ignore padded frames\n    features = features[:num_valid_frames]  # Shape: [num_valid_frames, feature_dim]: [10, 1472]\n\n    if num_valid_frames == 0:  # Edge case: all frames were padded\n        return (\n            torch.zeros(1472, device=self.device),\n            torch.zeros(1472, device=self.device),\n            torch.zeros(1472, device=self.device),\n            torch.zeros(1472, device=self.device),\n        )\n\n    # Split into mean and std components\n    mean_features = features[:, :1472]  # First 1472 features are mean-based\n    std_features = features[:, 1472:]   # Last 1472 features are std-based\n\n    # Compute per-feature statistics over frames\n    mean_mean = mean_features.mean(dim=0)  # Shape: [1472]\n    std_mean = std_features.mean(dim=0)    # Shape: [1472]\n    mean_var = mean_features.var(dim=0, unbiased=False)  # Shape: [1472]\n    std_var = std_features.var(dim=0, unbiased=False)    # Shape: [1472]\n\n    return mean_var, std_var, mean_mean, std_mean\n</code></pre>"},{"location":"documentations/metrics/#aigve.metrics.GstVqa.process","title":"<code>process(data_batch, data_samples)</code>","text":"<p>Process a batch of extracted deep features for GSTVQA evaluation and store results in a JSON file.</p> <p>Parameters:</p> Name Type Description Default <code>data_batch</code> <code>SequencTuplee</code> <p>A batch of data from the dataloader (not used here).</p> required <code>data_samples</code> <code>List[[Tensor], Tuple[int], Tuple[str]]</code> <p>A list containing three tuples: - A tuple of <code>deep_features</code>: Each item is a Tensor of shape [T, 2944].  - A tuple of <code>num_frames</code>: Each item is an integer representing the number of valid frames. - A tuple of <code>video_name</code>: Each item is a string representing the file name for the video. The len of these three tuples are the batch size.</p> required Source code in <code>aigve/metrics/video_quality_assessment/nn_based/gstvqa/gstvqa_metric.py</code> <pre><code>def process(self, data_batch: Sequence, data_samples: Sequence) -&gt; None:\n    \"\"\"\n    Process a batch of extracted deep features for GSTVQA evaluation and store results in a JSON file.\n\n    Args:\n        data_batch (SequencTuplee): A batch of data from the dataloader (not used here).\n        data_samples (List[ [torch.Tensor], Tuple[int], Tuple[str] ]): \n            A list containing three tuples:\n            - A tuple of `deep_features`: Each item is a Tensor of shape [T, 2944]. \n            - A tuple of `num_frames`: Each item is an integer representing the number of valid frames.\n            - A tuple of `video_name`: Each item is a string representing the file name for the video.\n            The len of these three tuples are the batch size.\n    \"\"\"\n    # data_samples an example: [\n    #     (tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n    #              [0., 0., 0.,  ..., 0., 0., 0.],\n    #              ...\n    #              [0., 0., 0.,  ..., 0., 0., 0.]]), \n    #      tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n    #              [0., 0., 0.,  ..., 0., 0., 0.],\n    #              ...\n    #              [0., 0., 0.,  ..., 0., 0., 0.]])), \n    #     (10, 10)\n    # ]\n    results = []\n    deep_features_tuple, num_frames_tuple, video_name_tuple = data_samples\n    with torch.no_grad():\n        for deep_features, num_valid_frames, video_name in zip(deep_features_tuple, num_frames_tuple, video_name_tuple):\n            if not isinstance(deep_features, torch.Tensor) or not isinstance(num_valid_frames, int):\n                raise TypeError(\"Expected deep_features to be a torch.Tensor and num_valid_frames to be an int.\")\n\n            if num_valid_frames == 0:  # Edge case: No valid frames\n                results.append({\"video_name\": 'N/A', \"GSTVQA_Score\": 0.0})\n                continue\n\n            # Remove padded features\n            features = deep_features[:num_valid_frames].to(self.device)\n\n            # Compute statistical features only on valid frames\n            mean_var, std_var, mean_mean, std_mean = self.compute_stat_features(features, num_valid_frames)\n            mean_var, std_var, mean_mean, std_mean = (\n                mean_var.to(self.device),\n                std_var.to(self.device),\n                mean_mean.to(self.device),\n                std_mean.to(self.device),\n            )\n\n            # Length tensor indicating the number of valid frames\n            length = torch.tensor([num_valid_frames]).to(self.device)\n            # print('features(input) shape', features.unsqueeze(1).shape) # torch.Size([10, 1, 1472])\n            # print('input_length shape', length.shape) # torch.Size([1])\n            # print('input_length', length) # torch.Size([1])\n            # print('mean_mean shape', mean_mean.shape) # torch.Size([1472])\n            # print('std_mean shape', std_mean.shape) # torch.Size([1472])\n            # print('mean_var shape', mean_var.shape) # torch.Size([1472])\n            # print('std_var shape', std_var.shape) # torch.Size([1472])\n\n            # Run GSTVQA model\n            outputs = self.model(features.unsqueeze(1), length, mean_var, std_var, mean_mean, std_mean)\n            score = outputs.item()\n            results.append({\"video_name\": video_name, \"GSTVQA_Score\": score})\n            # print(f\"Processed score {score:.4f} for {video_name}\")\n\n    self.results.extend(results)\n</code></pre>"},{"location":"documentations/metrics/#aigve.metrics.ISScore","title":"<code>ISScore</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Inception Score (IS) implementation.</p> <p>The Inception Score measures the quality and diversity of generated images by evaluating the KL divergence between the conditional class distribution and the marginal class distribution.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>Name of the model to use. Currently only 'inception_v3' is supported.</p> <code>'inception_v3'</code> <code>input_shape</code> <code>tuple</code> <p>Input shape for the model (height, width, channels).</p> <code>(299, 299, 3)</code> <code>splits</code> <code>int</code> <p>Number of splits to use when calculating the score.</p> <code>10</code> Source code in <code>aigve/metrics/video_quality_assessment/distribution_based/is_score.py</code> <pre><code>@MODELS.register_module()\n@METRICS.register_module()\nclass ISScore(BaseModel):\n    \"\"\"\n    Inception Score (IS) implementation.\n\n    The Inception Score measures the quality and diversity of generated images\n    by evaluating the KL divergence between the conditional class distribution\n    and the marginal class distribution.\n\n    Args:\n        model_name (str): Name of the model to use. Currently only 'inception_v3' is supported.\n        input_shape (tuple): Input shape for the model (height, width, channels).\n        splits (int): Number of splits to use when calculating the score.\n    \"\"\"\n\n    def __init__(self, model_name='inception_v3', input_shape=(299, 299, 3), splits=10):\n        super().__init__()\n        if model_name == 'inception_v3':\n            self.model = InceptionV3(include_top=True, input_shape=input_shape)\n        else:\n            raise ValueError(\"Only 'inception_v3' is currently supported.\")\n\n        self.splits = splits\n        self.results = []\n\n    def preprocess_images(self, images):\n        \"\"\"\n        Preprocess the images for the InceptionV3 model.\n\n        Parameters:\n        images (numpy array): Input images.\n\n        Returns:\n        numpy array: Preprocessed images.\n        \"\"\"\n        return preprocess_input(images)\n\n    def process(self, data_batch: dict, data_samples: Sequence[dict]) -&gt; None:\n        \"\"\"\n        Process one batch of data samples and predictions.\n\n        Args:\n            data_batch (dict): A batch of data from the dataloader.\n            data_samples (Sequence[dict]): A batch of data samples that\n                contain annotations and predictions.\n        \"\"\"\n        result = dict()\n        images = data_samples\n\n        # Calculate IS score\n        is_score, is_std = self.calculate_is(images)\n        result['is_score'] = is_score\n        result['is_std'] = is_std\n\n        self.results.append(result)\n\n    def compute_metrics(self, results: list) -&gt; Dict[str, float]:\n        \"\"\"\n        Compute the metrics from processed results.\n\n        Args:\n            results (list): The processed results of each batch.\n\n        Returns:\n            Dict[str, float]: The computed metrics.\n        \"\"\"\n        logger: MMLogger = MMLogger.get_current_instance()\n\n        is_scores = np.zeros(len(results))\n        is_stds = np.zeros(len(results))\n\n        for i, result in enumerate(results):\n            is_scores[i] = result['is_score']\n            is_stds[i] = result['is_std']\n\n        mean_is = np.mean(is_scores)\n        mean_std = np.mean(is_stds)\n\n        print(\"Test results: IS Score={:.4f} \u00b1 {:.4f}\".format(mean_is, mean_std))\n\n        return {'is': mean_is, 'is_std': mean_std}\n\n    def calculate_is(self, images):\n        \"\"\"\n        Calculate the Inception Score for a set of images.\n\n        Parameters:\n        images (numpy array): Input images.\n\n        Returns:\n        tuple: The mean and standard deviation of the inception score.\n        \"\"\"\n        # Preprocess images\n        images = self.preprocess_images(images)\n\n        # Get predictions\n        preds = self.model.predict(images)\n\n        # Calculate scores for each split\n        scores = []\n        n = preds.shape[0]\n        split_size = n // self.splits\n\n        for i in range(self.splits):\n            part = preds[i * split_size:(i + 1) * split_size]\n            kl = part * (np.log(part) - np.log(np.expand_dims(np.mean(part, axis=0), 0)))\n            kl = np.mean(np.sum(kl, axis=1))\n            scores.append(np.exp(kl))\n\n        return np.mean(scores), np.std(scores)\n</code></pre>"},{"location":"documentations/metrics/#aigve.metrics.ISScore.calculate_is","title":"<code>calculate_is(images)</code>","text":"<p>Calculate the Inception Score for a set of images.</p> <p>Parameters: images (numpy array): Input images.</p> <p>Returns: tuple: The mean and standard deviation of the inception score.</p> Source code in <code>aigve/metrics/video_quality_assessment/distribution_based/is_score.py</code> <pre><code>def calculate_is(self, images):\n    \"\"\"\n    Calculate the Inception Score for a set of images.\n\n    Parameters:\n    images (numpy array): Input images.\n\n    Returns:\n    tuple: The mean and standard deviation of the inception score.\n    \"\"\"\n    # Preprocess images\n    images = self.preprocess_images(images)\n\n    # Get predictions\n    preds = self.model.predict(images)\n\n    # Calculate scores for each split\n    scores = []\n    n = preds.shape[0]\n    split_size = n // self.splits\n\n    for i in range(self.splits):\n        part = preds[i * split_size:(i + 1) * split_size]\n        kl = part * (np.log(part) - np.log(np.expand_dims(np.mean(part, axis=0), 0)))\n        kl = np.mean(np.sum(kl, axis=1))\n        scores.append(np.exp(kl))\n\n    return np.mean(scores), np.std(scores)\n</code></pre>"},{"location":"documentations/metrics/#aigve.metrics.ISScore.compute_metrics","title":"<code>compute_metrics(results)</code>","text":"<p>Compute the metrics from processed results.</p> <p>Parameters:</p> Name Type Description Default <code>results</code> <code>list</code> <p>The processed results of each batch.</p> required <p>Returns:</p> Type Description <code>Dict[str, float]</code> <p>Dict[str, float]: The computed metrics.</p> Source code in <code>aigve/metrics/video_quality_assessment/distribution_based/is_score.py</code> <pre><code>def compute_metrics(self, results: list) -&gt; Dict[str, float]:\n    \"\"\"\n    Compute the metrics from processed results.\n\n    Args:\n        results (list): The processed results of each batch.\n\n    Returns:\n        Dict[str, float]: The computed metrics.\n    \"\"\"\n    logger: MMLogger = MMLogger.get_current_instance()\n\n    is_scores = np.zeros(len(results))\n    is_stds = np.zeros(len(results))\n\n    for i, result in enumerate(results):\n        is_scores[i] = result['is_score']\n        is_stds[i] = result['is_std']\n\n    mean_is = np.mean(is_scores)\n    mean_std = np.mean(is_stds)\n\n    print(\"Test results: IS Score={:.4f} \u00b1 {:.4f}\".format(mean_is, mean_std))\n\n    return {'is': mean_is, 'is_std': mean_std}\n</code></pre>"},{"location":"documentations/metrics/#aigve.metrics.ISScore.preprocess_images","title":"<code>preprocess_images(images)</code>","text":"<p>Preprocess the images for the InceptionV3 model.</p> <p>Parameters: images (numpy array): Input images.</p> <p>Returns: numpy array: Preprocessed images.</p> Source code in <code>aigve/metrics/video_quality_assessment/distribution_based/is_score.py</code> <pre><code>def preprocess_images(self, images):\n    \"\"\"\n    Preprocess the images for the InceptionV3 model.\n\n    Parameters:\n    images (numpy array): Input images.\n\n    Returns:\n    numpy array: Preprocessed images.\n    \"\"\"\n    return preprocess_input(images)\n</code></pre>"},{"location":"documentations/metrics/#aigve.metrics.ISScore.process","title":"<code>process(data_batch, data_samples)</code>","text":"<p>Process one batch of data samples and predictions.</p> <p>Parameters:</p> Name Type Description Default <code>data_batch</code> <code>dict</code> <p>A batch of data from the dataloader.</p> required <code>data_samples</code> <code>Sequence[dict]</code> <p>A batch of data samples that contain annotations and predictions.</p> required Source code in <code>aigve/metrics/video_quality_assessment/distribution_based/is_score.py</code> <pre><code>def process(self, data_batch: dict, data_samples: Sequence[dict]) -&gt; None:\n    \"\"\"\n    Process one batch of data samples and predictions.\n\n    Args:\n        data_batch (dict): A batch of data from the dataloader.\n        data_samples (Sequence[dict]): A batch of data samples that\n            contain annotations and predictions.\n    \"\"\"\n    result = dict()\n    images = data_samples\n\n    # Calculate IS score\n    is_score, is_std = self.calculate_is(images)\n    result['is_score'] = is_score\n    result['is_std'] = is_std\n\n    self.results.append(result)\n</code></pre>"},{"location":"documentations/metrics/#aigve.metrics.LightVQAPlus","title":"<code>LightVQAPlus</code>","text":"<p>               Bases: <code>BaseMetric</code></p> <p>LightVQA+ metric for evaluating video quality.</p> Source code in <code>aigve/metrics/video_quality_assessment/nn_based/lightvqa_plus/lightvqa_plus_metric.py</code> <pre><code>@METRICS.register_module()\nclass LightVQAPlus(BaseMetric):\n    \"\"\"LightVQA+ metric for evaluating video quality.\"\"\"\n\n    def __init__(self, model_path: str, swin_weights: str, is_gpu: bool = True):\n        super(LightVQAPlus, self).__init__()\n        self.model_path = model_path\n        self.swin_weights = swin_weights\n        self.device = torch.device(\"cuda\" if is_gpu else \"cpu\")\n\n        self.submodel_path = os.path.join(os.getcwd(), 'metrics/video_quality_assessment/nn_based/lightvqa_plus')\n        if not submodule_exists(self.submodel_path):\n            add_git_submodule(\n                repo_url='https://github.com/SaMMyCHoo/Light-VQA-plus.git', \n                submodule_path=self.submodel_path\n            )\n        lightvqa_path = os.path.join(self.submodel_path, \"Light_VQA_plus\")\n        if lightvqa_path not in sys.path:\n            sys.path.insert(0, lightvqa_path)\n\n        from .Light_VQA_plus.final_fusion_model import swin_small_patch4_window7_224 as create_model\n        self.model = create_model().to(self.device)\n\n        weights_dict = torch.load(os.path.join(os.getcwd(), self.model_path), map_location=self.device)\n        print(self.model.load_state_dict(weights_dict))\n\n        self.model.eval()\n\n    def process(self, data_batch: list, data_samples: list) -&gt; None:\n        \"\"\"\n        Process a batch of extracted deep features for LightVQA+ evaluation.\n        Args:\n            data_batch (Sequence): A batch of data from the dataloader (not used here).\n            data_samples (List[Tuple[torch.Tensor], Tuple[torch.Tensor], Tuple[torch.Tensor], Tuple[str]]):\n                - spatial_features (torch.Tensor): Extracts 8 evenly spaced key frames. Shape: [8, 3, 672, 1120].\n                - temporal_features (torch.Tensor): Motion features from SlowFast. Shape: [1, feature_dim(2304)].\n                - bns_features (torch.Tensor): Brightness &amp; Noise features. Shape: [8, 300].\n                - bc_features (torch.Tensor): Temporal brightness contrast features. Shape: [8, final_dim(20)].\n                - video_name (str): Video filename.\n        \"\"\"\n        results = []\n        spatial_features_tuple, temporal_features_tuple, bns_features_tuple, bc_features_tuple, video_name_tuple = data_samples\n        # print('spatial_features_tuple len: ', len(spatial_features_tuple)) # B\n        # print('spatial_features_tuple[0]: ', spatial_features_tuple[0].shape) # torch.Size([8, 3, 672, 1120])\n        # print('temporal_features_tuple[0]: ', temporal_features_tuple[0].shape) # torch.Size([1, 2304])\n        # print('bns_features_tuple[0]: ', bns_features_tuple[0].shape) # torch.Size([8, 300])\n        # print('bc_features_tuple[0]: ', bc_features_tuple[0].shape) # torch.Size([8, 20])\n\n        batch_size = len(spatial_features_tuple)\n        with torch.no_grad():\n            for i in range(batch_size):\n                video_name = video_name_tuple[i]\n                spatial_features = spatial_features_tuple[i].to(self.device) # torch.Size([8, 3, 672, 1120])\n                temporal_features = temporal_features_tuple[i].to(self.device) # torch.Size([1, 2304])\n                bns_features = bns_features_tuple[i].to(self.device) # torch.Size([8, 300])\n                bc_features = bc_features_tuple[i].to(self.device)  # Shape: [8, final_dim(20)]\n\n                concat_features = torch.cat([temporal_features, bc_features.view(1, -1)], dim=1) # torch.Size([1, 2304+8*20])\n                # print('concat_features: ', concat_features.shape) # torch.Size([1, 2464])\n                final_temporal_features = F.pad(concat_features, (0, 2604 - concat_features.shape[1]), mode=\"constant\", value=0) # torch.Size([1, 2604])\n                # print('final_temporal_features: ', final_temporal_features.shape) # torch.Size([1, 2604])\n\n                outputs = self.model(spatial_features, final_temporal_features, bns_features)\n                # print('outputs: ', outputs)\n                score = outputs.mean().item()\n\n                results.append({\"video_name\": video_name, \"LightVQAPlus_Score\": score})\n                print(f\"Processed score {score:.4f} for {video_name}\")\n\n        self.results.extend(results)\n\n    def compute_metrics(self, results: list) -&gt; Dict[str, float]:\n        \"\"\"Compute final LightVQA+ metrics.\"\"\"\n        scores = np.array([res[\"LightVQAPlus_Score\"] for res in self.results])\n        mean_score = np.mean(scores) if scores.size &gt; 0 else 0.0\n        print(f\"LightVQA+ mean score: {mean_score:.4f}\")\n\n        json_file_path = os.path.join(os.getcwd(), \"lightvqaplus_results.json\")\n        final_results = {\"video_results\": self.results, \"LightVQAPlus_Mean_Score\": mean_score}\n        with open(json_file_path, \"w\") as json_file:\n            json.dump(final_results, json_file, indent=4)\n        print(f\"LightVQA+ mean score saved to {json_file_path}\")\n\n        return {\"LightVQAPlus_Mean_Score\": mean_score}\n</code></pre>"},{"location":"documentations/metrics/#aigve.metrics.LightVQAPlus.compute_metrics","title":"<code>compute_metrics(results)</code>","text":"<p>Compute final LightVQA+ metrics.</p> Source code in <code>aigve/metrics/video_quality_assessment/nn_based/lightvqa_plus/lightvqa_plus_metric.py</code> <pre><code>def compute_metrics(self, results: list) -&gt; Dict[str, float]:\n    \"\"\"Compute final LightVQA+ metrics.\"\"\"\n    scores = np.array([res[\"LightVQAPlus_Score\"] for res in self.results])\n    mean_score = np.mean(scores) if scores.size &gt; 0 else 0.0\n    print(f\"LightVQA+ mean score: {mean_score:.4f}\")\n\n    json_file_path = os.path.join(os.getcwd(), \"lightvqaplus_results.json\")\n    final_results = {\"video_results\": self.results, \"LightVQAPlus_Mean_Score\": mean_score}\n    with open(json_file_path, \"w\") as json_file:\n        json.dump(final_results, json_file, indent=4)\n    print(f\"LightVQA+ mean score saved to {json_file_path}\")\n\n    return {\"LightVQAPlus_Mean_Score\": mean_score}\n</code></pre>"},{"location":"documentations/metrics/#aigve.metrics.LightVQAPlus.process","title":"<code>process(data_batch, data_samples)</code>","text":"<p>Process a batch of extracted deep features for LightVQA+ evaluation. Args:     data_batch (Sequence): A batch of data from the dataloader (not used here).     data_samples (List[Tuple[torch.Tensor], Tuple[torch.Tensor], Tuple[torch.Tensor], Tuple[str]]):         - spatial_features (torch.Tensor): Extracts 8 evenly spaced key frames. Shape: [8, 3, 672, 1120].         - temporal_features (torch.Tensor): Motion features from SlowFast. Shape: [1, feature_dim(2304)].         - bns_features (torch.Tensor): Brightness &amp; Noise features. Shape: [8, 300].         - bc_features (torch.Tensor): Temporal brightness contrast features. Shape: [8, final_dim(20)].         - video_name (str): Video filename.</p> Source code in <code>aigve/metrics/video_quality_assessment/nn_based/lightvqa_plus/lightvqa_plus_metric.py</code> <pre><code>def process(self, data_batch: list, data_samples: list) -&gt; None:\n    \"\"\"\n    Process a batch of extracted deep features for LightVQA+ evaluation.\n    Args:\n        data_batch (Sequence): A batch of data from the dataloader (not used here).\n        data_samples (List[Tuple[torch.Tensor], Tuple[torch.Tensor], Tuple[torch.Tensor], Tuple[str]]):\n            - spatial_features (torch.Tensor): Extracts 8 evenly spaced key frames. Shape: [8, 3, 672, 1120].\n            - temporal_features (torch.Tensor): Motion features from SlowFast. Shape: [1, feature_dim(2304)].\n            - bns_features (torch.Tensor): Brightness &amp; Noise features. Shape: [8, 300].\n            - bc_features (torch.Tensor): Temporal brightness contrast features. Shape: [8, final_dim(20)].\n            - video_name (str): Video filename.\n    \"\"\"\n    results = []\n    spatial_features_tuple, temporal_features_tuple, bns_features_tuple, bc_features_tuple, video_name_tuple = data_samples\n    # print('spatial_features_tuple len: ', len(spatial_features_tuple)) # B\n    # print('spatial_features_tuple[0]: ', spatial_features_tuple[0].shape) # torch.Size([8, 3, 672, 1120])\n    # print('temporal_features_tuple[0]: ', temporal_features_tuple[0].shape) # torch.Size([1, 2304])\n    # print('bns_features_tuple[0]: ', bns_features_tuple[0].shape) # torch.Size([8, 300])\n    # print('bc_features_tuple[0]: ', bc_features_tuple[0].shape) # torch.Size([8, 20])\n\n    batch_size = len(spatial_features_tuple)\n    with torch.no_grad():\n        for i in range(batch_size):\n            video_name = video_name_tuple[i]\n            spatial_features = spatial_features_tuple[i].to(self.device) # torch.Size([8, 3, 672, 1120])\n            temporal_features = temporal_features_tuple[i].to(self.device) # torch.Size([1, 2304])\n            bns_features = bns_features_tuple[i].to(self.device) # torch.Size([8, 300])\n            bc_features = bc_features_tuple[i].to(self.device)  # Shape: [8, final_dim(20)]\n\n            concat_features = torch.cat([temporal_features, bc_features.view(1, -1)], dim=1) # torch.Size([1, 2304+8*20])\n            # print('concat_features: ', concat_features.shape) # torch.Size([1, 2464])\n            final_temporal_features = F.pad(concat_features, (0, 2604 - concat_features.shape[1]), mode=\"constant\", value=0) # torch.Size([1, 2604])\n            # print('final_temporal_features: ', final_temporal_features.shape) # torch.Size([1, 2604])\n\n            outputs = self.model(spatial_features, final_temporal_features, bns_features)\n            # print('outputs: ', outputs)\n            score = outputs.mean().item()\n\n            results.append({\"video_name\": video_name, \"LightVQAPlus_Score\": score})\n            print(f\"Processed score {score:.4f} for {video_name}\")\n\n    self.results.extend(results)\n</code></pre>"},{"location":"documentations/metrics/#aigve.metrics.PickScore","title":"<code>PickScore</code>","text":"<p>               Bases: <code>BaseMetric</code></p> <p>Initialize the <code>PickScore</code> evaluator.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>The name of the PickScore model. Defaults to <code>yuvalkirstain/PickScore_v1</code>.</p> <code>'yuvalkirstain/PickScore_v1'</code> <code>logit_scale</code> <code>bool</code> <p>Whether to calcualte the cosine similarity as logits. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <p>None</p> Source code in <code>aigve/metrics/text_video_alignment/similarity_based/pickscore/pick_infer.py</code> <pre><code>@METRICS.register_module()\nclass PickScore(BaseMetric):\n    \"\"\" Initialize the ``PickScore`` evaluator.\n\n    Args:\n            model_name (str): The name of the PickScore model. Defaults to ``yuvalkirstain/PickScore_v1``.\n            logit_scale (bool): Whether to calcualte the cosine similarity as logits. Defaults to False.\n\n    Returns:\n            None\n    \"\"\"\n    def __init__(self, \n                 model_name: str = \"yuvalkirstain/PickScore_v1\", \n                 logit_scale: bool = False) -&gt; None:\n        super().__init__()\n        self.model_name = model_name\n        self.logit_scale = logit_scale\n\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self.model =AutoModel.from_pretrained(self.model_name).eval().to(self.device)\n        self.model.eval()\n\n\n    # def process(self, data_batch: dict, data_samples: Sequence[dict]) -&gt; None:\n    def process(self, data_batch: Sequence, data_samples: Sequence) -&gt; None:\n        \"\"\"PickScore process\n        Process one batch of data samples and predictions. The processed\n        results should be stored in ``self.results``, which will be used to\n        compute the metrics when all batches have been processed.\n\n        Args:\n            data_batch (Sequence): A batch of data from the dataloader.\n            data_samples (Sequence): A batch of data samples that\n                contain annotations and predictions.\n        \"\"\"\n\n        result = dict()\n\n        input_prompts, input_videos = data_samples\n        bsz = len(input_prompts)\n\n        # Ensure prompt_input is a tensor\n        if isinstance(input_prompts, tuple):\n            input_prompts = list(input_prompts)\n\n        if isinstance(input_videos, tuple):\n            input_videos = list(input_videos)\n\n        pickscore_sum, pickscore_cnt = 0, 0\n        logit_scale = self.model.logit_scale.exp() if self.logit_scale else 1\n        with torch.no_grad():\n            for input_prompt, input_frames in zip(input_prompts, input_videos):\n\n                input_prompt = input_prompt.to(self.device)\n                text_feature = self.model.get_text_features(input_prompt)\n                text_feature = text_feature / torch.norm(text_feature, dim=-1, keepdim=True)\n\n                input_frames = input_frames.to(self.device)  # Add batch dimension and move the frame to the device\n                frame_features = self.model.get_image_features(input_frames)\n                frame_features = frame_features / torch.norm(frame_features, dim=-1, keepdim=True)\n\n                pick_score = logit_scale *  (frame_features @ text_feature.T).mean().item()\n                print('current pickscore', pick_score)\n                pickscore_sum += pick_score\n                pickscore_cnt += 1\n\n        # get probabilities if you have multiple images to choose from\n        # probs = torch.softmax(scores, dim=-1)\n        pickscore_total_avg = pickscore_sum/pickscore_cnt\n        result['pick_score'] = pickscore_total_avg\n\n        self.results.append(result)\n\n\n    def compute_metrics(self, results: list) -&gt; Dict[str, float]:\n        \"\"\"Compute the metrics from processed results.\n\n        Args:\n            results (list): The processed results of each batch.\n\n        Returns:\n            Dict[str, float]: The computed metrics. The keys are the names of\n            the metrics, and the values are corresponding results.\n        \"\"\"\n        logger: MMLogger = MMLogger.get_current_instance()\n\n        pickscore_np = np.zeros(len(results))\n        for i, result in enumerate(results):\n            pickscore_np[i] = result['pick_score']\n\n        pickscore_sim_mean = np.mean(pickscore_np) \n\n        print(\"Test results: PickScore={:.4f}\"\n              .format(pickscore_sim_mean))\n\n        return result\n</code></pre>"},{"location":"documentations/metrics/#aigve.metrics.PickScore.compute_metrics","title":"<code>compute_metrics(results)</code>","text":"<p>Compute the metrics from processed results.</p> <p>Parameters:</p> Name Type Description Default <code>results</code> <code>list</code> <p>The processed results of each batch.</p> required <p>Returns:</p> Type Description <code>Dict[str, float]</code> <p>Dict[str, float]: The computed metrics. The keys are the names of</p> <code>Dict[str, float]</code> <p>the metrics, and the values are corresponding results.</p> Source code in <code>aigve/metrics/text_video_alignment/similarity_based/pickscore/pick_infer.py</code> <pre><code>def compute_metrics(self, results: list) -&gt; Dict[str, float]:\n    \"\"\"Compute the metrics from processed results.\n\n    Args:\n        results (list): The processed results of each batch.\n\n    Returns:\n        Dict[str, float]: The computed metrics. The keys are the names of\n        the metrics, and the values are corresponding results.\n    \"\"\"\n    logger: MMLogger = MMLogger.get_current_instance()\n\n    pickscore_np = np.zeros(len(results))\n    for i, result in enumerate(results):\n        pickscore_np[i] = result['pick_score']\n\n    pickscore_sim_mean = np.mean(pickscore_np) \n\n    print(\"Test results: PickScore={:.4f}\"\n          .format(pickscore_sim_mean))\n\n    return result\n</code></pre>"},{"location":"documentations/metrics/#aigve.metrics.PickScore.process","title":"<code>process(data_batch, data_samples)</code>","text":"<p>PickScore process Process one batch of data samples and predictions. The processed results should be stored in <code>self.results</code>, which will be used to compute the metrics when all batches have been processed.</p> <p>Parameters:</p> Name Type Description Default <code>data_batch</code> <code>Sequence</code> <p>A batch of data from the dataloader.</p> required <code>data_samples</code> <code>Sequence</code> <p>A batch of data samples that contain annotations and predictions.</p> required Source code in <code>aigve/metrics/text_video_alignment/similarity_based/pickscore/pick_infer.py</code> <pre><code>def process(self, data_batch: Sequence, data_samples: Sequence) -&gt; None:\n    \"\"\"PickScore process\n    Process one batch of data samples and predictions. The processed\n    results should be stored in ``self.results``, which will be used to\n    compute the metrics when all batches have been processed.\n\n    Args:\n        data_batch (Sequence): A batch of data from the dataloader.\n        data_samples (Sequence): A batch of data samples that\n            contain annotations and predictions.\n    \"\"\"\n\n    result = dict()\n\n    input_prompts, input_videos = data_samples\n    bsz = len(input_prompts)\n\n    # Ensure prompt_input is a tensor\n    if isinstance(input_prompts, tuple):\n        input_prompts = list(input_prompts)\n\n    if isinstance(input_videos, tuple):\n        input_videos = list(input_videos)\n\n    pickscore_sum, pickscore_cnt = 0, 0\n    logit_scale = self.model.logit_scale.exp() if self.logit_scale else 1\n    with torch.no_grad():\n        for input_prompt, input_frames in zip(input_prompts, input_videos):\n\n            input_prompt = input_prompt.to(self.device)\n            text_feature = self.model.get_text_features(input_prompt)\n            text_feature = text_feature / torch.norm(text_feature, dim=-1, keepdim=True)\n\n            input_frames = input_frames.to(self.device)  # Add batch dimension and move the frame to the device\n            frame_features = self.model.get_image_features(input_frames)\n            frame_features = frame_features / torch.norm(frame_features, dim=-1, keepdim=True)\n\n            pick_score = logit_scale *  (frame_features @ text_feature.T).mean().item()\n            print('current pickscore', pick_score)\n            pickscore_sum += pick_score\n            pickscore_cnt += 1\n\n    # get probabilities if you have multiple images to choose from\n    # probs = torch.softmax(scores, dim=-1)\n    pickscore_total_avg = pickscore_sum/pickscore_cnt\n    result['pick_score'] = pickscore_total_avg\n\n    self.results.append(result)\n</code></pre>"},{"location":"documentations/metrics/#aigve.metrics.SimpleVqa","title":"<code>SimpleVqa</code>","text":"<p>               Bases: <code>BaseMetric</code></p> <p>SimpleVQA metric for evaluating video quality.</p> Source code in <code>aigve/metrics/video_quality_assessment/nn_based/simplevqa/simplevqa_metric.py</code> <pre><code>@METRICS.register_module()\nclass SimpleVqa(BaseMetric):\n    \"\"\"SimpleVQA metric for evaluating video quality.\"\"\"\n    def __init__(self, model_path: str, is_gpu: bool = True):\n        super(SimpleVqa, self).__init__()\n        self.model_path = model_path\n        self.device = torch.device(\"cuda\" if is_gpu else \"cpu\")\n        self.submodel_path = os.path.join(os.getcwd(), 'metrics/video_quality_assessment/nn_based/simplevqa')\n        if not submodule_exists(self.submodel_path):\n            add_git_submodule(\n                repo_url='https://github.com/sunwei925/SimpleVQA.git', \n                submodule_path=self.submodel_path\n            )\n        simplevqa_path = os.path.join(self.submodel_path, \"SimpleVQA\")\n        if simplevqa_path not in sys.path:\n            sys.path.insert(0, simplevqa_path)\n        from .SimpleVQA.model import UGC_BVQA_model\n        from .SimpleVQA.test_demo import slowfast\n        self.model_motion = slowfast().to(self.device)\n        self.model = UGC_BVQA_model.resnet50(pretrained=False)\n        self.model = torch.nn.DataParallel(self.model).to(self.device)\n        self.model.load_state_dict(torch.load(os.path.join(os.getcwd(), self.model_path), map_location=self.device))\n        self.model.eval()\n\n    def process(self, data_batch: list, data_samples: list) -&gt; None:\n        \"\"\"\n        Process a batch of extracted deep features for SimpleVQA evaluation.\n        Args:\n            data_batch (Sequence): A batch of data from the dataloader (not used here).\n            data_samples (List[ Tuple[torch.Tensor], List[Tuple[torch.Tensor]], Tuple[str] ]):\n                A list containing three tuples:\n                - A tuple of `spatial_features` (torch.Tensor): Shape [v_len_second, 3, 448, 448]. \n                    `v_len_second` is total seconds of the video (though 2 for toy dataset) with minium 8 (i.e. min_video_seconds). \n                    The len of the tuple is the batch size. \n                - A list of `motion_features` (Tuple[torch.Tensor]): \n                    len(List) is total seconds of the video, with minium 8 (i.e. min_video_seconds).\n                    Each item of the list is a Tuple of motion feature tensors. Each has shape [32, 3, 224, 224].\n                    The len of the tuple is the batch size.\n                - A tuple of `video_name` (str): Video filename. The len of the tuple is the batch size.\n        \"\"\"\n        from .SimpleVQA.test_demo import pack_pathway_output\n\n        results = []\n        # print(type(data_samples)) # list\n        spatial_features_tuple, motion_features_list, video_name_tuple = data_samples\n        # print(len(spatial_features_tuple)) # 1\n        # print(spatial_features_tuple[0].shape) # torch.Size([8, 3, 448, 448])\n\n        # print(type(motion_features_list)) # List\n        # print(len(motion_features_list)) # 8\n        # print(type(motion_features_list[0])) # tuple\n        # print(len(motion_features_list[0])) # 1\n        # print(type(motion_features_list[0][0])) # Tensor\n        # print(motion_features_list[0][0].shape) # torch.Size([32, 3, 224, 224])\n\n        batch_size = len(spatial_features_tuple)\n        with torch.no_grad():\n            for i in range(batch_size):\n                video_name = video_name_tuple[i]\n                spatial_features = spatial_features_tuple[i].to(self.device).unsqueeze(0)  # Add batch dim. Shape: tensor with Size([1, v_len_second, 3, 448, 448])\n\n                # Take the i-th element from each tuple in motion_features_list\n                motion_features = [motion_features_list[j][i] for j in range(len(motion_features_list))] # Shape: List[tensor with Size([32, 3, 224, 224])], len of it is total seconds of the video, with minium 8.\n\n                if not all(isinstance(mf, torch.Tensor) for mf in motion_features):\n                    raise TypeError(\"Expected motion_features to be a list of tensors.\")\n\n                if len(motion_features) == 0:  # Edge case: No valid motion features\n                    results.append({\"video_name\": video_name, \"SimpleVQA_Score\": 0.0})\n                    continue\n\n                n_clip = len(motion_features)  # 8\n                feature_motion = torch.zeros([n_clip, 2048 + 256], device=self.device) \n                # Process each motion clip\n                for idx, clip in enumerate(motion_features):\n                    clip = clip.unsqueeze(dim=0).permute(0, 2, 1, 3, 4)  # Reshape to [1, C(3), T(32), H(224), W(224)]\n                    clip = pack_pathway_output(clip, self.device)  # Convert to SlowFast format\n                    slow_feature, fast_feature = self.model_motion(clip)\n                    slow_feature = slow_feature.squeeze()\n                    fast_feature = fast_feature.squeeze()\n\n                    motion_feature = torch.cat([slow_feature, fast_feature]).unsqueeze(0)  # Shape: [1, 2304]\n                    feature_motion[idx] = motion_feature \n\n                feature_motion = feature_motion.unsqueeze(0)  # Shape: [1, n_clip, 2304]\n\n                outputs = self.model(spatial_features, feature_motion)\n                score = outputs.item()\n\n                results.append({\"video_name\": video_name, \"SimpleVQA_Score\": score})\n                print(f\"Processed score {score:.4f} for {video_name}\")\n\n        self.results.extend(results)\n\n    def compute_metrics(self, results: list) -&gt; Dict[str, float]:\n        \"\"\"Compute final SimpleVQA-based metrics.\"\"\"\n        scores = np.array([res[\"SimpleVQA_Score\"] for res in self.results])\n        mean_score = np.mean(scores) if scores.size &gt; 0 else 0.0\n        print(f\"SimpleVQA mean score: {mean_score:.4f}\")\n\n        json_file_path = os.path.join(os.getcwd(), \"simplevqa_results.json\")\n        final_results = {\"video_results\": self.results, \"SimpleVQA_Mean_Score\": mean_score}\n        with open(json_file_path, \"w\") as json_file:\n            json.dump(final_results, json_file, indent=4)\n        print(f\"SimpleVQA mean score saved to {json_file_path}\")\n\n        return {\"SimpleVQA_Mean_Score\": mean_score}\n</code></pre>"},{"location":"documentations/metrics/#aigve.metrics.SimpleVqa.compute_metrics","title":"<code>compute_metrics(results)</code>","text":"<p>Compute final SimpleVQA-based metrics.</p> Source code in <code>aigve/metrics/video_quality_assessment/nn_based/simplevqa/simplevqa_metric.py</code> <pre><code>def compute_metrics(self, results: list) -&gt; Dict[str, float]:\n    \"\"\"Compute final SimpleVQA-based metrics.\"\"\"\n    scores = np.array([res[\"SimpleVQA_Score\"] for res in self.results])\n    mean_score = np.mean(scores) if scores.size &gt; 0 else 0.0\n    print(f\"SimpleVQA mean score: {mean_score:.4f}\")\n\n    json_file_path = os.path.join(os.getcwd(), \"simplevqa_results.json\")\n    final_results = {\"video_results\": self.results, \"SimpleVQA_Mean_Score\": mean_score}\n    with open(json_file_path, \"w\") as json_file:\n        json.dump(final_results, json_file, indent=4)\n    print(f\"SimpleVQA mean score saved to {json_file_path}\")\n\n    return {\"SimpleVQA_Mean_Score\": mean_score}\n</code></pre>"},{"location":"documentations/metrics/#aigve.metrics.SimpleVqa.process","title":"<code>process(data_batch, data_samples)</code>","text":"<p>Process a batch of extracted deep features for SimpleVQA evaluation. Args:     data_batch (Sequence): A batch of data from the dataloader (not used here).     data_samples (List[ Tuple[torch.Tensor], List[Tuple[torch.Tensor]], Tuple[str] ]):         A list containing three tuples:         - A tuple of <code>spatial_features</code> (torch.Tensor): Shape [v_len_second, 3, 448, 448].              <code>v_len_second</code> is total seconds of the video (though 2 for toy dataset) with minium 8 (i.e. min_video_seconds).              The len of the tuple is the batch size.          - A list of <code>motion_features</code> (Tuple[torch.Tensor]):              len(List) is total seconds of the video, with minium 8 (i.e. min_video_seconds).             Each item of the list is a Tuple of motion feature tensors. Each has shape [32, 3, 224, 224].             The len of the tuple is the batch size.         - A tuple of <code>video_name</code> (str): Video filename. The len of the tuple is the batch size.</p> Source code in <code>aigve/metrics/video_quality_assessment/nn_based/simplevqa/simplevqa_metric.py</code> <pre><code>def process(self, data_batch: list, data_samples: list) -&gt; None:\n    \"\"\"\n    Process a batch of extracted deep features for SimpleVQA evaluation.\n    Args:\n        data_batch (Sequence): A batch of data from the dataloader (not used here).\n        data_samples (List[ Tuple[torch.Tensor], List[Tuple[torch.Tensor]], Tuple[str] ]):\n            A list containing three tuples:\n            - A tuple of `spatial_features` (torch.Tensor): Shape [v_len_second, 3, 448, 448]. \n                `v_len_second` is total seconds of the video (though 2 for toy dataset) with minium 8 (i.e. min_video_seconds). \n                The len of the tuple is the batch size. \n            - A list of `motion_features` (Tuple[torch.Tensor]): \n                len(List) is total seconds of the video, with minium 8 (i.e. min_video_seconds).\n                Each item of the list is a Tuple of motion feature tensors. Each has shape [32, 3, 224, 224].\n                The len of the tuple is the batch size.\n            - A tuple of `video_name` (str): Video filename. The len of the tuple is the batch size.\n    \"\"\"\n    from .SimpleVQA.test_demo import pack_pathway_output\n\n    results = []\n    # print(type(data_samples)) # list\n    spatial_features_tuple, motion_features_list, video_name_tuple = data_samples\n    # print(len(spatial_features_tuple)) # 1\n    # print(spatial_features_tuple[0].shape) # torch.Size([8, 3, 448, 448])\n\n    # print(type(motion_features_list)) # List\n    # print(len(motion_features_list)) # 8\n    # print(type(motion_features_list[0])) # tuple\n    # print(len(motion_features_list[0])) # 1\n    # print(type(motion_features_list[0][0])) # Tensor\n    # print(motion_features_list[0][0].shape) # torch.Size([32, 3, 224, 224])\n\n    batch_size = len(spatial_features_tuple)\n    with torch.no_grad():\n        for i in range(batch_size):\n            video_name = video_name_tuple[i]\n            spatial_features = spatial_features_tuple[i].to(self.device).unsqueeze(0)  # Add batch dim. Shape: tensor with Size([1, v_len_second, 3, 448, 448])\n\n            # Take the i-th element from each tuple in motion_features_list\n            motion_features = [motion_features_list[j][i] for j in range(len(motion_features_list))] # Shape: List[tensor with Size([32, 3, 224, 224])], len of it is total seconds of the video, with minium 8.\n\n            if not all(isinstance(mf, torch.Tensor) for mf in motion_features):\n                raise TypeError(\"Expected motion_features to be a list of tensors.\")\n\n            if len(motion_features) == 0:  # Edge case: No valid motion features\n                results.append({\"video_name\": video_name, \"SimpleVQA_Score\": 0.0})\n                continue\n\n            n_clip = len(motion_features)  # 8\n            feature_motion = torch.zeros([n_clip, 2048 + 256], device=self.device) \n            # Process each motion clip\n            for idx, clip in enumerate(motion_features):\n                clip = clip.unsqueeze(dim=0).permute(0, 2, 1, 3, 4)  # Reshape to [1, C(3), T(32), H(224), W(224)]\n                clip = pack_pathway_output(clip, self.device)  # Convert to SlowFast format\n                slow_feature, fast_feature = self.model_motion(clip)\n                slow_feature = slow_feature.squeeze()\n                fast_feature = fast_feature.squeeze()\n\n                motion_feature = torch.cat([slow_feature, fast_feature]).unsqueeze(0)  # Shape: [1, 2304]\n                feature_motion[idx] = motion_feature \n\n            feature_motion = feature_motion.unsqueeze(0)  # Shape: [1, n_clip, 2304]\n\n            outputs = self.model(spatial_features, feature_motion)\n            score = outputs.item()\n\n            results.append({\"video_name\": video_name, \"SimpleVQA_Score\": score})\n            print(f\"Processed score {score:.4f} for {video_name}\")\n\n    self.results.extend(results)\n</code></pre>"},{"location":"documentations/metrics/#aigve.metrics.TIFAScore","title":"<code>TIFAScore</code>","text":"<p>               Bases: <code>BaseMetric</code></p> <p>Initialize the <code>TIFAScore</code> evaluator.</p> <p>Parameters:</p> Name Type Description Default <code>openai_key</code> <code>str</code> <p>The user's api key of the LLM models openai provides.</p> required <code>llm_model</code> <code>str</code> <p>The name of the LLM model used in the TIFAScore evaluator. Defaults to <code>gpt-3.5-turbo</code>.</p> <code>'gpt-3.5-turbo'</code> <code>unifiedqa_model_name</code> <code>str</code> <p>The name of the <code>UnifiedQAModel</code> used in TIFAScore evaluator. Defaults to <code>allenai/unifiedqa-v2-t5-large-1363200</code>.</p> <code>'allenai/unifiedqa-v2-t5-large-1363200'</code> <code>vqa_model_name</code> <code>str</code> <p>The name of the <code>VQAModel used</code> in TIFAScore evaluator. Defaults to <code>mplug-large</code>.</p> <code>'mplug-large'</code> <p>Returns:</p> Type Description <p>None</p> Source code in <code>aigve/metrics/text_video_alignment/gpt_based/TIFA/tifa_eval.py</code> <pre><code>@METRICS.register_module()\nclass TIFAScore(BaseMetric):\n    \"\"\" Initialize the ``TIFAScore`` evaluator.\n\n    Args:   \n            openai_key (str): The user's api key of the LLM models openai provides.\n            llm_model (str): The name of the LLM model used in the TIFAScore evaluator. Defaults to ``gpt-3.5-turbo``.\n            unifiedqa_model_name (str): The name of the ``UnifiedQAModel`` used in TIFAScore evaluator. Defaults to ``allenai/unifiedqa-v2-t5-large-1363200``.\n            vqa_model_name (str): The name of the ``VQAModel used`` in TIFAScore evaluator. Defaults to ``mplug-large``.\n\n    Returns:\n            None\n    \"\"\"\n    def __init__(self, \n                 openai_key,\n                 llm_model: str = 'gpt-3.5-turbo',\n                 unifiedqa_model_name: str = 'allenai/unifiedqa-v2-t5-large-1363200',\n                 vqa_model_name: str = 'mplug-large'):\n        super().__init__()\n\n        self.openai_key = openai_key\n        self.llm_model = llm_model\n        self.unifiedqa_model_name = unifiedqa_model_name\n        self.openai_completion, self.get_question_and_answers, self.filter_question_and_answers, self.unifiedqa_model, self.tifa_score_single, self.vqa_model = lazy_import()\n        self.unifiedqa_model = self.UnifiedQAModel(self.unifiedqa_model_name)\n        self.vqa_model_name = vqa_model_name\n        self.vqa_model = self.VQAModel(self.vqa_model_name)\n\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n        self.openai_setup()\n\n    def openai_setup(self):\n        print('set up openai client')\n        openai.api_key = self.openai_key\n        assert openai.api_key is not None\n        test_prompt_string = 'hello, how are you doing?'\n        print('test prompt: ', test_prompt_string)\n        response = self.openai_completion(\n            test_prompt_string,\n            model=self.llm_model,\n        )\n        print('test response: ', response)\n\n\n    def process(self, data_batch: Sequence, data_samples: Sequence) -&gt; None:\n        \"\"\" TIFAScore process\n        Process one batch of data samples and predictions. The processed\n        results should be stored in ``self.results``, which will be used to\n        compute the metrics when all batches have been processed.\n\n        Args:\n            data_batch (Sequence): A batch of data from the dataloader.\n            data_samples (Sequence): A batch of data samples that\n                contain annotations and predictions.\n        \"\"\"\n\n        result = dict()\n\n        input_prompts, input_videos = data_samples\n        bsz = len(input_prompts)\n\n        # Ensure prompt_input is a tensor\n        if isinstance(input_prompts, tuple):\n            input_prompts = list(input_prompts)\n\n        if isinstance(input_videos, tuple):\n            input_videos = list(input_videos)\n\n        average_tifa_score_list = []\n        for input_prompt, input_video in zip(input_prompts, input_videos):\n            tifa_score = []\n            # Generate questions with GPT-3.5-turbo\n            gpt3_questions = self.get_question_and_answers(input_prompt)\n            # print(gpt3_questions)\n            # Filter questions with UnifiedQA\n            filtered_questions = self.filter_question_and_answers(self.unifiedqa_model, gpt3_questions)\n            for index, frame_path in enumerate(input_video):\n                # calucluate TIFA score\n                result = self.tifa_score_single(self.vqa_model, filtered_questions, frame_path)\n                # print(result)\n                tifa_score.append(result['tifa_score'])\n            average_tifa_score = sum(tifa_score)/len(tifa_score)\n            average_tifa_score_list.append(average_tifa_score)\n\n        result['tifa_score'] = sum(average_tifa_score_list)/len(average_tifa_score_list)\n\n        self.results.append(result)\n\n\n    def compute_metrics(self, results: list) -&gt; Dict[str, float]:\n        \"\"\"Compute the metrics from processed results.\n\n        Args:\n            results (list): The processed results of each batch.\n\n        Returns:\n            Dict[str, float]: The computed metrics. The keys are the names of\n            the metrics, and the values are corresponding results.\n        \"\"\"\n        logger: MMLogger = MMLogger.get_current_instance()\n\n        tifa_score_np = np.zeros(len(results))\n        for i, result in enumerate(results):\n            tifa_score_np[i] = result['tifa_score']\n\n        tifa_score_np_mean = np.mean(tifa_score_np) \n\n        print(\"Test results: tifa score={:.4f}\"\n              .format(tifa_score_np_mean))\n\n        return result\n</code></pre>"},{"location":"documentations/metrics/#aigve.metrics.TIFAScore.compute_metrics","title":"<code>compute_metrics(results)</code>","text":"<p>Compute the metrics from processed results.</p> <p>Parameters:</p> Name Type Description Default <code>results</code> <code>list</code> <p>The processed results of each batch.</p> required <p>Returns:</p> Type Description <code>Dict[str, float]</code> <p>Dict[str, float]: The computed metrics. The keys are the names of</p> <code>Dict[str, float]</code> <p>the metrics, and the values are corresponding results.</p> Source code in <code>aigve/metrics/text_video_alignment/gpt_based/TIFA/tifa_eval.py</code> <pre><code>def compute_metrics(self, results: list) -&gt; Dict[str, float]:\n    \"\"\"Compute the metrics from processed results.\n\n    Args:\n        results (list): The processed results of each batch.\n\n    Returns:\n        Dict[str, float]: The computed metrics. The keys are the names of\n        the metrics, and the values are corresponding results.\n    \"\"\"\n    logger: MMLogger = MMLogger.get_current_instance()\n\n    tifa_score_np = np.zeros(len(results))\n    for i, result in enumerate(results):\n        tifa_score_np[i] = result['tifa_score']\n\n    tifa_score_np_mean = np.mean(tifa_score_np) \n\n    print(\"Test results: tifa score={:.4f}\"\n          .format(tifa_score_np_mean))\n\n    return result\n</code></pre>"},{"location":"documentations/metrics/#aigve.metrics.TIFAScore.process","title":"<code>process(data_batch, data_samples)</code>","text":"<p>TIFAScore process Process one batch of data samples and predictions. The processed results should be stored in <code>self.results</code>, which will be used to compute the metrics when all batches have been processed.</p> <p>Parameters:</p> Name Type Description Default <code>data_batch</code> <code>Sequence</code> <p>A batch of data from the dataloader.</p> required <code>data_samples</code> <code>Sequence</code> <p>A batch of data samples that contain annotations and predictions.</p> required Source code in <code>aigve/metrics/text_video_alignment/gpt_based/TIFA/tifa_eval.py</code> <pre><code>def process(self, data_batch: Sequence, data_samples: Sequence) -&gt; None:\n    \"\"\" TIFAScore process\n    Process one batch of data samples and predictions. The processed\n    results should be stored in ``self.results``, which will be used to\n    compute the metrics when all batches have been processed.\n\n    Args:\n        data_batch (Sequence): A batch of data from the dataloader.\n        data_samples (Sequence): A batch of data samples that\n            contain annotations and predictions.\n    \"\"\"\n\n    result = dict()\n\n    input_prompts, input_videos = data_samples\n    bsz = len(input_prompts)\n\n    # Ensure prompt_input is a tensor\n    if isinstance(input_prompts, tuple):\n        input_prompts = list(input_prompts)\n\n    if isinstance(input_videos, tuple):\n        input_videos = list(input_videos)\n\n    average_tifa_score_list = []\n    for input_prompt, input_video in zip(input_prompts, input_videos):\n        tifa_score = []\n        # Generate questions with GPT-3.5-turbo\n        gpt3_questions = self.get_question_and_answers(input_prompt)\n        # print(gpt3_questions)\n        # Filter questions with UnifiedQA\n        filtered_questions = self.filter_question_and_answers(self.unifiedqa_model, gpt3_questions)\n        for index, frame_path in enumerate(input_video):\n            # calucluate TIFA score\n            result = self.tifa_score_single(self.vqa_model, filtered_questions, frame_path)\n            # print(result)\n            tifa_score.append(result['tifa_score'])\n        average_tifa_score = sum(tifa_score)/len(tifa_score)\n        average_tifa_score_list.append(average_tifa_score)\n\n    result['tifa_score'] = sum(average_tifa_score_list)/len(average_tifa_score_list)\n\n    self.results.append(result)\n</code></pre>"},{"location":"documentations/metrics/#aigve.metrics.VIEEvalScore","title":"<code>VIEEvalScore</code>","text":"<p>               Bases: <code>BaseMetric</code></p> <p>Initialize the <code>VIEEvalScore</code> evaluator.</p> <p>Parameters:</p> Name Type Description Default <code>llm_backbone</code> <code>str</code> <p>The name of the LLM model used in the VIEEvalScore evaluator. Defaults to <code>got4o</code>.</p> <code>'gpt4o'</code> <code>api_key_path</code> <code>str</code> <p>The user's api key path to initialize LLM models provides by openai.</p> <code>'AIGVE_Tool/metrics/text_video_alignment/gpt_based/VIE/api_key.txt'</code> <code>task(str)</code> <p>The task the VIEEvalScore evaluator conducts. Defaults to ''t2v''.</p> required <p>Returns:</p> Type Description <p>None</p> Source code in <code>aigve/metrics/text_video_alignment/gpt_based/VIE/vie_eval.py</code> <pre><code>@METRICS.register_module()\nclass VIEEvalScore(BaseMetric):\n    \"\"\" Initialize the ``VIEEvalScore`` evaluator.\n\n    Args:\n            llm_backbone (str): The name of the LLM model used in the VIEEvalScore evaluator. Defaults to ``got4o``.\n            api_key_path (str): The user's api key path to initialize LLM models provides by openai.\n            task(str): The task the VIEEvalScore evaluator conducts. Defaults to ''t2v''.\n\n    Returns:\n            None\n    \"\"\"\n    def __init__(self,\n                 llm_backbone: str = \"gpt4o\",\n                 api_key_path: str = 'AIGVE_Tool/metrics/text_video_alignment/gpt_based/VIE/api_key.txt',\n                 task: str = 't2v',\n                 ):\n        super().__init__()\n\n        self.api_key_path = api_key_path\n        self.llm_backbone = llm_backbone\n        self.task = task\n\n        self.submodel_path = 'metrics/text_video_alignment/gpt_based/VIE'\n        if not submodule_exists(self.submodel_path):\n            add_git_submodule(\n                repo_url='https://github.com/TIGER-AI-Lab/VIEScore.git', \n                submodule_path=self.submodel_path\n            )  \n        self.submodel_path = 'metrics/text_video_alignment/gpt_based/dsg'\n        if not submodule_exists(self.submodel_path):\n            add_git_submodule(\n                repo_url='https://github.com/j-min/DSG.git', \n                submodule_path=self.submodel_path\n            )  \n        from .VIEScore.viescore import VIEScore \n        from .DSG.dsg.vqa_utils import MPLUG, InstructBLIP\n\n\n        self.vie_score = VIEScore(backbone=self.llm_backbone, task=self.task, key_path=self.api_key_path)\n\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\n    def process(self, data_batch: Sequence, data_samples: Sequence) -&gt; None:\n        \"\"\"VIEScore process\n        Process one batch of data samples and predictions. The processed\n        results should be stored in ``self.results``, which will be used to\n        compute the metrics when all batches have been processed.\n\n        Args:\n            data_batch (Sequence): A batch of data from the dataloader.\n            data_samples (Sequence): A batch of data samples that\n                contain annotations and predictions.\n        \"\"\"\n\n        result = dict()\n\n        input_prompts, input_videos = data_samples\n        bsz = len(input_prompts)\n\n        # Ensure prompt_input is a tensor\n        if isinstance(input_prompts, tuple):\n            input_prompts = list(input_prompts)\n\n        if isinstance(input_videos, tuple):\n            input_videos = list(input_videos)\n\n        average_vie_score_list = []\n        for input_prompt, input_video in zip(input_prompts, input_videos):\n            vie_score_list = []\n            for index, frame_path in enumerate(input_video):\n                pil_image = Image.open(frame_path)\n                score_list = self.vie_score.evaluate(pil_image, input_prompt)\n                sementics_score, quality_score, overall_score = score_list\n                vie_score_list.append(overall_score)\n            average_vie_score = sum(vie_score_list)/len(vie_score_list)\n            average_vie_score_list.append(average_vie_score)\n\n        result['vie_score'] = sum(average_vie_score_list)/len(average_vie_score_list)\n\n        self.results.append(result)\n\n\n    def compute_metrics(self, results: list) -&gt; Dict[str, float]:\n        \"\"\"Compute the metrics from processed results.\n\n        Args:\n            results (list): The processed results of each batch.\n\n        Returns:\n            Dict[str, float]: The computed metrics. The keys are the names of\n            the metrics, and the values are corresponding results.\n        \"\"\"\n        logger: MMLogger = MMLogger.get_current_instance()\n\n        vie_score_np = np.zeros(len(results))\n        for i, result in enumerate(results):\n            vie_score_np[i] = result['vie_score']\n\n        vie_score_np_mean = np.mean(vie_score_np) \n\n        print(\"Test results: vie score with dependency={:.4f}\"\n              .format(vie_score_np_mean))\n\n        return result\n</code></pre>"},{"location":"documentations/metrics/#aigve.metrics.VIEEvalScore.compute_metrics","title":"<code>compute_metrics(results)</code>","text":"<p>Compute the metrics from processed results.</p> <p>Parameters:</p> Name Type Description Default <code>results</code> <code>list</code> <p>The processed results of each batch.</p> required <p>Returns:</p> Type Description <code>Dict[str, float]</code> <p>Dict[str, float]: The computed metrics. The keys are the names of</p> <code>Dict[str, float]</code> <p>the metrics, and the values are corresponding results.</p> Source code in <code>aigve/metrics/text_video_alignment/gpt_based/VIE/vie_eval.py</code> <pre><code>def compute_metrics(self, results: list) -&gt; Dict[str, float]:\n    \"\"\"Compute the metrics from processed results.\n\n    Args:\n        results (list): The processed results of each batch.\n\n    Returns:\n        Dict[str, float]: The computed metrics. The keys are the names of\n        the metrics, and the values are corresponding results.\n    \"\"\"\n    logger: MMLogger = MMLogger.get_current_instance()\n\n    vie_score_np = np.zeros(len(results))\n    for i, result in enumerate(results):\n        vie_score_np[i] = result['vie_score']\n\n    vie_score_np_mean = np.mean(vie_score_np) \n\n    print(\"Test results: vie score with dependency={:.4f}\"\n          .format(vie_score_np_mean))\n\n    return result\n</code></pre>"},{"location":"documentations/metrics/#aigve.metrics.VIEEvalScore.process","title":"<code>process(data_batch, data_samples)</code>","text":"<p>VIEScore process Process one batch of data samples and predictions. The processed results should be stored in <code>self.results</code>, which will be used to compute the metrics when all batches have been processed.</p> <p>Parameters:</p> Name Type Description Default <code>data_batch</code> <code>Sequence</code> <p>A batch of data from the dataloader.</p> required <code>data_samples</code> <code>Sequence</code> <p>A batch of data samples that contain annotations and predictions.</p> required Source code in <code>aigve/metrics/text_video_alignment/gpt_based/VIE/vie_eval.py</code> <pre><code>def process(self, data_batch: Sequence, data_samples: Sequence) -&gt; None:\n    \"\"\"VIEScore process\n    Process one batch of data samples and predictions. The processed\n    results should be stored in ``self.results``, which will be used to\n    compute the metrics when all batches have been processed.\n\n    Args:\n        data_batch (Sequence): A batch of data from the dataloader.\n        data_samples (Sequence): A batch of data samples that\n            contain annotations and predictions.\n    \"\"\"\n\n    result = dict()\n\n    input_prompts, input_videos = data_samples\n    bsz = len(input_prompts)\n\n    # Ensure prompt_input is a tensor\n    if isinstance(input_prompts, tuple):\n        input_prompts = list(input_prompts)\n\n    if isinstance(input_videos, tuple):\n        input_videos = list(input_videos)\n\n    average_vie_score_list = []\n    for input_prompt, input_video in zip(input_prompts, input_videos):\n        vie_score_list = []\n        for index, frame_path in enumerate(input_video):\n            pil_image = Image.open(frame_path)\n            score_list = self.vie_score.evaluate(pil_image, input_prompt)\n            sementics_score, quality_score, overall_score = score_list\n            vie_score_list.append(overall_score)\n        average_vie_score = sum(vie_score_list)/len(vie_score_list)\n        average_vie_score_list.append(average_vie_score)\n\n    result['vie_score'] = sum(average_vie_score_list)/len(average_vie_score_list)\n\n    self.results.append(result)\n</code></pre>"},{"location":"documentations/metrics/#aigve.metrics.VideoPhy","title":"<code>VideoPhy</code>","text":"<p>               Bases: <code>BaseMetric</code></p> Source code in <code>aigve/metrics/multi_aspect_metrics/videophy/videophy_metric.py</code> <pre><code>@METRICS.register_module()\nclass VideoPhy(BaseMetric):\n    def __init__(self,\n                hf_token: str,\n                collect_device: Optional[Union[str, torch.device]] = None,\n                prefix: Optional[str] = None,\n                metric_path: str = None,\n                model_path: str = 'videophysics/videocon_physics',\n                datainfo_path: str = None,\n                test_index: int = None,\n                 **kwargs):\n\n        \"\"\"\n        This function is used to initialize the VideoPhy metric.\n\n        Args:\n            collect_device (str or torch.device): The device to use for collecting the data\n            prefix (str): The prefix to use for the metric name\n            metric_path (str): The path to the metric\n            model_path (str): The path to the model\n            datainfo_path (str): The path to the data info\n            test_index (int): The index of the test\n        \"\"\"\n\n        super().__init__(collect_device=collect_device, prefix=prefix)\n        # self.train_index = train_index\n        self.metric_path = metric_path\n        self.model_path = model_path\n        self.datainfo_path = datainfo_path\n        self.test_index = test_index\n        self.hf_token = hf_token\n        self.results = []\n\n        # self.submodule_path = './metrics/aigve'\n        # if not submodule_exists(self.submodule_path):\n        #     add_git_submodule(\n        #         repo_url='https://github.com/Hritikbansal/videophy.git',\n        #         submodule_path=self.submodule_path\n        #     )\n\n        self.tokenizer = LlamaTokenizer.from_pretrained(self.model_path, token=self.hf_token)\n        self.image_processor = MplugOwlImageProcessor.from_pretrained(self.model_path)\n        self.processor = MplugOwlProcessor(self.image_processor, self.tokenizer)\n        self.model = MplugOwlForConditionalGeneration.from_pretrained(\n            self.model_path,\n            torch_dtype=torch.bfloat16,\n        ).to('cuda')\n        self.model.eval()\n\n    def get_entail(self, logits, input_ids):\n        \"\"\"\n        This function is used to get the entailment scores.\n\n        Args:\n            logits (torch.Tensor): A tensor containing the logits\n            input_ids (torch.Tensor): A tensor containing the input IDs\n        \"\"\"\n        softmax = nn.Softmax(dim=2)\n        logits = softmax(logits)\n        token_id_yes = self.tokenizer.encode('Yes', add_special_tokens=False)[0]\n        token_id_no = self.tokenizer.encode('No', add_special_tokens=False)[0]\n        entailment = []\n        for j in range(len(logits)):\n            for i in range(len(input_ids[j])):\n                if input_ids[j][i] == self.tokenizer.pad_token_id:  # pad token if the answer is not present\n                    i = i - 1\n                    break\n                elif i == len(input_ids[j]) - 1:\n                    break\n            score = logits[j][i][token_id_yes] / (logits[j][i][token_id_yes] + logits[j][i][token_id_no])\n            entailment.append(score)\n        entailment = torch.stack(entailment)\n        return entailment\n\n    def get_logits(self, data_batch):\n        \"\"\"\n        This function is used to get the logits for each input in the data batch.\n\n        Args:\n            data_batch (dict): A dictionary containing the data batch\n        Returns:\n            logits (torch.Tensor): A tensor containing the logits for each input in the data batch\n        \"\"\"\n        # Iterate over each item in the data batch\n        for k, v in data_batch.items():\n            # Check if the item is a tensor\n            if torch.is_tensor(v):\n                # Convert float tensors to bfloat16\n                if v.dtype == torch.float:\n                    data_batch[k] = v.bfloat16()\n                # Move the tensor to the model's device (e.g., GPU)\n                data_batch[k] = data_batch[k].to(self.model.device)\n\n        # print(\"Data batch: \", data_batch.keys())\n        outputs = self.model(pixel_values=data_batch['pixel_values'], video_pixel_values=data_batch['video_pixel_values'],\n                        labels=None, \\\n                        num_images=data_batch['num_images'], num_videos=data_batch['num_videos'], input_ids=data_batch['input_ids'],\n                        non_padding_mask=data_batch['non_padding_mask'], \\\n                        non_media_mask=data_batch['non_media_mask'], prompt_mask=data_batch['prompt_mask'])\n        logits = outputs['logits']\n        return logits\n\n\n    def process(self, data_batch: Any, data_samples: Sequence[dict]) -&gt; None:\n        \"\"\"\n        This function is used to process the data batch and compute the metric.\n\n        Args:\n            data_batch (dict): A dictionary containing the data batch\n            data_samples (list): A list of dictionaries containing the data samples\n        \"\"\"\n        logits = self.get_logits(data_batch)\n        entails_scores =  self.get_entail(logits, data_batch['input_ids'])\n\n        self.results.extend(entails_scores.cpu().detach().to(torch.float32).numpy().tolist())\n        # self.results = entails_scores.cpu().detach().to(torch.float32).numpy().tolist()\n        # print(self.results)\n\n\n    def compute_metrics(self, results: list) -&gt; dict:\n        \"\"\"\n        This function is used to compute the metrics.\n\n        Args:\n            results (list): A list of results\n        \"\"\"\n        return {\n            'entailment': float(np.mean(results))\n        }\n</code></pre>"},{"location":"documentations/metrics/#aigve.metrics.VideoPhy.__init__","title":"<code>__init__(hf_token, collect_device=None, prefix=None, metric_path=None, model_path='videophysics/videocon_physics', datainfo_path=None, test_index=None, **kwargs)</code>","text":"<p>This function is used to initialize the VideoPhy metric.</p> <p>Parameters:</p> Name Type Description Default <code>collect_device</code> <code>str or device</code> <p>The device to use for collecting the data</p> <code>None</code> <code>prefix</code> <code>str</code> <p>The prefix to use for the metric name</p> <code>None</code> <code>metric_path</code> <code>str</code> <p>The path to the metric</p> <code>None</code> <code>model_path</code> <code>str</code> <p>The path to the model</p> <code>'videophysics/videocon_physics'</code> <code>datainfo_path</code> <code>str</code> <p>The path to the data info</p> <code>None</code> <code>test_index</code> <code>int</code> <p>The index of the test</p> <code>None</code> Source code in <code>aigve/metrics/multi_aspect_metrics/videophy/videophy_metric.py</code> <pre><code>def __init__(self,\n            hf_token: str,\n            collect_device: Optional[Union[str, torch.device]] = None,\n            prefix: Optional[str] = None,\n            metric_path: str = None,\n            model_path: str = 'videophysics/videocon_physics',\n            datainfo_path: str = None,\n            test_index: int = None,\n             **kwargs):\n\n    \"\"\"\n    This function is used to initialize the VideoPhy metric.\n\n    Args:\n        collect_device (str or torch.device): The device to use for collecting the data\n        prefix (str): The prefix to use for the metric name\n        metric_path (str): The path to the metric\n        model_path (str): The path to the model\n        datainfo_path (str): The path to the data info\n        test_index (int): The index of the test\n    \"\"\"\n\n    super().__init__(collect_device=collect_device, prefix=prefix)\n    # self.train_index = train_index\n    self.metric_path = metric_path\n    self.model_path = model_path\n    self.datainfo_path = datainfo_path\n    self.test_index = test_index\n    self.hf_token = hf_token\n    self.results = []\n\n    # self.submodule_path = './metrics/aigve'\n    # if not submodule_exists(self.submodule_path):\n    #     add_git_submodule(\n    #         repo_url='https://github.com/Hritikbansal/videophy.git',\n    #         submodule_path=self.submodule_path\n    #     )\n\n    self.tokenizer = LlamaTokenizer.from_pretrained(self.model_path, token=self.hf_token)\n    self.image_processor = MplugOwlImageProcessor.from_pretrained(self.model_path)\n    self.processor = MplugOwlProcessor(self.image_processor, self.tokenizer)\n    self.model = MplugOwlForConditionalGeneration.from_pretrained(\n        self.model_path,\n        torch_dtype=torch.bfloat16,\n    ).to('cuda')\n    self.model.eval()\n</code></pre>"},{"location":"documentations/metrics/#aigve.metrics.VideoPhy.compute_metrics","title":"<code>compute_metrics(results)</code>","text":"<p>This function is used to compute the metrics.</p> <p>Parameters:</p> Name Type Description Default <code>results</code> <code>list</code> <p>A list of results</p> required Source code in <code>aigve/metrics/multi_aspect_metrics/videophy/videophy_metric.py</code> <pre><code>def compute_metrics(self, results: list) -&gt; dict:\n    \"\"\"\n    This function is used to compute the metrics.\n\n    Args:\n        results (list): A list of results\n    \"\"\"\n    return {\n        'entailment': float(np.mean(results))\n    }\n</code></pre>"},{"location":"documentations/metrics/#aigve.metrics.VideoPhy.get_entail","title":"<code>get_entail(logits, input_ids)</code>","text":"<p>This function is used to get the entailment scores.</p> <p>Parameters:</p> Name Type Description Default <code>logits</code> <code>Tensor</code> <p>A tensor containing the logits</p> required <code>input_ids</code> <code>Tensor</code> <p>A tensor containing the input IDs</p> required Source code in <code>aigve/metrics/multi_aspect_metrics/videophy/videophy_metric.py</code> <pre><code>def get_entail(self, logits, input_ids):\n    \"\"\"\n    This function is used to get the entailment scores.\n\n    Args:\n        logits (torch.Tensor): A tensor containing the logits\n        input_ids (torch.Tensor): A tensor containing the input IDs\n    \"\"\"\n    softmax = nn.Softmax(dim=2)\n    logits = softmax(logits)\n    token_id_yes = self.tokenizer.encode('Yes', add_special_tokens=False)[0]\n    token_id_no = self.tokenizer.encode('No', add_special_tokens=False)[0]\n    entailment = []\n    for j in range(len(logits)):\n        for i in range(len(input_ids[j])):\n            if input_ids[j][i] == self.tokenizer.pad_token_id:  # pad token if the answer is not present\n                i = i - 1\n                break\n            elif i == len(input_ids[j]) - 1:\n                break\n        score = logits[j][i][token_id_yes] / (logits[j][i][token_id_yes] + logits[j][i][token_id_no])\n        entailment.append(score)\n    entailment = torch.stack(entailment)\n    return entailment\n</code></pre>"},{"location":"documentations/metrics/#aigve.metrics.VideoPhy.get_logits","title":"<code>get_logits(data_batch)</code>","text":"<p>This function is used to get the logits for each input in the data batch.</p> <p>Parameters:</p> Name Type Description Default <code>data_batch</code> <code>dict</code> <p>A dictionary containing the data batch</p> required <p>Returns:     logits (torch.Tensor): A tensor containing the logits for each input in the data batch</p> Source code in <code>aigve/metrics/multi_aspect_metrics/videophy/videophy_metric.py</code> <pre><code>def get_logits(self, data_batch):\n    \"\"\"\n    This function is used to get the logits for each input in the data batch.\n\n    Args:\n        data_batch (dict): A dictionary containing the data batch\n    Returns:\n        logits (torch.Tensor): A tensor containing the logits for each input in the data batch\n    \"\"\"\n    # Iterate over each item in the data batch\n    for k, v in data_batch.items():\n        # Check if the item is a tensor\n        if torch.is_tensor(v):\n            # Convert float tensors to bfloat16\n            if v.dtype == torch.float:\n                data_batch[k] = v.bfloat16()\n            # Move the tensor to the model's device (e.g., GPU)\n            data_batch[k] = data_batch[k].to(self.model.device)\n\n    # print(\"Data batch: \", data_batch.keys())\n    outputs = self.model(pixel_values=data_batch['pixel_values'], video_pixel_values=data_batch['video_pixel_values'],\n                    labels=None, \\\n                    num_images=data_batch['num_images'], num_videos=data_batch['num_videos'], input_ids=data_batch['input_ids'],\n                    non_padding_mask=data_batch['non_padding_mask'], \\\n                    non_media_mask=data_batch['non_media_mask'], prompt_mask=data_batch['prompt_mask'])\n    logits = outputs['logits']\n    return logits\n</code></pre>"},{"location":"documentations/metrics/#aigve.metrics.VideoPhy.process","title":"<code>process(data_batch, data_samples)</code>","text":"<p>This function is used to process the data batch and compute the metric.</p> <p>Parameters:</p> Name Type Description Default <code>data_batch</code> <code>dict</code> <p>A dictionary containing the data batch</p> required <code>data_samples</code> <code>list</code> <p>A list of dictionaries containing the data samples</p> required Source code in <code>aigve/metrics/multi_aspect_metrics/videophy/videophy_metric.py</code> <pre><code>def process(self, data_batch: Any, data_samples: Sequence[dict]) -&gt; None:\n    \"\"\"\n    This function is used to process the data batch and compute the metric.\n\n    Args:\n        data_batch (dict): A dictionary containing the data batch\n        data_samples (list): A list of dictionaries containing the data samples\n    \"\"\"\n    logits = self.get_logits(data_batch)\n    entails_scores =  self.get_entail(logits, data_batch['input_ids'])\n\n    self.results.extend(entails_scores.cpu().detach().to(torch.float32).numpy().tolist())\n</code></pre>"},{"location":"documentations/metrics/#aigve.metrics.VideoScore","title":"<code>VideoScore</code>","text":"<p>               Bases: <code>BaseMetric</code></p> Source code in <code>aigve/metrics/multi_aspect_metrics/videoscore/videoscore_metric.py</code> <pre><code>@METRICS.register_module()\nclass VideoScore(BaseMetric):\n    def __init__(self,\n                collect_device: Optional[Union[str, torch.device]] = None,\n                prefix: Optional[str] = None,\n                metric_path: str = None,\n                model_path: str = 'TIGER-Lab/VideoScore-v1.1',\n                datainfo_path: str = None,\n                test_index: int = None,\n                 **kwargs):\n        \"\"\"\n        Args:\n            collect_device (Optional[Union[str, torch.device]]): The device to collect the data on.\n            prefix (Optional[str]): The prefix to use for the metric.\n            metric_path (str): The path to the metric file.\n            model_path (str): The path to the model file.\n            datainfo_path (str): The path to the datainfo file.\n            test_index (int): The index of the test data.\n        \"\"\"\n        super().__init__(collect_device=collect_device, prefix=prefix)\n        # self.train_index = train_index\n        # TODO: ARE THERE PARAMETERS REQUIRED FOR THIS METRIC?\n        self.metric_path = metric_path\n        self.model_path = model_path\n        self.datainfo_path = datainfo_path\n        self.test_index = test_index\n\n\n        self.model = Idefics2ForSequenceClassification.from_pretrained(self.model_path, torch_dtype=torch.bfloat16).eval()\n        self.device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n        self.model.to(self.device)\n\n        self.results = []\n\n    def process(self, data_batch: Any, data_samples: Sequence[dict]) -&gt; None:\n        \"\"\"\n        Args:\n            data_batch (Any): The data batch to process.\n            data_samples (Sequence[dict]): The data samples to process.\n        \"\"\"\n\n\n        data_batch = {k: v[0].to(self.model.device) for k, v in data_batch.items()}\n\n        with torch.no_grad():\n            outputs = self.model(**data_batch)\n\n        logits = outputs.logits.cpu().detach().to(torch.float32).numpy()\n        num_aspects = logits.shape[-1]\n\n        aspect_scores = []\n        for i in range(num_aspects):\n            aspect_scores.append(round(logits[0, i].item(), 3))\n\n        self.results.append(aspect_scores)\n\n    def compute_metrics(self, results: list) -&gt; dict:\n        \"\"\"\n        Args:\n            results (list): The results to compute the metrics from.\n        \"\"\"\n        results = np.array(results)\n        mean_scores = np.mean(results, axis=1)\n\n        return {'visual_quailty': results[:, 0].tolist(),\n                'temporal_consistency': results[:, 1].tolist(),\n                'dynamic_degree': results[:, 2].tolist(),\n                'text-to-video_alignment': results[:, 3].tolist(),\n                'factual_consistency': results[:, 4].tolist(),\n                'summary': {'visual_quality': mean_scores[0], 'temporal_consistency': mean_scores[1],\n                            'dynamic_degree': mean_scores[2], 'text-to-video_alignment': mean_scores[3],\n                            'factual_consistency': mean_scores[4]}}\n</code></pre>"},{"location":"documentations/metrics/#aigve.metrics.VideoScore.__init__","title":"<code>__init__(collect_device=None, prefix=None, metric_path=None, model_path='TIGER-Lab/VideoScore-v1.1', datainfo_path=None, test_index=None, **kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>collect_device</code> <code>Optional[Union[str, device]]</code> <p>The device to collect the data on.</p> <code>None</code> <code>prefix</code> <code>Optional[str]</code> <p>The prefix to use for the metric.</p> <code>None</code> <code>metric_path</code> <code>str</code> <p>The path to the metric file.</p> <code>None</code> <code>model_path</code> <code>str</code> <p>The path to the model file.</p> <code>'TIGER-Lab/VideoScore-v1.1'</code> <code>datainfo_path</code> <code>str</code> <p>The path to the datainfo file.</p> <code>None</code> <code>test_index</code> <code>int</code> <p>The index of the test data.</p> <code>None</code> Source code in <code>aigve/metrics/multi_aspect_metrics/videoscore/videoscore_metric.py</code> <pre><code>def __init__(self,\n            collect_device: Optional[Union[str, torch.device]] = None,\n            prefix: Optional[str] = None,\n            metric_path: str = None,\n            model_path: str = 'TIGER-Lab/VideoScore-v1.1',\n            datainfo_path: str = None,\n            test_index: int = None,\n             **kwargs):\n    \"\"\"\n    Args:\n        collect_device (Optional[Union[str, torch.device]]): The device to collect the data on.\n        prefix (Optional[str]): The prefix to use for the metric.\n        metric_path (str): The path to the metric file.\n        model_path (str): The path to the model file.\n        datainfo_path (str): The path to the datainfo file.\n        test_index (int): The index of the test data.\n    \"\"\"\n    super().__init__(collect_device=collect_device, prefix=prefix)\n    # self.train_index = train_index\n    # TODO: ARE THERE PARAMETERS REQUIRED FOR THIS METRIC?\n    self.metric_path = metric_path\n    self.model_path = model_path\n    self.datainfo_path = datainfo_path\n    self.test_index = test_index\n\n\n    self.model = Idefics2ForSequenceClassification.from_pretrained(self.model_path, torch_dtype=torch.bfloat16).eval()\n    self.device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n    self.model.to(self.device)\n\n    self.results = []\n</code></pre>"},{"location":"documentations/metrics/#aigve.metrics.VideoScore.compute_metrics","title":"<code>compute_metrics(results)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>results</code> <code>list</code> <p>The results to compute the metrics from.</p> required Source code in <code>aigve/metrics/multi_aspect_metrics/videoscore/videoscore_metric.py</code> <pre><code>def compute_metrics(self, results: list) -&gt; dict:\n    \"\"\"\n    Args:\n        results (list): The results to compute the metrics from.\n    \"\"\"\n    results = np.array(results)\n    mean_scores = np.mean(results, axis=1)\n\n    return {'visual_quailty': results[:, 0].tolist(),\n            'temporal_consistency': results[:, 1].tolist(),\n            'dynamic_degree': results[:, 2].tolist(),\n            'text-to-video_alignment': results[:, 3].tolist(),\n            'factual_consistency': results[:, 4].tolist(),\n            'summary': {'visual_quality': mean_scores[0], 'temporal_consistency': mean_scores[1],\n                        'dynamic_degree': mean_scores[2], 'text-to-video_alignment': mean_scores[3],\n                        'factual_consistency': mean_scores[4]}}\n</code></pre>"},{"location":"documentations/metrics/#aigve.metrics.VideoScore.process","title":"<code>process(data_batch, data_samples)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>data_batch</code> <code>Any</code> <p>The data batch to process.</p> required <code>data_samples</code> <code>Sequence[dict]</code> <p>The data samples to process.</p> required Source code in <code>aigve/metrics/multi_aspect_metrics/videoscore/videoscore_metric.py</code> <pre><code>def process(self, data_batch: Any, data_samples: Sequence[dict]) -&gt; None:\n    \"\"\"\n    Args:\n        data_batch (Any): The data batch to process.\n        data_samples (Sequence[dict]): The data samples to process.\n    \"\"\"\n\n\n    data_batch = {k: v[0].to(self.model.device) for k, v in data_batch.items()}\n\n    with torch.no_grad():\n        outputs = self.model(**data_batch)\n\n    logits = outputs.logits.cpu().detach().to(torch.float32).numpy()\n    num_aspects = logits.shape[-1]\n\n    aspect_scores = []\n    for i in range(num_aspects):\n        aspect_scores.append(round(logits[0, i].item(), 3))\n\n    self.results.append(aspect_scores)\n</code></pre>"},{"location":"documentations/metrics/#organization-of-this-module","title":"Organization of this Module","text":""},{"location":"documentations/metrics/#neural-network-based-evaluation-metrics","title":"Neural Network-Based Evaluation Metrics","text":"<ul> <li>GSTVQA</li> <li>SimpleVQA</li> <li>ModularBVQA</li> </ul>"},{"location":"documentations/metrics/#distribution-based-evaluation-metricsn-metrics","title":"Distribution-Based Evaluation Metricsn Metrics","text":"<ul> <li>FID</li> <li>FVD</li> <li>IS Score </li> </ul>"},{"location":"documentations/metrics/#vision-language-similarity-based-evaluation-metrics-metrics","title":"Vision-Language Similarity-Based Evaluation Metrics Metrics","text":"<ul> <li>CLIPSim</li> <li>CLIPTemp</li> <li>BLIPSim</li> <li>Pickscore</li> </ul>"},{"location":"documentations/metrics/#vision-language-understanding-based-evaluation-metrics","title":"Vision-Language Understanding-Based Evaluation Metrics","text":"<ul> <li>VIEScore</li> <li>TIFA</li> <li>DSG</li> </ul>"},{"location":"documentations/metrics/#multi-faceted-evaluation-metrics","title":"Multi-Faceted Evaluation Metrics","text":"<ul> <li>VideoPhy</li> <li>VBench</li> <li>EvalCrafter</li> </ul>"},{"location":"documentations/metrics/blipsim/","title":"class BlipSimScore","text":"<p>               Bases: <code>BaseMetric</code></p> <p>Initialize the <code>BLIPSimScore</code> evaluator.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>The name of the BLIP model. Defaults to <code>Salesforce/blip-itm-base-coco</code>.</p> <code>'Salesforce/blip-itm-base-coco'</code> <code>logit_scale</code> <code>bool</code> <p>Whether to calcualte the cosine similarity as logits. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <p>None</p> Source code in <code>aigve/metrics/text_video_alignment/similarity_based/blipscore/blipsim.py</code> <pre><code>@METRICS.register_module()\nclass BlipSimScore(BaseMetric):\n    \"\"\" Initialize the ``BLIPSimScore`` evaluator.\n\n    Args:\n            model_name (str): The name of the BLIP model. Defaults to ``Salesforce/blip-itm-base-coco``.\n            logit_scale (bool): Whether to calcualte the cosine similarity as logits. Defaults to False.\n\n    Returns:\n            None\n    \"\"\"\n    def __init__(self,\n                 model_name: str = \"Salesforce/blip-itm-base-coco\",\n                 logit_scale: bool = False,\n                 ) -&gt; None:\n        super().__init__()\n        self.model_name = model_name\n        self.logit_scale = logit_scale\n\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self.model = BlipForImageTextRetrieval.from_pretrained(self.model_name).to(self.device)\n        self.model.eval()\n\n\n# def process(self, data_batch: dict, data_samples: Sequence[dict]) -&gt; None:\n    def process(self, data_batch: Sequence, data_samples: Sequence) -&gt; None:\n        \"\"\"BLIPSimScore process\n        Process one batch of data samples and predictions. The processed\n        results should be stored in ``self.results``, which will be used to\n        compute the metrics when all batches have been processed.\n\n        Args:\n            data_batch (Sequence): A batch of data from the dataloader.\n            data_samples (Sequence): A batch of data samples that\n                contain annotations and predictions.\n        \"\"\"\n\n        result = dict()\n\n        input_prompts, input_videos = data_samples  \n        bsz = len(input_prompts)\n\n        # Ensure prompt_input is a tensor\n        if isinstance(input_prompts, tuple):\n            input_prompts = list(input_prompts)\n\n        if isinstance(input_videos, tuple):\n            input_videos = list(input_videos)\n\n\n        # Initialize an empty tensor to store the concatenated features\n        blip_score_sum, blip_score_cnt = 0, 0\n        logit_scale = self.model.logit_scale.exp() if self.logit_scale else 1\n        with torch.no_grad():\n            for input_prompt, input_frames in zip(input_prompts, input_videos):\n                # If frame is a tuple, extract the tensor. Assume tensor is the first element.\n                # if isinstance(input_prompt_frame_pair, tuple):\n                #     input_prompt_frame_pair = input_prompt_frame_pair[0]\n\n                # for key, value in input_prompt_frame_pair.items():\n                #     if isinstance(value, list):\n                #         input_prompt_frame_pair[key] = value[0]\n\n                # input_prompt_frame_pair = input_prompt_frame_pair.to(\"cuda\")  # Add batch dimension and move the frame to the device\n                # blip_cosine_sim_score = self.model(**input_prompt_frame_pair, use_itm_head=False)[0].item()\n                # blip_scores.append(blip_cosine_sim_score)\n                input_prompt = input_prompt.to(self.device)\n                input_frames = input_frames.to(self.device)\n                blip_cosine_sim_score = self.model(input_ids=input_prompt, pixel_values=input_frames, use_itm_head=False)[0].mean().item()\n                blip_cosine_sim_score *= logit_scale\n                print('current blip cosine similarity score', blip_cosine_sim_score)\n                blip_score_sum += blip_cosine_sim_score\n                blip_score_cnt += 1\n\n        # Calculate the average BLIP score across all frames\n        blip_score_frames_avg = blip_score_sum/blip_score_cnt\n\n        result['blip_sim_score'] = blip_score_frames_avg\n\n        self.results.append(result)\n\n\n    def compute_metrics(self, results: list) -&gt; Dict[str, float]:\n        \"\"\"Compute the metrics from processed results.\n\n        Args:\n            results (list): The processed results of each batch.\n\n        Returns:\n            Dict[str, float]: The computed metrics. The keys are the names of\n            the metrics, and the values are corresponding results.\n        \"\"\"\n        logger: MMLogger = MMLogger.get_current_instance()\n\n        blip_score_np = np.zeros(len(results))\n        for i, result in enumerate(results):\n            blip_score_np[i] = result['blip_sim_score']\n\n        blip_sim_mean = np.mean(blip_score_np) \n\n        print(\"Test results: blip similarity score={:.4f}\"\n              .format(blip_sim_mean))\n\n        return result\n</code></pre>"},{"location":"documentations/metrics/blipsim/#aigve.metrics.text_video_alignment.similarity_based.blipscore.BlipSimScore.compute_metrics","title":"<code>compute_metrics(results)</code>","text":"<p>Compute the metrics from processed results.</p> <p>Parameters:</p> Name Type Description Default <code>results</code> <code>list</code> <p>The processed results of each batch.</p> required <p>Returns:</p> Type Description <code>Dict[str, float]</code> <p>Dict[str, float]: The computed metrics. The keys are the names of</p> <code>Dict[str, float]</code> <p>the metrics, and the values are corresponding results.</p> Source code in <code>aigve/metrics/text_video_alignment/similarity_based/blipscore/blipsim.py</code> <pre><code>def compute_metrics(self, results: list) -&gt; Dict[str, float]:\n    \"\"\"Compute the metrics from processed results.\n\n    Args:\n        results (list): The processed results of each batch.\n\n    Returns:\n        Dict[str, float]: The computed metrics. The keys are the names of\n        the metrics, and the values are corresponding results.\n    \"\"\"\n    logger: MMLogger = MMLogger.get_current_instance()\n\n    blip_score_np = np.zeros(len(results))\n    for i, result in enumerate(results):\n        blip_score_np[i] = result['blip_sim_score']\n\n    blip_sim_mean = np.mean(blip_score_np) \n\n    print(\"Test results: blip similarity score={:.4f}\"\n          .format(blip_sim_mean))\n\n    return result\n</code></pre>"},{"location":"documentations/metrics/blipsim/#aigve.metrics.text_video_alignment.similarity_based.blipscore.BlipSimScore.process","title":"<code>process(data_batch, data_samples)</code>","text":"<p>BLIPSimScore process Process one batch of data samples and predictions. The processed results should be stored in <code>self.results</code>, which will be used to compute the metrics when all batches have been processed.</p> <p>Parameters:</p> Name Type Description Default <code>data_batch</code> <code>Sequence</code> <p>A batch of data from the dataloader.</p> required <code>data_samples</code> <code>Sequence</code> <p>A batch of data samples that contain annotations and predictions.</p> required Source code in <code>aigve/metrics/text_video_alignment/similarity_based/blipscore/blipsim.py</code> <pre><code>def process(self, data_batch: Sequence, data_samples: Sequence) -&gt; None:\n    \"\"\"BLIPSimScore process\n    Process one batch of data samples and predictions. The processed\n    results should be stored in ``self.results``, which will be used to\n    compute the metrics when all batches have been processed.\n\n    Args:\n        data_batch (Sequence): A batch of data from the dataloader.\n        data_samples (Sequence): A batch of data samples that\n            contain annotations and predictions.\n    \"\"\"\n\n    result = dict()\n\n    input_prompts, input_videos = data_samples  \n    bsz = len(input_prompts)\n\n    # Ensure prompt_input is a tensor\n    if isinstance(input_prompts, tuple):\n        input_prompts = list(input_prompts)\n\n    if isinstance(input_videos, tuple):\n        input_videos = list(input_videos)\n\n\n    # Initialize an empty tensor to store the concatenated features\n    blip_score_sum, blip_score_cnt = 0, 0\n    logit_scale = self.model.logit_scale.exp() if self.logit_scale else 1\n    with torch.no_grad():\n        for input_prompt, input_frames in zip(input_prompts, input_videos):\n            # If frame is a tuple, extract the tensor. Assume tensor is the first element.\n            # if isinstance(input_prompt_frame_pair, tuple):\n            #     input_prompt_frame_pair = input_prompt_frame_pair[0]\n\n            # for key, value in input_prompt_frame_pair.items():\n            #     if isinstance(value, list):\n            #         input_prompt_frame_pair[key] = value[0]\n\n            # input_prompt_frame_pair = input_prompt_frame_pair.to(\"cuda\")  # Add batch dimension and move the frame to the device\n            # blip_cosine_sim_score = self.model(**input_prompt_frame_pair, use_itm_head=False)[0].item()\n            # blip_scores.append(blip_cosine_sim_score)\n            input_prompt = input_prompt.to(self.device)\n            input_frames = input_frames.to(self.device)\n            blip_cosine_sim_score = self.model(input_ids=input_prompt, pixel_values=input_frames, use_itm_head=False)[0].mean().item()\n            blip_cosine_sim_score *= logit_scale\n            print('current blip cosine similarity score', blip_cosine_sim_score)\n            blip_score_sum += blip_cosine_sim_score\n            blip_score_cnt += 1\n\n    # Calculate the average BLIP score across all frames\n    blip_score_frames_avg = blip_score_sum/blip_score_cnt\n\n    result['blip_sim_score'] = blip_score_frames_avg\n\n    self.results.append(result)\n</code></pre>"},{"location":"documentations/metrics/clipsim/","title":"class CLIPSimScore","text":"<p>               Bases: <code>BaseMetric</code></p> <p>Initialize the <code>CLIPSimScore</code> evaluator.</p> <p>Parameters:</p> Name Type Description Default <code>processor_name</code> <code>str</code> <p>The name of the CLIP processor, which wraps a CLIP feature extractor and a CLIP tokenizer into this single procesor.                Defaults to <code>openai/clip-vit-base-patch32</code>.</p> <code>'openai/clip-vit-base-patch32'</code> <code>model_name</code> <code>str</code> <p>The name of the CLIP model. Defaults to <code>openai/clip-vit-base-patch32</code>.</p> <code>'openai/clip-vit-base-patch32'</code> <code>logit_scale</code> <code>bool</code> <p>Whether to calcualte the cosine similarity as logits. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <p>None</p> Source code in <code>aigve/metrics/text_video_alignment/similarity_based/clipscore/clipsim.py</code> <pre><code>@METRICS.register_module()\nclass CLIPSimScore(BaseMetric):\n    \"\"\" Initialize the ``CLIPSimScore`` evaluator.\n\n    Args:\n            processor_name (str): The name of the CLIP processor, which wraps a CLIP feature extractor and a CLIP tokenizer into this single procesor. \n                                  Defaults to ``openai/clip-vit-base-patch32``.\n            model_name (str): The name of the CLIP model. Defaults to ``openai/clip-vit-base-patch32``.\n            logit_scale (bool): Whether to calcualte the cosine similarity as logits. Defaults to False.\n\n    Returns:\n            None\n    \"\"\"\n    def __init__(self,\n                 processor_name: str = \"openai/clip-vit-base-patch32\",\n                 model_name: str = \"openai/clip-vit-base-patch32\",\n                 logit_scale: bool = False,\n                #  train_index: int = 4\n                 ) -&gt; None:\n        super().__init__()\n        self.processor_name = processor_name\n        self.model_name = model_name\n        self.logit_scale = logit_scale\n\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self.processor = AutoProcessor.from_pretrained(self.processor_name)\n        self.model = CLIPModel.from_pretrained(self.model_name).to(self.device)\n        self.model.eval()\n\n\n# def process(self, data_batch: dict, data_samples: Sequence[dict]) -&gt; None:\n    def process(self, data_batch: Sequence, data_samples: Sequence) -&gt; None:\n        \"\"\"CLIPSimScore process\n        Process one batch of data samples and predictions. The processed\n        results should be stored in ``self.results``, which will be used to\n        compute the metrics when all batches have been processed.\n\n        Args:\n            data_batch (Sequence): A batch of data from the dataloader.\n            data_samples (Sequence): A batch of data samples that\n                contain annotations and predictions.\n        \"\"\"\n\n        result = dict()\n\n        input_prompts, input_videos = data_samples\n        bsz = len(input_prompts)\n\n        # Ensure prompt_input is a tensor\n        if isinstance(input_prompts, tuple):\n            input_prompts = list(input_prompts)\n\n        if isinstance(input_videos, tuple):\n            input_videos = list(input_videos)\n\n        # Initialize an empty list to store each similarity score\n        clip_score_sum, clip_score_cnt = 0, 0\n        logit_scale = self.model.logit_scale.exp() if self.logit_scale else 1\n        with torch.no_grad():\n            for input_prompt, input_frames in zip(input_prompts, input_videos):\n                input_prompt = input_prompt.to(self.device)\n                text_feature = self.model.get_text_features(input_prompt) # [bsz, hid_dim]\n                text_feature = text_feature / torch.norm(text_feature, dim=-1, keepdim=True)\n\n                input_frames = input_frames.to(self.device)  # Add batch dimension and move the frame to the device\n                frame_feature = self.model.get_image_features(input_frames)\n                frame_feature = frame_feature / torch.norm(frame_feature, dim=-1, keepdim=True)\n\n                clip_score = logit_scale * (frame_feature @ text_feature.T).mean().item()\n                print('current clip similarity score', clip_score)\n                clip_score_sum += clip_score\n                clip_score_cnt += 1\n\n        # Calculate the average CLIP score across all frames\n        clip_score_videos_avg = clip_score_sum/clip_score_cnt\n\n        result['clip_sim_score'] = clip_score_videos_avg\n\n        self.results.append(result)\n\n\n    def compute_metrics(self, results: list) -&gt; Dict[str, float]:\n        \"\"\"Compute the metrics from processed results.\n\n        Args:\n            results (list): The processed results of each batch.\n\n        Returns:\n            Dict[str, float]: The computed metrics. The keys are the names of\n            the metrics, and the values are corresponding results.\n        \"\"\"\n        logger: MMLogger = MMLogger.get_current_instance()\n\n        clip_score_np = np.zeros(len(results))\n        for i, result in enumerate(results):\n            clip_score_np[i] = result['clip_sim_score']\n\n        clip_sim_mean = np.mean(clip_score_np) \n\n        print(\"Test results: clip similarity score={:.4f}\"\n              .format(clip_sim_mean))\n\n        return result\n</code></pre>"},{"location":"documentations/metrics/clipsim/#aigve.metrics.text_video_alignment.similarity_based.clipscore.CLIPSimScore.compute_metrics","title":"<code>compute_metrics(results)</code>","text":"<p>Compute the metrics from processed results.</p> <p>Parameters:</p> Name Type Description Default <code>results</code> <code>list</code> <p>The processed results of each batch.</p> required <p>Returns:</p> Type Description <code>Dict[str, float]</code> <p>Dict[str, float]: The computed metrics. The keys are the names of</p> <code>Dict[str, float]</code> <p>the metrics, and the values are corresponding results.</p> Source code in <code>aigve/metrics/text_video_alignment/similarity_based/clipscore/clipsim.py</code> <pre><code>def compute_metrics(self, results: list) -&gt; Dict[str, float]:\n    \"\"\"Compute the metrics from processed results.\n\n    Args:\n        results (list): The processed results of each batch.\n\n    Returns:\n        Dict[str, float]: The computed metrics. The keys are the names of\n        the metrics, and the values are corresponding results.\n    \"\"\"\n    logger: MMLogger = MMLogger.get_current_instance()\n\n    clip_score_np = np.zeros(len(results))\n    for i, result in enumerate(results):\n        clip_score_np[i] = result['clip_sim_score']\n\n    clip_sim_mean = np.mean(clip_score_np) \n\n    print(\"Test results: clip similarity score={:.4f}\"\n          .format(clip_sim_mean))\n\n    return result\n</code></pre>"},{"location":"documentations/metrics/clipsim/#aigve.metrics.text_video_alignment.similarity_based.clipscore.CLIPSimScore.process","title":"<code>process(data_batch, data_samples)</code>","text":"<p>CLIPSimScore process Process one batch of data samples and predictions. The processed results should be stored in <code>self.results</code>, which will be used to compute the metrics when all batches have been processed.</p> <p>Parameters:</p> Name Type Description Default <code>data_batch</code> <code>Sequence</code> <p>A batch of data from the dataloader.</p> required <code>data_samples</code> <code>Sequence</code> <p>A batch of data samples that contain annotations and predictions.</p> required Source code in <code>aigve/metrics/text_video_alignment/similarity_based/clipscore/clipsim.py</code> <pre><code>def process(self, data_batch: Sequence, data_samples: Sequence) -&gt; None:\n    \"\"\"CLIPSimScore process\n    Process one batch of data samples and predictions. The processed\n    results should be stored in ``self.results``, which will be used to\n    compute the metrics when all batches have been processed.\n\n    Args:\n        data_batch (Sequence): A batch of data from the dataloader.\n        data_samples (Sequence): A batch of data samples that\n            contain annotations and predictions.\n    \"\"\"\n\n    result = dict()\n\n    input_prompts, input_videos = data_samples\n    bsz = len(input_prompts)\n\n    # Ensure prompt_input is a tensor\n    if isinstance(input_prompts, tuple):\n        input_prompts = list(input_prompts)\n\n    if isinstance(input_videos, tuple):\n        input_videos = list(input_videos)\n\n    # Initialize an empty list to store each similarity score\n    clip_score_sum, clip_score_cnt = 0, 0\n    logit_scale = self.model.logit_scale.exp() if self.logit_scale else 1\n    with torch.no_grad():\n        for input_prompt, input_frames in zip(input_prompts, input_videos):\n            input_prompt = input_prompt.to(self.device)\n            text_feature = self.model.get_text_features(input_prompt) # [bsz, hid_dim]\n            text_feature = text_feature / torch.norm(text_feature, dim=-1, keepdim=True)\n\n            input_frames = input_frames.to(self.device)  # Add batch dimension and move the frame to the device\n            frame_feature = self.model.get_image_features(input_frames)\n            frame_feature = frame_feature / torch.norm(frame_feature, dim=-1, keepdim=True)\n\n            clip_score = logit_scale * (frame_feature @ text_feature.T).mean().item()\n            print('current clip similarity score', clip_score)\n            clip_score_sum += clip_score\n            clip_score_cnt += 1\n\n    # Calculate the average CLIP score across all frames\n    clip_score_videos_avg = clip_score_sum/clip_score_cnt\n\n    result['clip_sim_score'] = clip_score_videos_avg\n\n    self.results.append(result)\n</code></pre>"},{"location":"documentations/metrics/cliptemp/","title":"class CLIPTempScore","text":"<p>               Bases: <code>BaseMetric</code></p> <p>Initialize the <code>CLIPTempScore</code> evaluator.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>The name of the CLIP encoder model. Defaults to <code>openai/clip-vit-base-patch32</code>.</p> <code>'openai/clip-vit-base-patch32'</code> <code>logit_scale</code> <code>bool</code> <p>Whether to calcualte the cosine similarity as logits. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <p>None</p> Source code in <code>aigve/metrics/text_video_alignment/similarity_based/clipscore/cliptemp.py</code> <pre><code>@METRICS.register_module()\nclass CLIPTempScore(BaseMetric):\n    \"\"\" Initialize the ``CLIPTempScore`` evaluator.\n\n    Args:\n            model_name (str): The name of the CLIP encoder model. Defaults to ``openai/clip-vit-base-patch32``.\n            logit_scale (bool): Whether to calcualte the cosine similarity as logits. Defaults to False.\n\n    Returns:\n            None\n    \"\"\"\n    def __init__(self,\n                 model_name: str = \"openai/clip-vit-base-patch32\",\n                 logit_scale: bool = False,\n                #  train_index: int = 4\n                 ) -&gt; None:\n        super().__init__()\n        self.model_name = model_name\n        self.logit_scale = logit_scale\n\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self.model = CLIPModel.from_pretrained(self.model_name).to(self.device)\n        self.model.eval()\n\n\n# def process(self, data_batch: dict, data_samples: Sequence[dict]) -&gt; None:\n    def process(self, data_batch: Sequence, data_samples: Sequence) -&gt; None:\n        \"\"\"CLIPTempScore process\n        Process one batch of data samples and predictions. The processed\n        results should be stored in ``self.results``, which will be used to\n        compute the metrics when all batches have been processed.\n\n        Args:\n            data_batch (Sequence): A batch of data from the dataloader.\n            data_samples (Sequence): A batch of data samples that\n                contain annotations and predictions.\n        \"\"\"\n\n        result = dict()\n\n        input_videos = data_samples\n        # bsz = len(input_videos)\n\n\n        # Ensure prompt_input is a tensor        \n        if isinstance(input_videos, tuple):\n            input_videos = list(input_videos)\n\n        # Generate embeddings for each frame and concatenate the features\n        clip_temp_score_sum, clip_temp_score_cnt = 0, 0\n        logit_scale = self.model.logit_scale.exp() if self.logit_scale else 1\n        with torch.no_grad():  \n            for input_frames in input_videos: # Too many frames in a video, must split before CLIP embedding, limited by the memory\n                input_frames = input_frames.to(self.device)\n                frame_feature = self.model.get_image_features(input_frames)\n                frame_feature = frame_feature / torch.norm(frame_feature, dim=-1, keepdim=True)\n                # print(frame_feature.shape)\n\n                clip_temp_score_list = []\n                for i in range(frame_feature.shape[0]-1):\n                    clip_temp_score = logit_scale * frame_feature[i].unsqueeze(0) @ frame_feature[i+1].unsqueeze(0).T\n                    clip_temp_score = clip_temp_score.item()\n                    # print(clip_temp_score)\n                    clip_temp_score_list.append(clip_temp_score)\n                clip_temp_cur_avg_score = sum(clip_temp_score_list)/len(clip_temp_score_list)\n                clip_temp_score_sum += clip_temp_cur_avg_score\n                clip_temp_score_cnt += 1\n                print('current clip temp similarity score', clip_temp_cur_avg_score)\n\n        clip_temp_score_avg = clip_temp_score_sum/clip_temp_score_cnt\n\n        result['clip_temp_score'] = clip_temp_score_avg\n\n        self.results.append(result)\n\n\n    def compute_metrics(self, results: list) -&gt; Dict[str, float]:\n        \"\"\"Compute the metrics from processed results.\n\n        Args:\n            results (list): The processed results of each batch.\n\n        Returns:\n            Dict[str, float]: The computed metrics. The keys are the names of\n            the metrics, and the values are corresponding results.\n        \"\"\"\n        logger: MMLogger = MMLogger.get_current_instance()\n\n        clip_score_np = np.zeros(len(results))\n        for i, result in enumerate(results):\n            clip_score_np[i] = result['clip_temp_score']\n\n        clip_temp_mean = np.mean(clip_score_np) \n\n        print(\"Test results: clip temporal consistency score={:.4f}\"\n              .format(clip_temp_mean))\n\n        return result\n</code></pre>"},{"location":"documentations/metrics/cliptemp/#aigve.metrics.text_video_alignment.similarity_based.clipscore.CLIPTempScore.compute_metrics","title":"<code>compute_metrics(results)</code>","text":"<p>Compute the metrics from processed results.</p> <p>Parameters:</p> Name Type Description Default <code>results</code> <code>list</code> <p>The processed results of each batch.</p> required <p>Returns:</p> Type Description <code>Dict[str, float]</code> <p>Dict[str, float]: The computed metrics. The keys are the names of</p> <code>Dict[str, float]</code> <p>the metrics, and the values are corresponding results.</p> Source code in <code>aigve/metrics/text_video_alignment/similarity_based/clipscore/cliptemp.py</code> <pre><code>def compute_metrics(self, results: list) -&gt; Dict[str, float]:\n    \"\"\"Compute the metrics from processed results.\n\n    Args:\n        results (list): The processed results of each batch.\n\n    Returns:\n        Dict[str, float]: The computed metrics. The keys are the names of\n        the metrics, and the values are corresponding results.\n    \"\"\"\n    logger: MMLogger = MMLogger.get_current_instance()\n\n    clip_score_np = np.zeros(len(results))\n    for i, result in enumerate(results):\n        clip_score_np[i] = result['clip_temp_score']\n\n    clip_temp_mean = np.mean(clip_score_np) \n\n    print(\"Test results: clip temporal consistency score={:.4f}\"\n          .format(clip_temp_mean))\n\n    return result\n</code></pre>"},{"location":"documentations/metrics/cliptemp/#aigve.metrics.text_video_alignment.similarity_based.clipscore.CLIPTempScore.process","title":"<code>process(data_batch, data_samples)</code>","text":"<p>CLIPTempScore process Process one batch of data samples and predictions. The processed results should be stored in <code>self.results</code>, which will be used to compute the metrics when all batches have been processed.</p> <p>Parameters:</p> Name Type Description Default <code>data_batch</code> <code>Sequence</code> <p>A batch of data from the dataloader.</p> required <code>data_samples</code> <code>Sequence</code> <p>A batch of data samples that contain annotations and predictions.</p> required Source code in <code>aigve/metrics/text_video_alignment/similarity_based/clipscore/cliptemp.py</code> <pre><code>def process(self, data_batch: Sequence, data_samples: Sequence) -&gt; None:\n    \"\"\"CLIPTempScore process\n    Process one batch of data samples and predictions. The processed\n    results should be stored in ``self.results``, which will be used to\n    compute the metrics when all batches have been processed.\n\n    Args:\n        data_batch (Sequence): A batch of data from the dataloader.\n        data_samples (Sequence): A batch of data samples that\n            contain annotations and predictions.\n    \"\"\"\n\n    result = dict()\n\n    input_videos = data_samples\n    # bsz = len(input_videos)\n\n\n    # Ensure prompt_input is a tensor        \n    if isinstance(input_videos, tuple):\n        input_videos = list(input_videos)\n\n    # Generate embeddings for each frame and concatenate the features\n    clip_temp_score_sum, clip_temp_score_cnt = 0, 0\n    logit_scale = self.model.logit_scale.exp() if self.logit_scale else 1\n    with torch.no_grad():  \n        for input_frames in input_videos: # Too many frames in a video, must split before CLIP embedding, limited by the memory\n            input_frames = input_frames.to(self.device)\n            frame_feature = self.model.get_image_features(input_frames)\n            frame_feature = frame_feature / torch.norm(frame_feature, dim=-1, keepdim=True)\n            # print(frame_feature.shape)\n\n            clip_temp_score_list = []\n            for i in range(frame_feature.shape[0]-1):\n                clip_temp_score = logit_scale * frame_feature[i].unsqueeze(0) @ frame_feature[i+1].unsqueeze(0).T\n                clip_temp_score = clip_temp_score.item()\n                # print(clip_temp_score)\n                clip_temp_score_list.append(clip_temp_score)\n            clip_temp_cur_avg_score = sum(clip_temp_score_list)/len(clip_temp_score_list)\n            clip_temp_score_sum += clip_temp_cur_avg_score\n            clip_temp_score_cnt += 1\n            print('current clip temp similarity score', clip_temp_cur_avg_score)\n\n    clip_temp_score_avg = clip_temp_score_sum/clip_temp_score_cnt\n\n    result['clip_temp_score'] = clip_temp_score_avg\n\n    self.results.append(result)\n</code></pre>"},{"location":"documentations/metrics/dsg/","title":"class DSGScore","text":"<p>               Bases: <code>BaseMetric</code></p> <p>Initialize the <code>DSGScore</code> evaluator.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>The name of the VQA model used in the DSGScore evaluator. Defaults to <code>InstructBLIP</code>, you can also choose the \"MPLUG\" as the VQA model.</p> required <code>verbose</code> <code>bool</code> <p>Whether the intermediate output processes is required. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <p>None</p> Source code in <code>aigve/metrics/text_video_alignment/gpt_based/dsg/dsg_eval.py</code> <pre><code>@METRICS.register_module()\nclass DSGScore(BaseMetric):\n    \"\"\" Initialize the ``DSGScore`` evaluator.\n\n    Args:\n            model_name (str): The name of the VQA model used in the DSGScore evaluator. Defaults to ``InstructBLIP``, you can also choose the \"MPLUG\" as the VQA model.\n            verbose (bool): Whether the intermediate output processes is required. Defaults to False.\n\n    Returns:\n            None\n    \"\"\"\n    def __init__(self, \n                 vqa_model_name: str = \"InstructBLIP\",\n                 verbose: bool = False):\n        super().__init__()\n\n        self.submodel_path = 'metrics/text_video_alignment/gpt_based/dsg'\n        if not submodule_exists(self.submodel_path):\n            add_git_submodule(\n                repo_url='https://github.com/j-min/DSG.git', \n                submodule_path=self.submodel_path\n            )     \n        from .DSG.dsg.vqa_utils import MPLUG, InstructBLIP\n\n        self.vqa_model_name = vqa_model_name\n        assert self.vqa_model_name in [\"InstructBLIP\", \"MPLUG\"]\n        if self.vqa_model_name == 'InstructBLIP':\n            self.vqa_model = InstructBLIP()\n        else:\n            self.vqa_model = MPLUG()\n\n        self.verbose = verbose\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\n    def evaluate_image_dsg(self, qid_list, frame_index, frame):\n        \"\"\" Evaluate a generated image with DSG evaluator; this is the intermediate process of the ``process`` function. \n\n        Args:\n                qid_list (List[str]): The list of DSG parse question generation results.\n                frame_index (int): The index number of the currently evaluated frame.\n                frame (List[List[float]): The current evaluated frame.\n\n        Returns:\n                None\n        \"\"\"\n        if self.verbose:\n            print(\"#\"*50)\n            print(\"2) Answer questions given the generated image, with VQA\")\n            print(\"#\"*50)\n\n        # 2) answer questions with the generated image\n        qid2answer = {}\n        qid2scores = {}\n\n        qid2tuple, qid2dependency, qid2question = qid_list\n        for id, question in qid2question.items():\n            answer = self.vqa_model.vqa(image=frame, question=question)\n            print(answer)\n            qid2answer[id] = answer\n            qid2scores[id] = float('yes' in answer)\n\n        average_score_without_dep = sum(qid2scores.values()) / len(qid2scores)\n        print(average_score_without_dep, qid2answer, qid2scores)\n\n        if self.verbose:\n            print(\"#\"*50)\n            print(\"3) Zero-out scores from invalid questions\")\n            print(\"#\"*50)\n\n        # 3) zero-out scores from invalid questions \n        qid2validity = {}\n        qid2scores_after_filtering = deepcopy(qid2scores)\n\n        # print('qid2scores', qid2scores)\n        # print('qid2dependency', qid2dependency)\n        for id, parent_ids in qid2dependency.items():\n            # zero-out scores if parent questions are answered 'no'\n            any_parent_answered_no = False\n            for parent_id in parent_ids:\n                parent_id = list(parent_id)[0]\n                if parent_id == 0:\n                    continue\n                if qid2scores[parent_id] == 0:\n                    any_parent_answered_no = True\n                    break\n            if any_parent_answered_no:\n                qid2scores_after_filtering[id] = 0.0\n                qid2validity[id] = False\n            else:\n                qid2validity[id] = True\n\n        if self.verbose:\n            print(\"Per-quesiton eval results (after using dependency)\")\n            for id in qid2question:\n                print(\"ID\", id)\n                print(\"question\", qid2question[id])\n                print(\"answer\", qid2answer[id])\n                print(\"validity\", qid2validity[id])\n                print(\"score (before filtering)\", qid2scores[id])\n                print(\"score (after filtering)\", qid2scores_after_filtering[id])\n                print()\n\n        if self.verbose:\n            print(\"#\"*50)\n            print(\"4) Calculate the final score by averaging\")\n            print(\"#\"*50)\n\n        average_score_with_dep = sum(qid2scores_after_filtering.values()) / len(qid2scores)\n\n        return {\n            'frame_index': frame_index,\n            'qid2tuple': qid2tuple,\n            'qid2dependency': qid2dependency,\n            'qid2question': qid2question,\n            'qid2answer': qid2answer,\n            'qid2scores': qid2scores,\n            'qid2validity': qid2validity,\n            'average_score_with_dependency': average_score_with_dep,\n            'average_score_without_dependency': average_score_without_dep\n        }\n\n\n    def process(self, data_batch: Sequence, data_samples: Sequence) -&gt; None:\n        \"\"\"DSGScore process\n\n        Process one batch of data samples and predictions. The processed\n        results should be stored in ``self.results``, which will be used to\n        compute the metrics when all batches have been processed.\n\n        Args:\n            data_batch (Sequence): A batch of data from the dataloader.\n            data_samples (Sequence): A batch of data samples that\n                contain annotations and predictions.\n        \"\"\"\n\n        result = dict()\n\n        input_qid_lists, input_videos = data_samples\n        bsz = len(input_qid_lists)\n        # print('input_qid_lists: ', input_qid_lists)\n\n        # Ensure prompt_input is a tensor\n        if isinstance(input_qid_lists, tuple):\n            input_qid_lists = list(input_qid_lists)\n\n        if isinstance(input_videos, tuple):\n            input_videos = list(input_videos)\n\n        average_dep_score_list, average_wo_dep_score_list = [], []\n        for input_qid_list, input_video in zip([input_qid_lists], input_videos):\n            evaluate_dict_list = []\n            dep_score, wo_dep_score = [], []\n            for index, frame in enumerate(input_video):\n                # print('input_qid_list: ', input_qid_list)\n                evaluate_dict = self.evaluate_image_dsg(qid_list=input_qid_list, \n                                                        frame_index=index, \n                                                        frame=frame)\n                evaluate_dict_list.append(evaluate_dict)\n                frame_average_score_with_dependency = evaluate_dict['average_score_with_dependency']\n                dep_score.append(frame_average_score_with_dependency)\n                frame_average_score_without_dependency = evaluate_dict['average_score_without_dependency']\n                wo_dep_score.append(frame_average_score_without_dependency)\n            avg_dep_score, avg_wo_dep_score = sum(dep_score)/len(dep_score), sum(wo_dep_score)/len(dep_score)\n            average_dep_score_list.append(avg_dep_score)\n            average_wo_dep_score_list.append(avg_wo_dep_score)\n\n\n        result['average_dep_dgs_score'] = sum(average_dep_score_list)/len(average_dep_score_list)\n        result['average_wo_dep_dgs_score'] = sum(average_wo_dep_score_list)/len(average_wo_dep_score_list)\n\n        self.results.append(result)\n\n\n    def compute_metrics(self, results: list) -&gt; Dict[str, float]:\n        \"\"\"Compute the metrics from processed results.\n\n        Args:\n            results (list): The processed results of each batch.\n\n        Returns:\n            Dict[str, float]: The computed metrics. The keys are the names of\n            the metrics, and the values are corresponding results.\n        \"\"\"\n        logger: MMLogger = MMLogger.get_current_instance()\n\n        dep_dsg_score_np = np.zeros(len(results))\n        wo_dep_dsg_score_np = np.zeros(len(results))\n        for i, result in enumerate(results):\n            dep_dsg_score_np[i] = result['average_dep_dgs_score']\n            wo_dep_dsg_score_np[i] = result['average_wo_dep_dgs_score']\n\n        dep_dsg_score_np_mean = np.mean(dep_dsg_score_np) \n        wo_dep_dsg_score_np_mean = np.mean(wo_dep_dsg_score_np)\n\n        print(\"Test results: dsg score with dependency={:.4f}\"\n              .format(dep_dsg_score_np_mean))\n        print(\"Test results: dsg score without dependency={:.4f}\"\n              .format(wo_dep_dsg_score_np_mean))\n\n        return result\n</code></pre>"},{"location":"documentations/metrics/dsg/#aigve.metrics.text_video_alignment.gpt_based.dsg.DSGScore.compute_metrics","title":"<code>compute_metrics(results)</code>","text":"<p>Compute the metrics from processed results.</p> <p>Parameters:</p> Name Type Description Default <code>results</code> <code>list</code> <p>The processed results of each batch.</p> required <p>Returns:</p> Type Description <code>Dict[str, float]</code> <p>Dict[str, float]: The computed metrics. The keys are the names of</p> <code>Dict[str, float]</code> <p>the metrics, and the values are corresponding results.</p> Source code in <code>aigve/metrics/text_video_alignment/gpt_based/dsg/dsg_eval.py</code> <pre><code>def compute_metrics(self, results: list) -&gt; Dict[str, float]:\n    \"\"\"Compute the metrics from processed results.\n\n    Args:\n        results (list): The processed results of each batch.\n\n    Returns:\n        Dict[str, float]: The computed metrics. The keys are the names of\n        the metrics, and the values are corresponding results.\n    \"\"\"\n    logger: MMLogger = MMLogger.get_current_instance()\n\n    dep_dsg_score_np = np.zeros(len(results))\n    wo_dep_dsg_score_np = np.zeros(len(results))\n    for i, result in enumerate(results):\n        dep_dsg_score_np[i] = result['average_dep_dgs_score']\n        wo_dep_dsg_score_np[i] = result['average_wo_dep_dgs_score']\n\n    dep_dsg_score_np_mean = np.mean(dep_dsg_score_np) \n    wo_dep_dsg_score_np_mean = np.mean(wo_dep_dsg_score_np)\n\n    print(\"Test results: dsg score with dependency={:.4f}\"\n          .format(dep_dsg_score_np_mean))\n    print(\"Test results: dsg score without dependency={:.4f}\"\n          .format(wo_dep_dsg_score_np_mean))\n\n    return result\n</code></pre>"},{"location":"documentations/metrics/dsg/#aigve.metrics.text_video_alignment.gpt_based.dsg.DSGScore.evaluate_image_dsg","title":"<code>evaluate_image_dsg(qid_list, frame_index, frame)</code>","text":"<p>Evaluate a generated image with DSG evaluator; this is the intermediate process of the <code>process</code> function. </p> <p>Parameters:</p> Name Type Description Default <code>qid_list</code> <code>List[str]</code> <p>The list of DSG parse question generation results.</p> required <code>frame_index</code> <code>int</code> <p>The index number of the currently evaluated frame.</p> required <code>frame</code> <code>List[List[float]</code> <p>The current evaluated frame.</p> required <p>Returns:</p> Type Description <p>None</p> Source code in <code>aigve/metrics/text_video_alignment/gpt_based/dsg/dsg_eval.py</code> <pre><code>def evaluate_image_dsg(self, qid_list, frame_index, frame):\n    \"\"\" Evaluate a generated image with DSG evaluator; this is the intermediate process of the ``process`` function. \n\n    Args:\n            qid_list (List[str]): The list of DSG parse question generation results.\n            frame_index (int): The index number of the currently evaluated frame.\n            frame (List[List[float]): The current evaluated frame.\n\n    Returns:\n            None\n    \"\"\"\n    if self.verbose:\n        print(\"#\"*50)\n        print(\"2) Answer questions given the generated image, with VQA\")\n        print(\"#\"*50)\n\n    # 2) answer questions with the generated image\n    qid2answer = {}\n    qid2scores = {}\n\n    qid2tuple, qid2dependency, qid2question = qid_list\n    for id, question in qid2question.items():\n        answer = self.vqa_model.vqa(image=frame, question=question)\n        print(answer)\n        qid2answer[id] = answer\n        qid2scores[id] = float('yes' in answer)\n\n    average_score_without_dep = sum(qid2scores.values()) / len(qid2scores)\n    print(average_score_without_dep, qid2answer, qid2scores)\n\n    if self.verbose:\n        print(\"#\"*50)\n        print(\"3) Zero-out scores from invalid questions\")\n        print(\"#\"*50)\n\n    # 3) zero-out scores from invalid questions \n    qid2validity = {}\n    qid2scores_after_filtering = deepcopy(qid2scores)\n\n    # print('qid2scores', qid2scores)\n    # print('qid2dependency', qid2dependency)\n    for id, parent_ids in qid2dependency.items():\n        # zero-out scores if parent questions are answered 'no'\n        any_parent_answered_no = False\n        for parent_id in parent_ids:\n            parent_id = list(parent_id)[0]\n            if parent_id == 0:\n                continue\n            if qid2scores[parent_id] == 0:\n                any_parent_answered_no = True\n                break\n        if any_parent_answered_no:\n            qid2scores_after_filtering[id] = 0.0\n            qid2validity[id] = False\n        else:\n            qid2validity[id] = True\n\n    if self.verbose:\n        print(\"Per-quesiton eval results (after using dependency)\")\n        for id in qid2question:\n            print(\"ID\", id)\n            print(\"question\", qid2question[id])\n            print(\"answer\", qid2answer[id])\n            print(\"validity\", qid2validity[id])\n            print(\"score (before filtering)\", qid2scores[id])\n            print(\"score (after filtering)\", qid2scores_after_filtering[id])\n            print()\n\n    if self.verbose:\n        print(\"#\"*50)\n        print(\"4) Calculate the final score by averaging\")\n        print(\"#\"*50)\n\n    average_score_with_dep = sum(qid2scores_after_filtering.values()) / len(qid2scores)\n\n    return {\n        'frame_index': frame_index,\n        'qid2tuple': qid2tuple,\n        'qid2dependency': qid2dependency,\n        'qid2question': qid2question,\n        'qid2answer': qid2answer,\n        'qid2scores': qid2scores,\n        'qid2validity': qid2validity,\n        'average_score_with_dependency': average_score_with_dep,\n        'average_score_without_dependency': average_score_without_dep\n    }\n</code></pre>"},{"location":"documentations/metrics/dsg/#aigve.metrics.text_video_alignment.gpt_based.dsg.DSGScore.process","title":"<code>process(data_batch, data_samples)</code>","text":"<p>DSGScore process</p> <p>Process one batch of data samples and predictions. The processed results should be stored in <code>self.results</code>, which will be used to compute the metrics when all batches have been processed.</p> <p>Parameters:</p> Name Type Description Default <code>data_batch</code> <code>Sequence</code> <p>A batch of data from the dataloader.</p> required <code>data_samples</code> <code>Sequence</code> <p>A batch of data samples that contain annotations and predictions.</p> required Source code in <code>aigve/metrics/text_video_alignment/gpt_based/dsg/dsg_eval.py</code> <pre><code>def process(self, data_batch: Sequence, data_samples: Sequence) -&gt; None:\n    \"\"\"DSGScore process\n\n    Process one batch of data samples and predictions. The processed\n    results should be stored in ``self.results``, which will be used to\n    compute the metrics when all batches have been processed.\n\n    Args:\n        data_batch (Sequence): A batch of data from the dataloader.\n        data_samples (Sequence): A batch of data samples that\n            contain annotations and predictions.\n    \"\"\"\n\n    result = dict()\n\n    input_qid_lists, input_videos = data_samples\n    bsz = len(input_qid_lists)\n    # print('input_qid_lists: ', input_qid_lists)\n\n    # Ensure prompt_input is a tensor\n    if isinstance(input_qid_lists, tuple):\n        input_qid_lists = list(input_qid_lists)\n\n    if isinstance(input_videos, tuple):\n        input_videos = list(input_videos)\n\n    average_dep_score_list, average_wo_dep_score_list = [], []\n    for input_qid_list, input_video in zip([input_qid_lists], input_videos):\n        evaluate_dict_list = []\n        dep_score, wo_dep_score = [], []\n        for index, frame in enumerate(input_video):\n            # print('input_qid_list: ', input_qid_list)\n            evaluate_dict = self.evaluate_image_dsg(qid_list=input_qid_list, \n                                                    frame_index=index, \n                                                    frame=frame)\n            evaluate_dict_list.append(evaluate_dict)\n            frame_average_score_with_dependency = evaluate_dict['average_score_with_dependency']\n            dep_score.append(frame_average_score_with_dependency)\n            frame_average_score_without_dependency = evaluate_dict['average_score_without_dependency']\n            wo_dep_score.append(frame_average_score_without_dependency)\n        avg_dep_score, avg_wo_dep_score = sum(dep_score)/len(dep_score), sum(wo_dep_score)/len(dep_score)\n        average_dep_score_list.append(avg_dep_score)\n        average_wo_dep_score_list.append(avg_wo_dep_score)\n\n\n    result['average_dep_dgs_score'] = sum(average_dep_score_list)/len(average_dep_score_list)\n    result['average_wo_dep_dgs_score'] = sum(average_wo_dep_score_list)/len(average_wo_dep_score_list)\n\n    self.results.append(result)\n</code></pre>"},{"location":"documentations/metrics/fid/","title":"class FID","text":"<p>               Bases: <code>BaseModel</code></p> Source code in <code>aigve/metrics/video_quality_assessment/distribution_based/fid.py</code> <pre><code>@MODELS.register_module()\n@METRICS.register_module()\nclass FIDScore(BaseModel):\n\n    def __init__(self, model_name='inception_v3', input_shape=(299, 299, 3), pooling='avg'):\n        super().__init__()\n        if model_name == 'inception_v3':\n            self.model = InceptionV3(include_top=False, pooling=pooling, input_shape=input_shape)\n        else:\n            raise ValueError(\"Only 'inception_v3' is currently supported.\")\n\n    def preprocess_images(self, images):\n        \"\"\"\n        Preprocess the images for the InceptionV3 model.\n\n        Parameters:\n        images (numpy array): Input images.\n\n        Returns:\n        numpy array: Preprocessed images.\n        \"\"\"\n        return preprocess_input(images)\n\n    def calculate_statistics(self, images):\n        \"\"\"\n        Calculate the feature statistics (mean and covariance) for a set of images.\n\n        Parameters:\n        images (numpy array): Preprocessed images.\n\n        Returns:\n        tuple: Mean and covariance of the features.\n        \"\"\"\n        features = self.model.predict(images)\n        mu = features.mean(axis=0)\n        sigma = np.cov(features, rowvar=False)\n        return mu, sigma\n\n    def process(self, data_batch: dict, data_samples: Sequence[dict]) -&gt; None:\n        \"\"\"\n        Process one batch of data samples and predictions.\n\n        Args:\n            data_batch (dict): A batch of data from the dataloader.\n            data_samples (Sequence[dict]): A batch of data samples that\n                contain annotations and predictions.\n        \"\"\"\n        result = dict()\n        images1, images2 = data_samples\n\n        # Calculate FID score\n        fid_score = self.calculate_fid(images1, images2)\n        result['fid_score'] = fid_score\n\n        self.results.append(result)\n\n    def compute_metrics(self, results: list) -&gt; Dict[str, float]:\n        \"\"\"\n        Compute the metrics from processed results.\n\n        Args:\n            results (list): The processed results of each batch.\n\n        Returns:\n            Dict[str, float]: The computed metrics.\n        \"\"\"\n        logger: MMLogger = MMLogger.get_current_instance()\n\n        fid_scores = np.zeros(len(results))\n        for i, result in enumerate(results):\n            fid_scores[i] = result['fid_score']\n\n        mean_fid = np.mean(fid_scores)\n\n        print(\"Test results: FID Score={:.4f}\".format(mean_fid))\n\n        return {'fid': mean_fid}\n\n    def calculate_fid(self, images1, images2):\n        \"\"\"\n        Calculate the FID score between two sets of images.\n\n        Parameters:\n        images1 (numpy array): First set of images.\n        images2 (numpy array): Second set of images.\n\n        Returns:\n        float: The FID score.\n        \"\"\"\n        # Preprocess images\n        images1 = self.preprocess_images(images1)\n        images2 = self.preprocess_images(images2)\n\n        # Calculate statistics\n        mu1, sigma1 = self.calculate_statistics(images1)\n        mu2, sigma2 = self.calculate_statistics(images2)\n\n        # Compute FID score\n        ssdiff = np.sum((mu1 - mu2) ** 2.0)\n        covmean = sqrtm(sigma1.dot(sigma2))\n\n        # Check and correct for imaginary numbers\n        if np.iscomplexobj(covmean):\n            covmean = covmean.real\n\n        fid = ssdiff + np.trace(sigma1 + sigma2 - 2.0 * covmean)\n        return fid\n</code></pre>"},{"location":"documentations/metrics/fid/#aigve.metrics.video_quality_assessment.distribution_based.fid.FIDScore.calculate_fid","title":"<code>calculate_fid(images1, images2)</code>","text":"<p>Calculate the FID score between two sets of images.</p> <p>Parameters: images1 (numpy array): First set of images. images2 (numpy array): Second set of images.</p> <p>Returns: float: The FID score.</p> Source code in <code>aigve/metrics/video_quality_assessment/distribution_based/fid.py</code> <pre><code>def calculate_fid(self, images1, images2):\n    \"\"\"\n    Calculate the FID score between two sets of images.\n\n    Parameters:\n    images1 (numpy array): First set of images.\n    images2 (numpy array): Second set of images.\n\n    Returns:\n    float: The FID score.\n    \"\"\"\n    # Preprocess images\n    images1 = self.preprocess_images(images1)\n    images2 = self.preprocess_images(images2)\n\n    # Calculate statistics\n    mu1, sigma1 = self.calculate_statistics(images1)\n    mu2, sigma2 = self.calculate_statistics(images2)\n\n    # Compute FID score\n    ssdiff = np.sum((mu1 - mu2) ** 2.0)\n    covmean = sqrtm(sigma1.dot(sigma2))\n\n    # Check and correct for imaginary numbers\n    if np.iscomplexobj(covmean):\n        covmean = covmean.real\n\n    fid = ssdiff + np.trace(sigma1 + sigma2 - 2.0 * covmean)\n    return fid\n</code></pre>"},{"location":"documentations/metrics/fid/#aigve.metrics.video_quality_assessment.distribution_based.fid.FIDScore.calculate_statistics","title":"<code>calculate_statistics(images)</code>","text":"<p>Calculate the feature statistics (mean and covariance) for a set of images.</p> <p>Parameters: images (numpy array): Preprocessed images.</p> <p>Returns: tuple: Mean and covariance of the features.</p> Source code in <code>aigve/metrics/video_quality_assessment/distribution_based/fid.py</code> <pre><code>def calculate_statistics(self, images):\n    \"\"\"\n    Calculate the feature statistics (mean and covariance) for a set of images.\n\n    Parameters:\n    images (numpy array): Preprocessed images.\n\n    Returns:\n    tuple: Mean and covariance of the features.\n    \"\"\"\n    features = self.model.predict(images)\n    mu = features.mean(axis=0)\n    sigma = np.cov(features, rowvar=False)\n    return mu, sigma\n</code></pre>"},{"location":"documentations/metrics/fid/#aigve.metrics.video_quality_assessment.distribution_based.fid.FIDScore.compute_metrics","title":"<code>compute_metrics(results)</code>","text":"<p>Compute the metrics from processed results.</p> <p>Parameters:</p> Name Type Description Default <code>results</code> <code>list</code> <p>The processed results of each batch.</p> required <p>Returns:</p> Type Description <code>Dict[str, float]</code> <p>Dict[str, float]: The computed metrics.</p> Source code in <code>aigve/metrics/video_quality_assessment/distribution_based/fid.py</code> <pre><code>def compute_metrics(self, results: list) -&gt; Dict[str, float]:\n    \"\"\"\n    Compute the metrics from processed results.\n\n    Args:\n        results (list): The processed results of each batch.\n\n    Returns:\n        Dict[str, float]: The computed metrics.\n    \"\"\"\n    logger: MMLogger = MMLogger.get_current_instance()\n\n    fid_scores = np.zeros(len(results))\n    for i, result in enumerate(results):\n        fid_scores[i] = result['fid_score']\n\n    mean_fid = np.mean(fid_scores)\n\n    print(\"Test results: FID Score={:.4f}\".format(mean_fid))\n\n    return {'fid': mean_fid}\n</code></pre>"},{"location":"documentations/metrics/fid/#aigve.metrics.video_quality_assessment.distribution_based.fid.FIDScore.preprocess_images","title":"<code>preprocess_images(images)</code>","text":"<p>Preprocess the images for the InceptionV3 model.</p> <p>Parameters: images (numpy array): Input images.</p> <p>Returns: numpy array: Preprocessed images.</p> Source code in <code>aigve/metrics/video_quality_assessment/distribution_based/fid.py</code> <pre><code>def preprocess_images(self, images):\n    \"\"\"\n    Preprocess the images for the InceptionV3 model.\n\n    Parameters:\n    images (numpy array): Input images.\n\n    Returns:\n    numpy array: Preprocessed images.\n    \"\"\"\n    return preprocess_input(images)\n</code></pre>"},{"location":"documentations/metrics/fid/#aigve.metrics.video_quality_assessment.distribution_based.fid.FIDScore.process","title":"<code>process(data_batch, data_samples)</code>","text":"<p>Process one batch of data samples and predictions.</p> <p>Parameters:</p> Name Type Description Default <code>data_batch</code> <code>dict</code> <p>A batch of data from the dataloader.</p> required <code>data_samples</code> <code>Sequence[dict]</code> <p>A batch of data samples that contain annotations and predictions.</p> required Source code in <code>aigve/metrics/video_quality_assessment/distribution_based/fid.py</code> <pre><code>def process(self, data_batch: dict, data_samples: Sequence[dict]) -&gt; None:\n    \"\"\"\n    Process one batch of data samples and predictions.\n\n    Args:\n        data_batch (dict): A batch of data from the dataloader.\n        data_samples (Sequence[dict]): A batch of data samples that\n            contain annotations and predictions.\n    \"\"\"\n    result = dict()\n    images1, images2 = data_samples\n\n    # Calculate FID score\n    fid_score = self.calculate_fid(images1, images2)\n    result['fid_score'] = fid_score\n\n    self.results.append(result)\n</code></pre>"},{"location":"documentations/metrics/fvd/","title":"class FVD","text":"<p>               Bases: <code>BaseModel</code></p> Source code in <code>aigve/metrics/video_quality_assessment/distribution_based/fvd.py</code> <pre><code>@MODELS.register_module()\n@METRICS.register_module()\nclass FVDScore(BaseModel):\n    def __init__(self, model_path, feature_layer=-2):\n        super().__init__()\n        self.i3d_model = self.load_i3d_model(model_path, feature_layer)\n        self.results = []\n\n    @staticmethod\n    def load_i3d_model(model_path, feature_layer):\n        i3d_model = tf.keras.models.load_model(model_path)\n        feature_model = Model(inputs=i3d_model.input, outputs=i3d_model.layers[feature_layer].output)\n        return feature_model\n\n    def preprocess_videos(self, videos):\n        return preprocess_input(videos)\n\n    def calculate_statistics(self, videos):\n        features = self.i3d_model.predict(videos)\n        mu = features.mean(axis=0)\n        sigma = np.cov(features, rowvar=False)\n        return mu, sigma\n\n    def process(self, data_batch: dict, data_samples: Sequence[dict]) -&gt; None:\n        result = dict()\n        videos1, videos2 = data_samples\n\n        # Calculate FVD score\n        fvd_score = self.calculate_fvd(videos1, videos2)\n        result['fvd_score'] = fvd_score\n\n        self.results.append(result)\n\n    def compute_metrics(self, results: list) -&gt; Dict[str, float]:\n        logger: MMLogger = MMLogger.get_current_instance()\n\n        fvd_scores = np.zeros(len(results))\n        for i, result in enumerate(results):\n            fvd_scores[i] = result['fvd_score']\n\n        mean_fvd = np.mean(fvd_scores)\n\n        print(\"Test results: FVD Score={:.4f}\".format(mean_fvd))\n\n        return {'fvd': mean_fvd}\n\n    def calculate_fvd(self, videos1, videos2):\n        # Preprocess videos\n        videos1 = self.preprocess_videos(videos1)\n        videos2 = self.preprocess_videos(videos2)\n\n        # Calculate statistics\n        mu1, sigma1 = self.calculate_statistics(videos1)\n        mu2, sigma2 = self.calculate_statistics(videos2)\n\n        # Compute FVD score\n        ssdiff = np.sum((mu1 - mu2) ** 2.0)\n        covmean = sqrtm(sigma1.dot(sigma2))\n\n        # Check and correct for imaginary numbers\n        if np.iscomplexobj(covmean):\n            covmean = covmean.real\n\n        fvd = ssdiff + np.trace(sigma1 + sigma2 - 2.0 * covmean)\n        return fvd\n</code></pre>"},{"location":"documentations/metrics/gstvqa/","title":"class GSTVQA","text":"<p>               Bases: <code>BaseMetric</code></p> <p>GstVQA metric modified for the toy dataset. (Supporting 2944-dim features).</p> Source code in <code>aigve/metrics/video_quality_assessment/nn_based/gstvqa/gstvqa_metric.py</code> <pre><code>@METRICS.register_module()\nclass GstVqa(BaseMetric):\n    \"\"\"GstVQA metric modified for the toy dataset. (Supporting 2944-dim features).\"\"\"\n\n    def __init__(self, model_path: str):\n        super(GstVqa, self).__init__()\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self.submodel_path = os.path.join(os.getcwd(), 'metrics/video_quality_assessment/nn_based/gstvqa')\n        if not submodule_exists(self.submodel_path):\n            add_git_submodule(\n                repo_url='https://github.com/Baoliang93/GSTVQA.git', \n                submodule_path=self.submodel_path\n            )\n        from .GSTVQA.TCSVT_Release.GVQA_Release.GVQA_Cross.cross_test import GSTVQA as GSTVQA_model\n        self.model = GSTVQA_model().to(self.device)\n        self.model.load_state_dict(torch.load(model_path, map_location=self.device))\n        self.model.eval()\n        # self.criterion = nn.L1Loss().to(self.device)\n\n    def compute_stat_features(self, features: torch.Tensor, num_valid_frames: int) -&gt; Tuple[torch.Tensor]:\n        \"\"\"Compute statistical features mean_var, std_var, mean_mean, std_mean from extracted deep features.\n\n        Args:\n            features (torch.Tensor): Tensor of shape [T, 2944].\n            num_valid_frames (int): Number of valid frames before padding.\n\n        Returns:\n            Tuple[torch.Tensor]: (mean_var, std_var, mean_mean, std_mean), each of shape [1472].\n        \"\"\"\n        # Ignore padded frames\n        features = features[:num_valid_frames]  # Shape: [num_valid_frames, feature_dim]: [10, 1472]\n\n        if num_valid_frames == 0:  # Edge case: all frames were padded\n            return (\n                torch.zeros(1472, device=self.device),\n                torch.zeros(1472, device=self.device),\n                torch.zeros(1472, device=self.device),\n                torch.zeros(1472, device=self.device),\n            )\n\n        # Split into mean and std components\n        mean_features = features[:, :1472]  # First 1472 features are mean-based\n        std_features = features[:, 1472:]   # Last 1472 features are std-based\n\n        # Compute per-feature statistics over frames\n        mean_mean = mean_features.mean(dim=0)  # Shape: [1472]\n        std_mean = std_features.mean(dim=0)    # Shape: [1472]\n        mean_var = mean_features.var(dim=0, unbiased=False)  # Shape: [1472]\n        std_var = std_features.var(dim=0, unbiased=False)    # Shape: [1472]\n\n        return mean_var, std_var, mean_mean, std_mean\n\n    def process(self, data_batch: Sequence, data_samples: Sequence) -&gt; None:\n        \"\"\"\n        Process a batch of extracted deep features for GSTVQA evaluation and store results in a JSON file.\n\n        Args:\n            data_batch (SequencTuplee): A batch of data from the dataloader (not used here).\n            data_samples (List[ [torch.Tensor], Tuple[int], Tuple[str] ]): \n                A list containing three tuples:\n                - A tuple of `deep_features`: Each item is a Tensor of shape [T, 2944]. \n                - A tuple of `num_frames`: Each item is an integer representing the number of valid frames.\n                - A tuple of `video_name`: Each item is a string representing the file name for the video.\n                The len of these three tuples are the batch size.\n        \"\"\"\n        # data_samples an example: [\n        #     (tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n        #              [0., 0., 0.,  ..., 0., 0., 0.],\n        #              ...\n        #              [0., 0., 0.,  ..., 0., 0., 0.]]), \n        #      tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n        #              [0., 0., 0.,  ..., 0., 0., 0.],\n        #              ...\n        #              [0., 0., 0.,  ..., 0., 0., 0.]])), \n        #     (10, 10)\n        # ]\n        results = []\n        deep_features_tuple, num_frames_tuple, video_name_tuple = data_samples\n        with torch.no_grad():\n            for deep_features, num_valid_frames, video_name in zip(deep_features_tuple, num_frames_tuple, video_name_tuple):\n                if not isinstance(deep_features, torch.Tensor) or not isinstance(num_valid_frames, int):\n                    raise TypeError(\"Expected deep_features to be a torch.Tensor and num_valid_frames to be an int.\")\n\n                if num_valid_frames == 0:  # Edge case: No valid frames\n                    results.append({\"video_name\": 'N/A', \"GSTVQA_Score\": 0.0})\n                    continue\n\n                # Remove padded features\n                features = deep_features[:num_valid_frames].to(self.device)\n\n                # Compute statistical features only on valid frames\n                mean_var, std_var, mean_mean, std_mean = self.compute_stat_features(features, num_valid_frames)\n                mean_var, std_var, mean_mean, std_mean = (\n                    mean_var.to(self.device),\n                    std_var.to(self.device),\n                    mean_mean.to(self.device),\n                    std_mean.to(self.device),\n                )\n\n                # Length tensor indicating the number of valid frames\n                length = torch.tensor([num_valid_frames]).to(self.device)\n                # print('features(input) shape', features.unsqueeze(1).shape) # torch.Size([10, 1, 1472])\n                # print('input_length shape', length.shape) # torch.Size([1])\n                # print('input_length', length) # torch.Size([1])\n                # print('mean_mean shape', mean_mean.shape) # torch.Size([1472])\n                # print('std_mean shape', std_mean.shape) # torch.Size([1472])\n                # print('mean_var shape', mean_var.shape) # torch.Size([1472])\n                # print('std_var shape', std_var.shape) # torch.Size([1472])\n\n                # Run GSTVQA model\n                outputs = self.model(features.unsqueeze(1), length, mean_var, std_var, mean_mean, std_mean)\n                score = outputs.item()\n                results.append({\"video_name\": video_name, \"GSTVQA_Score\": score})\n                # print(f\"Processed score {score:.4f} for {video_name}\")\n\n        self.results.extend(results)\n\n\n    def compute_metrics(self, results: list) -&gt; Dict[str, float]:\n        \"\"\"Compute final GSTVQA-based metrics.\"\"\"\n        scores = np.array([res['GSTVQA_Score'] for res in self.results])\n        mean_score = np.mean(scores)\n        print(f\"GSTVQA mean score: {mean_score:.4f}\")\n\n        json_file_path = os.path.join(os.getcwd(), \"gstvqa_results.json\")\n        final_results = {\"video_results\": self.results, \"GSTVQA_Mean_Score\": mean_score}\n        with open(json_file_path, \"w\") as json_file:\n            json.dump(final_results, json_file, indent=4)\n        print(f\"GSTVQA mean score saved to {json_file_path}\")\n\n        return {'GSTVQA_Mean_Score': mean_score}\n</code></pre>"},{"location":"documentations/metrics/gstvqa/#aigve.metrics.video_quality_assessment.nn_based.gstvqa.GstVqa.compute_metrics","title":"<code>compute_metrics(results)</code>","text":"<p>Compute final GSTVQA-based metrics.</p> Source code in <code>aigve/metrics/video_quality_assessment/nn_based/gstvqa/gstvqa_metric.py</code> <pre><code>def compute_metrics(self, results: list) -&gt; Dict[str, float]:\n    \"\"\"Compute final GSTVQA-based metrics.\"\"\"\n    scores = np.array([res['GSTVQA_Score'] for res in self.results])\n    mean_score = np.mean(scores)\n    print(f\"GSTVQA mean score: {mean_score:.4f}\")\n\n    json_file_path = os.path.join(os.getcwd(), \"gstvqa_results.json\")\n    final_results = {\"video_results\": self.results, \"GSTVQA_Mean_Score\": mean_score}\n    with open(json_file_path, \"w\") as json_file:\n        json.dump(final_results, json_file, indent=4)\n    print(f\"GSTVQA mean score saved to {json_file_path}\")\n\n    return {'GSTVQA_Mean_Score': mean_score}\n</code></pre>"},{"location":"documentations/metrics/gstvqa/#aigve.metrics.video_quality_assessment.nn_based.gstvqa.GstVqa.compute_stat_features","title":"<code>compute_stat_features(features, num_valid_frames)</code>","text":"<p>Compute statistical features mean_var, std_var, mean_mean, std_mean from extracted deep features.</p> <p>Parameters:</p> Name Type Description Default <code>features</code> <code>Tensor</code> <p>Tensor of shape [T, 2944].</p> required <code>num_valid_frames</code> <code>int</code> <p>Number of valid frames before padding.</p> required <p>Returns:</p> Type Description <code>Tuple[Tensor]</code> <p>Tuple[torch.Tensor]: (mean_var, std_var, mean_mean, std_mean), each of shape [1472].</p> Source code in <code>aigve/metrics/video_quality_assessment/nn_based/gstvqa/gstvqa_metric.py</code> <pre><code>def compute_stat_features(self, features: torch.Tensor, num_valid_frames: int) -&gt; Tuple[torch.Tensor]:\n    \"\"\"Compute statistical features mean_var, std_var, mean_mean, std_mean from extracted deep features.\n\n    Args:\n        features (torch.Tensor): Tensor of shape [T, 2944].\n        num_valid_frames (int): Number of valid frames before padding.\n\n    Returns:\n        Tuple[torch.Tensor]: (mean_var, std_var, mean_mean, std_mean), each of shape [1472].\n    \"\"\"\n    # Ignore padded frames\n    features = features[:num_valid_frames]  # Shape: [num_valid_frames, feature_dim]: [10, 1472]\n\n    if num_valid_frames == 0:  # Edge case: all frames were padded\n        return (\n            torch.zeros(1472, device=self.device),\n            torch.zeros(1472, device=self.device),\n            torch.zeros(1472, device=self.device),\n            torch.zeros(1472, device=self.device),\n        )\n\n    # Split into mean and std components\n    mean_features = features[:, :1472]  # First 1472 features are mean-based\n    std_features = features[:, 1472:]   # Last 1472 features are std-based\n\n    # Compute per-feature statistics over frames\n    mean_mean = mean_features.mean(dim=0)  # Shape: [1472]\n    std_mean = std_features.mean(dim=0)    # Shape: [1472]\n    mean_var = mean_features.var(dim=0, unbiased=False)  # Shape: [1472]\n    std_var = std_features.var(dim=0, unbiased=False)    # Shape: [1472]\n\n    return mean_var, std_var, mean_mean, std_mean\n</code></pre>"},{"location":"documentations/metrics/gstvqa/#aigve.metrics.video_quality_assessment.nn_based.gstvqa.GstVqa.process","title":"<code>process(data_batch, data_samples)</code>","text":"<p>Process a batch of extracted deep features for GSTVQA evaluation and store results in a JSON file.</p> <p>Parameters:</p> Name Type Description Default <code>data_batch</code> <code>SequencTuplee</code> <p>A batch of data from the dataloader (not used here).</p> required <code>data_samples</code> <code>List[[Tensor], Tuple[int], Tuple[str]]</code> <p>A list containing three tuples: - A tuple of <code>deep_features</code>: Each item is a Tensor of shape [T, 2944].  - A tuple of <code>num_frames</code>: Each item is an integer representing the number of valid frames. - A tuple of <code>video_name</code>: Each item is a string representing the file name for the video. The len of these three tuples are the batch size.</p> required Source code in <code>aigve/metrics/video_quality_assessment/nn_based/gstvqa/gstvqa_metric.py</code> <pre><code>def process(self, data_batch: Sequence, data_samples: Sequence) -&gt; None:\n    \"\"\"\n    Process a batch of extracted deep features for GSTVQA evaluation and store results in a JSON file.\n\n    Args:\n        data_batch (SequencTuplee): A batch of data from the dataloader (not used here).\n        data_samples (List[ [torch.Tensor], Tuple[int], Tuple[str] ]): \n            A list containing three tuples:\n            - A tuple of `deep_features`: Each item is a Tensor of shape [T, 2944]. \n            - A tuple of `num_frames`: Each item is an integer representing the number of valid frames.\n            - A tuple of `video_name`: Each item is a string representing the file name for the video.\n            The len of these three tuples are the batch size.\n    \"\"\"\n    # data_samples an example: [\n    #     (tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n    #              [0., 0., 0.,  ..., 0., 0., 0.],\n    #              ...\n    #              [0., 0., 0.,  ..., 0., 0., 0.]]), \n    #      tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n    #              [0., 0., 0.,  ..., 0., 0., 0.],\n    #              ...\n    #              [0., 0., 0.,  ..., 0., 0., 0.]])), \n    #     (10, 10)\n    # ]\n    results = []\n    deep_features_tuple, num_frames_tuple, video_name_tuple = data_samples\n    with torch.no_grad():\n        for deep_features, num_valid_frames, video_name in zip(deep_features_tuple, num_frames_tuple, video_name_tuple):\n            if not isinstance(deep_features, torch.Tensor) or not isinstance(num_valid_frames, int):\n                raise TypeError(\"Expected deep_features to be a torch.Tensor and num_valid_frames to be an int.\")\n\n            if num_valid_frames == 0:  # Edge case: No valid frames\n                results.append({\"video_name\": 'N/A', \"GSTVQA_Score\": 0.0})\n                continue\n\n            # Remove padded features\n            features = deep_features[:num_valid_frames].to(self.device)\n\n            # Compute statistical features only on valid frames\n            mean_var, std_var, mean_mean, std_mean = self.compute_stat_features(features, num_valid_frames)\n            mean_var, std_var, mean_mean, std_mean = (\n                mean_var.to(self.device),\n                std_var.to(self.device),\n                mean_mean.to(self.device),\n                std_mean.to(self.device),\n            )\n\n            # Length tensor indicating the number of valid frames\n            length = torch.tensor([num_valid_frames]).to(self.device)\n            # print('features(input) shape', features.unsqueeze(1).shape) # torch.Size([10, 1, 1472])\n            # print('input_length shape', length.shape) # torch.Size([1])\n            # print('input_length', length) # torch.Size([1])\n            # print('mean_mean shape', mean_mean.shape) # torch.Size([1472])\n            # print('std_mean shape', std_mean.shape) # torch.Size([1472])\n            # print('mean_var shape', mean_var.shape) # torch.Size([1472])\n            # print('std_var shape', std_var.shape) # torch.Size([1472])\n\n            # Run GSTVQA model\n            outputs = self.model(features.unsqueeze(1), length, mean_var, std_var, mean_mean, std_mean)\n            score = outputs.item()\n            results.append({\"video_name\": video_name, \"GSTVQA_Score\": score})\n            # print(f\"Processed score {score:.4f} for {video_name}\")\n\n    self.results.extend(results)\n</code></pre>"},{"location":"documentations/metrics/is_score/","title":"class IS Score","text":""},{"location":"documentations/metrics/lightvqaplus/","title":"class LightVQA+","text":"<p>               Bases: <code>BaseMetric</code></p> <p>LightVQA+ metric for evaluating video quality.</p> Source code in <code>aigve/metrics/video_quality_assessment/nn_based/lightvqa_plus/lightvqa_plus_metric.py</code> <pre><code>@METRICS.register_module()\nclass LightVQAPlus(BaseMetric):\n    \"\"\"LightVQA+ metric for evaluating video quality.\"\"\"\n\n    def __init__(self, model_path: str, swin_weights: str, is_gpu: bool = True):\n        super(LightVQAPlus, self).__init__()\n        self.model_path = model_path\n        self.swin_weights = swin_weights\n        self.device = torch.device(\"cuda\" if is_gpu else \"cpu\")\n\n        self.submodel_path = os.path.join(os.getcwd(), 'metrics/video_quality_assessment/nn_based/lightvqa_plus')\n        if not submodule_exists(self.submodel_path):\n            add_git_submodule(\n                repo_url='https://github.com/SaMMyCHoo/Light-VQA-plus.git', \n                submodule_path=self.submodel_path\n            )\n        lightvqa_path = os.path.join(self.submodel_path, \"Light_VQA_plus\")\n        if lightvqa_path not in sys.path:\n            sys.path.insert(0, lightvqa_path)\n\n        from .Light_VQA_plus.final_fusion_model import swin_small_patch4_window7_224 as create_model\n        self.model = create_model().to(self.device)\n\n        weights_dict = torch.load(os.path.join(os.getcwd(), self.model_path), map_location=self.device)\n        print(self.model.load_state_dict(weights_dict))\n\n        self.model.eval()\n\n    def process(self, data_batch: list, data_samples: list) -&gt; None:\n        \"\"\"\n        Process a batch of extracted deep features for LightVQA+ evaluation.\n        Args:\n            data_batch (Sequence): A batch of data from the dataloader (not used here).\n            data_samples (List[Tuple[torch.Tensor], Tuple[torch.Tensor], Tuple[torch.Tensor], Tuple[str]]):\n                - spatial_features (torch.Tensor): Extracts 8 evenly spaced key frames. Shape: [8, 3, 672, 1120].\n                - temporal_features (torch.Tensor): Motion features from SlowFast. Shape: [1, feature_dim(2304)].\n                - bns_features (torch.Tensor): Brightness &amp; Noise features. Shape: [8, 300].\n                - bc_features (torch.Tensor): Temporal brightness contrast features. Shape: [8, final_dim(20)].\n                - video_name (str): Video filename.\n        \"\"\"\n        results = []\n        spatial_features_tuple, temporal_features_tuple, bns_features_tuple, bc_features_tuple, video_name_tuple = data_samples\n        # print('spatial_features_tuple len: ', len(spatial_features_tuple)) # B\n        # print('spatial_features_tuple[0]: ', spatial_features_tuple[0].shape) # torch.Size([8, 3, 672, 1120])\n        # print('temporal_features_tuple[0]: ', temporal_features_tuple[0].shape) # torch.Size([1, 2304])\n        # print('bns_features_tuple[0]: ', bns_features_tuple[0].shape) # torch.Size([8, 300])\n        # print('bc_features_tuple[0]: ', bc_features_tuple[0].shape) # torch.Size([8, 20])\n\n        batch_size = len(spatial_features_tuple)\n        with torch.no_grad():\n            for i in range(batch_size):\n                video_name = video_name_tuple[i]\n                spatial_features = spatial_features_tuple[i].to(self.device) # torch.Size([8, 3, 672, 1120])\n                temporal_features = temporal_features_tuple[i].to(self.device) # torch.Size([1, 2304])\n                bns_features = bns_features_tuple[i].to(self.device) # torch.Size([8, 300])\n                bc_features = bc_features_tuple[i].to(self.device)  # Shape: [8, final_dim(20)]\n\n                concat_features = torch.cat([temporal_features, bc_features.view(1, -1)], dim=1) # torch.Size([1, 2304+8*20])\n                # print('concat_features: ', concat_features.shape) # torch.Size([1, 2464])\n                final_temporal_features = F.pad(concat_features, (0, 2604 - concat_features.shape[1]), mode=\"constant\", value=0) # torch.Size([1, 2604])\n                # print('final_temporal_features: ', final_temporal_features.shape) # torch.Size([1, 2604])\n\n                outputs = self.model(spatial_features, final_temporal_features, bns_features)\n                # print('outputs: ', outputs)\n                score = outputs.mean().item()\n\n                results.append({\"video_name\": video_name, \"LightVQAPlus_Score\": score})\n                print(f\"Processed score {score:.4f} for {video_name}\")\n\n        self.results.extend(results)\n\n    def compute_metrics(self, results: list) -&gt; Dict[str, float]:\n        \"\"\"Compute final LightVQA+ metrics.\"\"\"\n        scores = np.array([res[\"LightVQAPlus_Score\"] for res in self.results])\n        mean_score = np.mean(scores) if scores.size &gt; 0 else 0.0\n        print(f\"LightVQA+ mean score: {mean_score:.4f}\")\n\n        json_file_path = os.path.join(os.getcwd(), \"lightvqaplus_results.json\")\n        final_results = {\"video_results\": self.results, \"LightVQAPlus_Mean_Score\": mean_score}\n        with open(json_file_path, \"w\") as json_file:\n            json.dump(final_results, json_file, indent=4)\n        print(f\"LightVQA+ mean score saved to {json_file_path}\")\n\n        return {\"LightVQAPlus_Mean_Score\": mean_score}\n</code></pre>"},{"location":"documentations/metrics/lightvqaplus/#aigve.metrics.video_quality_assessment.nn_based.lightvqa_plus.LightVQAPlus.compute_metrics","title":"<code>compute_metrics(results)</code>","text":"<p>Compute final LightVQA+ metrics.</p> Source code in <code>aigve/metrics/video_quality_assessment/nn_based/lightvqa_plus/lightvqa_plus_metric.py</code> <pre><code>def compute_metrics(self, results: list) -&gt; Dict[str, float]:\n    \"\"\"Compute final LightVQA+ metrics.\"\"\"\n    scores = np.array([res[\"LightVQAPlus_Score\"] for res in self.results])\n    mean_score = np.mean(scores) if scores.size &gt; 0 else 0.0\n    print(f\"LightVQA+ mean score: {mean_score:.4f}\")\n\n    json_file_path = os.path.join(os.getcwd(), \"lightvqaplus_results.json\")\n    final_results = {\"video_results\": self.results, \"LightVQAPlus_Mean_Score\": mean_score}\n    with open(json_file_path, \"w\") as json_file:\n        json.dump(final_results, json_file, indent=4)\n    print(f\"LightVQA+ mean score saved to {json_file_path}\")\n\n    return {\"LightVQAPlus_Mean_Score\": mean_score}\n</code></pre>"},{"location":"documentations/metrics/lightvqaplus/#aigve.metrics.video_quality_assessment.nn_based.lightvqa_plus.LightVQAPlus.process","title":"<code>process(data_batch, data_samples)</code>","text":"<p>Process a batch of extracted deep features for LightVQA+ evaluation. Args:     data_batch (Sequence): A batch of data from the dataloader (not used here).     data_samples (List[Tuple[torch.Tensor], Tuple[torch.Tensor], Tuple[torch.Tensor], Tuple[str]]):         - spatial_features (torch.Tensor): Extracts 8 evenly spaced key frames. Shape: [8, 3, 672, 1120].         - temporal_features (torch.Tensor): Motion features from SlowFast. Shape: [1, feature_dim(2304)].         - bns_features (torch.Tensor): Brightness &amp; Noise features. Shape: [8, 300].         - bc_features (torch.Tensor): Temporal brightness contrast features. Shape: [8, final_dim(20)].         - video_name (str): Video filename.</p> Source code in <code>aigve/metrics/video_quality_assessment/nn_based/lightvqa_plus/lightvqa_plus_metric.py</code> <pre><code>def process(self, data_batch: list, data_samples: list) -&gt; None:\n    \"\"\"\n    Process a batch of extracted deep features for LightVQA+ evaluation.\n    Args:\n        data_batch (Sequence): A batch of data from the dataloader (not used here).\n        data_samples (List[Tuple[torch.Tensor], Tuple[torch.Tensor], Tuple[torch.Tensor], Tuple[str]]):\n            - spatial_features (torch.Tensor): Extracts 8 evenly spaced key frames. Shape: [8, 3, 672, 1120].\n            - temporal_features (torch.Tensor): Motion features from SlowFast. Shape: [1, feature_dim(2304)].\n            - bns_features (torch.Tensor): Brightness &amp; Noise features. Shape: [8, 300].\n            - bc_features (torch.Tensor): Temporal brightness contrast features. Shape: [8, final_dim(20)].\n            - video_name (str): Video filename.\n    \"\"\"\n    results = []\n    spatial_features_tuple, temporal_features_tuple, bns_features_tuple, bc_features_tuple, video_name_tuple = data_samples\n    # print('spatial_features_tuple len: ', len(spatial_features_tuple)) # B\n    # print('spatial_features_tuple[0]: ', spatial_features_tuple[0].shape) # torch.Size([8, 3, 672, 1120])\n    # print('temporal_features_tuple[0]: ', temporal_features_tuple[0].shape) # torch.Size([1, 2304])\n    # print('bns_features_tuple[0]: ', bns_features_tuple[0].shape) # torch.Size([8, 300])\n    # print('bc_features_tuple[0]: ', bc_features_tuple[0].shape) # torch.Size([8, 20])\n\n    batch_size = len(spatial_features_tuple)\n    with torch.no_grad():\n        for i in range(batch_size):\n            video_name = video_name_tuple[i]\n            spatial_features = spatial_features_tuple[i].to(self.device) # torch.Size([8, 3, 672, 1120])\n            temporal_features = temporal_features_tuple[i].to(self.device) # torch.Size([1, 2304])\n            bns_features = bns_features_tuple[i].to(self.device) # torch.Size([8, 300])\n            bc_features = bc_features_tuple[i].to(self.device)  # Shape: [8, final_dim(20)]\n\n            concat_features = torch.cat([temporal_features, bc_features.view(1, -1)], dim=1) # torch.Size([1, 2304+8*20])\n            # print('concat_features: ', concat_features.shape) # torch.Size([1, 2464])\n            final_temporal_features = F.pad(concat_features, (0, 2604 - concat_features.shape[1]), mode=\"constant\", value=0) # torch.Size([1, 2604])\n            # print('final_temporal_features: ', final_temporal_features.shape) # torch.Size([1, 2604])\n\n            outputs = self.model(spatial_features, final_temporal_features, bns_features)\n            # print('outputs: ', outputs)\n            score = outputs.mean().item()\n\n            results.append({\"video_name\": video_name, \"LightVQAPlus_Score\": score})\n            print(f\"Processed score {score:.4f} for {video_name}\")\n\n    self.results.extend(results)\n</code></pre>"},{"location":"documentations/metrics/pickscore/","title":"class PickScore","text":"<p>               Bases: <code>BaseMetric</code></p> <p>Initialize the <code>PickScore</code> evaluator.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>The name of the PickScore model. Defaults to <code>yuvalkirstain/PickScore_v1</code>.</p> <code>'yuvalkirstain/PickScore_v1'</code> <code>logit_scale</code> <code>bool</code> <p>Whether to calcualte the cosine similarity as logits. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <p>None</p> Source code in <code>aigve/metrics/text_video_alignment/similarity_based/pickscore/pick_infer.py</code> <pre><code>@METRICS.register_module()\nclass PickScore(BaseMetric):\n    \"\"\" Initialize the ``PickScore`` evaluator.\n\n    Args:\n            model_name (str): The name of the PickScore model. Defaults to ``yuvalkirstain/PickScore_v1``.\n            logit_scale (bool): Whether to calcualte the cosine similarity as logits. Defaults to False.\n\n    Returns:\n            None\n    \"\"\"\n    def __init__(self, \n                 model_name: str = \"yuvalkirstain/PickScore_v1\", \n                 logit_scale: bool = False) -&gt; None:\n        super().__init__()\n        self.model_name = model_name\n        self.logit_scale = logit_scale\n\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self.model =AutoModel.from_pretrained(self.model_name).eval().to(self.device)\n        self.model.eval()\n\n\n    # def process(self, data_batch: dict, data_samples: Sequence[dict]) -&gt; None:\n    def process(self, data_batch: Sequence, data_samples: Sequence) -&gt; None:\n        \"\"\"PickScore process\n        Process one batch of data samples and predictions. The processed\n        results should be stored in ``self.results``, which will be used to\n        compute the metrics when all batches have been processed.\n\n        Args:\n            data_batch (Sequence): A batch of data from the dataloader.\n            data_samples (Sequence): A batch of data samples that\n                contain annotations and predictions.\n        \"\"\"\n\n        result = dict()\n\n        input_prompts, input_videos = data_samples\n        bsz = len(input_prompts)\n\n        # Ensure prompt_input is a tensor\n        if isinstance(input_prompts, tuple):\n            input_prompts = list(input_prompts)\n\n        if isinstance(input_videos, tuple):\n            input_videos = list(input_videos)\n\n        pickscore_sum, pickscore_cnt = 0, 0\n        logit_scale = self.model.logit_scale.exp() if self.logit_scale else 1\n        with torch.no_grad():\n            for input_prompt, input_frames in zip(input_prompts, input_videos):\n\n                input_prompt = input_prompt.to(self.device)\n                text_feature = self.model.get_text_features(input_prompt)\n                text_feature = text_feature / torch.norm(text_feature, dim=-1, keepdim=True)\n\n                input_frames = input_frames.to(self.device)  # Add batch dimension and move the frame to the device\n                frame_features = self.model.get_image_features(input_frames)\n                frame_features = frame_features / torch.norm(frame_features, dim=-1, keepdim=True)\n\n                pick_score = logit_scale *  (frame_features @ text_feature.T).mean().item()\n                print('current pickscore', pick_score)\n                pickscore_sum += pick_score\n                pickscore_cnt += 1\n\n        # get probabilities if you have multiple images to choose from\n        # probs = torch.softmax(scores, dim=-1)\n        pickscore_total_avg = pickscore_sum/pickscore_cnt\n        result['pick_score'] = pickscore_total_avg\n\n        self.results.append(result)\n\n\n    def compute_metrics(self, results: list) -&gt; Dict[str, float]:\n        \"\"\"Compute the metrics from processed results.\n\n        Args:\n            results (list): The processed results of each batch.\n\n        Returns:\n            Dict[str, float]: The computed metrics. The keys are the names of\n            the metrics, and the values are corresponding results.\n        \"\"\"\n        logger: MMLogger = MMLogger.get_current_instance()\n\n        pickscore_np = np.zeros(len(results))\n        for i, result in enumerate(results):\n            pickscore_np[i] = result['pick_score']\n\n        pickscore_sim_mean = np.mean(pickscore_np) \n\n        print(\"Test results: PickScore={:.4f}\"\n              .format(pickscore_sim_mean))\n\n        return result\n</code></pre>"},{"location":"documentations/metrics/pickscore/#aigve.metrics.text_video_alignment.similarity_based.pickscore.PickScore.compute_metrics","title":"<code>compute_metrics(results)</code>","text":"<p>Compute the metrics from processed results.</p> <p>Parameters:</p> Name Type Description Default <code>results</code> <code>list</code> <p>The processed results of each batch.</p> required <p>Returns:</p> Type Description <code>Dict[str, float]</code> <p>Dict[str, float]: The computed metrics. The keys are the names of</p> <code>Dict[str, float]</code> <p>the metrics, and the values are corresponding results.</p> Source code in <code>aigve/metrics/text_video_alignment/similarity_based/pickscore/pick_infer.py</code> <pre><code>def compute_metrics(self, results: list) -&gt; Dict[str, float]:\n    \"\"\"Compute the metrics from processed results.\n\n    Args:\n        results (list): The processed results of each batch.\n\n    Returns:\n        Dict[str, float]: The computed metrics. The keys are the names of\n        the metrics, and the values are corresponding results.\n    \"\"\"\n    logger: MMLogger = MMLogger.get_current_instance()\n\n    pickscore_np = np.zeros(len(results))\n    for i, result in enumerate(results):\n        pickscore_np[i] = result['pick_score']\n\n    pickscore_sim_mean = np.mean(pickscore_np) \n\n    print(\"Test results: PickScore={:.4f}\"\n          .format(pickscore_sim_mean))\n\n    return result\n</code></pre>"},{"location":"documentations/metrics/pickscore/#aigve.metrics.text_video_alignment.similarity_based.pickscore.PickScore.process","title":"<code>process(data_batch, data_samples)</code>","text":"<p>PickScore process Process one batch of data samples and predictions. The processed results should be stored in <code>self.results</code>, which will be used to compute the metrics when all batches have been processed.</p> <p>Parameters:</p> Name Type Description Default <code>data_batch</code> <code>Sequence</code> <p>A batch of data from the dataloader.</p> required <code>data_samples</code> <code>Sequence</code> <p>A batch of data samples that contain annotations and predictions.</p> required Source code in <code>aigve/metrics/text_video_alignment/similarity_based/pickscore/pick_infer.py</code> <pre><code>def process(self, data_batch: Sequence, data_samples: Sequence) -&gt; None:\n    \"\"\"PickScore process\n    Process one batch of data samples and predictions. The processed\n    results should be stored in ``self.results``, which will be used to\n    compute the metrics when all batches have been processed.\n\n    Args:\n        data_batch (Sequence): A batch of data from the dataloader.\n        data_samples (Sequence): A batch of data samples that\n            contain annotations and predictions.\n    \"\"\"\n\n    result = dict()\n\n    input_prompts, input_videos = data_samples\n    bsz = len(input_prompts)\n\n    # Ensure prompt_input is a tensor\n    if isinstance(input_prompts, tuple):\n        input_prompts = list(input_prompts)\n\n    if isinstance(input_videos, tuple):\n        input_videos = list(input_videos)\n\n    pickscore_sum, pickscore_cnt = 0, 0\n    logit_scale = self.model.logit_scale.exp() if self.logit_scale else 1\n    with torch.no_grad():\n        for input_prompt, input_frames in zip(input_prompts, input_videos):\n\n            input_prompt = input_prompt.to(self.device)\n            text_feature = self.model.get_text_features(input_prompt)\n            text_feature = text_feature / torch.norm(text_feature, dim=-1, keepdim=True)\n\n            input_frames = input_frames.to(self.device)  # Add batch dimension and move the frame to the device\n            frame_features = self.model.get_image_features(input_frames)\n            frame_features = frame_features / torch.norm(frame_features, dim=-1, keepdim=True)\n\n            pick_score = logit_scale *  (frame_features @ text_feature.T).mean().item()\n            print('current pickscore', pick_score)\n            pickscore_sum += pick_score\n            pickscore_cnt += 1\n\n    # get probabilities if you have multiple images to choose from\n    # probs = torch.softmax(scores, dim=-1)\n    pickscore_total_avg = pickscore_sum/pickscore_cnt\n    result['pick_score'] = pickscore_total_avg\n\n    self.results.append(result)\n</code></pre>"},{"location":"documentations/metrics/simplevqa/","title":"class SimpleVQA","text":"<p>               Bases: <code>BaseMetric</code></p> <p>SimpleVQA metric for evaluating video quality.</p> Source code in <code>aigve/metrics/video_quality_assessment/nn_based/simplevqa/simplevqa_metric.py</code> <pre><code>@METRICS.register_module()\nclass SimpleVqa(BaseMetric):\n    \"\"\"SimpleVQA metric for evaluating video quality.\"\"\"\n    def __init__(self, model_path: str, is_gpu: bool = True):\n        super(SimpleVqa, self).__init__()\n        self.model_path = model_path\n        self.device = torch.device(\"cuda\" if is_gpu else \"cpu\")\n        self.submodel_path = os.path.join(os.getcwd(), 'metrics/video_quality_assessment/nn_based/simplevqa')\n        if not submodule_exists(self.submodel_path):\n            add_git_submodule(\n                repo_url='https://github.com/sunwei925/SimpleVQA.git', \n                submodule_path=self.submodel_path\n            )\n        simplevqa_path = os.path.join(self.submodel_path, \"SimpleVQA\")\n        if simplevqa_path not in sys.path:\n            sys.path.insert(0, simplevqa_path)\n        from .SimpleVQA.model import UGC_BVQA_model\n        from .SimpleVQA.test_demo import slowfast\n        self.model_motion = slowfast().to(self.device)\n        self.model = UGC_BVQA_model.resnet50(pretrained=False)\n        self.model = torch.nn.DataParallel(self.model).to(self.device)\n        self.model.load_state_dict(torch.load(os.path.join(os.getcwd(), self.model_path), map_location=self.device))\n        self.model.eval()\n\n    def process(self, data_batch: list, data_samples: list) -&gt; None:\n        \"\"\"\n        Process a batch of extracted deep features for SimpleVQA evaluation.\n        Args:\n            data_batch (Sequence): A batch of data from the dataloader (not used here).\n            data_samples (List[ Tuple[torch.Tensor], List[Tuple[torch.Tensor]], Tuple[str] ]):\n                A list containing three tuples:\n                - A tuple of `spatial_features` (torch.Tensor): Shape [v_len_second, 3, 448, 448]. \n                    `v_len_second` is total seconds of the video (though 2 for toy dataset) with minium 8 (i.e. min_video_seconds). \n                    The len of the tuple is the batch size. \n                - A list of `motion_features` (Tuple[torch.Tensor]): \n                    len(List) is total seconds of the video, with minium 8 (i.e. min_video_seconds).\n                    Each item of the list is a Tuple of motion feature tensors. Each has shape [32, 3, 224, 224].\n                    The len of the tuple is the batch size.\n                - A tuple of `video_name` (str): Video filename. The len of the tuple is the batch size.\n        \"\"\"\n        from .SimpleVQA.test_demo import pack_pathway_output\n\n        results = []\n        # print(type(data_samples)) # list\n        spatial_features_tuple, motion_features_list, video_name_tuple = data_samples\n        # print(len(spatial_features_tuple)) # 1\n        # print(spatial_features_tuple[0].shape) # torch.Size([8, 3, 448, 448])\n\n        # print(type(motion_features_list)) # List\n        # print(len(motion_features_list)) # 8\n        # print(type(motion_features_list[0])) # tuple\n        # print(len(motion_features_list[0])) # 1\n        # print(type(motion_features_list[0][0])) # Tensor\n        # print(motion_features_list[0][0].shape) # torch.Size([32, 3, 224, 224])\n\n        batch_size = len(spatial_features_tuple)\n        with torch.no_grad():\n            for i in range(batch_size):\n                video_name = video_name_tuple[i]\n                spatial_features = spatial_features_tuple[i].to(self.device).unsqueeze(0)  # Add batch dim. Shape: tensor with Size([1, v_len_second, 3, 448, 448])\n\n                # Take the i-th element from each tuple in motion_features_list\n                motion_features = [motion_features_list[j][i] for j in range(len(motion_features_list))] # Shape: List[tensor with Size([32, 3, 224, 224])], len of it is total seconds of the video, with minium 8.\n\n                if not all(isinstance(mf, torch.Tensor) for mf in motion_features):\n                    raise TypeError(\"Expected motion_features to be a list of tensors.\")\n\n                if len(motion_features) == 0:  # Edge case: No valid motion features\n                    results.append({\"video_name\": video_name, \"SimpleVQA_Score\": 0.0})\n                    continue\n\n                n_clip = len(motion_features)  # 8\n                feature_motion = torch.zeros([n_clip, 2048 + 256], device=self.device) \n                # Process each motion clip\n                for idx, clip in enumerate(motion_features):\n                    clip = clip.unsqueeze(dim=0).permute(0, 2, 1, 3, 4)  # Reshape to [1, C(3), T(32), H(224), W(224)]\n                    clip = pack_pathway_output(clip, self.device)  # Convert to SlowFast format\n                    slow_feature, fast_feature = self.model_motion(clip)\n                    slow_feature = slow_feature.squeeze()\n                    fast_feature = fast_feature.squeeze()\n\n                    motion_feature = torch.cat([slow_feature, fast_feature]).unsqueeze(0)  # Shape: [1, 2304]\n                    feature_motion[idx] = motion_feature \n\n                feature_motion = feature_motion.unsqueeze(0)  # Shape: [1, n_clip, 2304]\n\n                outputs = self.model(spatial_features, feature_motion)\n                score = outputs.item()\n\n                results.append({\"video_name\": video_name, \"SimpleVQA_Score\": score})\n                print(f\"Processed score {score:.4f} for {video_name}\")\n\n        self.results.extend(results)\n\n    def compute_metrics(self, results: list) -&gt; Dict[str, float]:\n        \"\"\"Compute final SimpleVQA-based metrics.\"\"\"\n        scores = np.array([res[\"SimpleVQA_Score\"] for res in self.results])\n        mean_score = np.mean(scores) if scores.size &gt; 0 else 0.0\n        print(f\"SimpleVQA mean score: {mean_score:.4f}\")\n\n        json_file_path = os.path.join(os.getcwd(), \"simplevqa_results.json\")\n        final_results = {\"video_results\": self.results, \"SimpleVQA_Mean_Score\": mean_score}\n        with open(json_file_path, \"w\") as json_file:\n            json.dump(final_results, json_file, indent=4)\n        print(f\"SimpleVQA mean score saved to {json_file_path}\")\n\n        return {\"SimpleVQA_Mean_Score\": mean_score}\n</code></pre>"},{"location":"documentations/metrics/simplevqa/#aigve.metrics.video_quality_assessment.nn_based.simplevqa.SimpleVqa.compute_metrics","title":"<code>compute_metrics(results)</code>","text":"<p>Compute final SimpleVQA-based metrics.</p> Source code in <code>aigve/metrics/video_quality_assessment/nn_based/simplevqa/simplevqa_metric.py</code> <pre><code>def compute_metrics(self, results: list) -&gt; Dict[str, float]:\n    \"\"\"Compute final SimpleVQA-based metrics.\"\"\"\n    scores = np.array([res[\"SimpleVQA_Score\"] for res in self.results])\n    mean_score = np.mean(scores) if scores.size &gt; 0 else 0.0\n    print(f\"SimpleVQA mean score: {mean_score:.4f}\")\n\n    json_file_path = os.path.join(os.getcwd(), \"simplevqa_results.json\")\n    final_results = {\"video_results\": self.results, \"SimpleVQA_Mean_Score\": mean_score}\n    with open(json_file_path, \"w\") as json_file:\n        json.dump(final_results, json_file, indent=4)\n    print(f\"SimpleVQA mean score saved to {json_file_path}\")\n\n    return {\"SimpleVQA_Mean_Score\": mean_score}\n</code></pre>"},{"location":"documentations/metrics/simplevqa/#aigve.metrics.video_quality_assessment.nn_based.simplevqa.SimpleVqa.process","title":"<code>process(data_batch, data_samples)</code>","text":"<p>Process a batch of extracted deep features for SimpleVQA evaluation. Args:     data_batch (Sequence): A batch of data from the dataloader (not used here).     data_samples (List[ Tuple[torch.Tensor], List[Tuple[torch.Tensor]], Tuple[str] ]):         A list containing three tuples:         - A tuple of <code>spatial_features</code> (torch.Tensor): Shape [v_len_second, 3, 448, 448].              <code>v_len_second</code> is total seconds of the video (though 2 for toy dataset) with minium 8 (i.e. min_video_seconds).              The len of the tuple is the batch size.          - A list of <code>motion_features</code> (Tuple[torch.Tensor]):              len(List) is total seconds of the video, with minium 8 (i.e. min_video_seconds).             Each item of the list is a Tuple of motion feature tensors. Each has shape [32, 3, 224, 224].             The len of the tuple is the batch size.         - A tuple of <code>video_name</code> (str): Video filename. The len of the tuple is the batch size.</p> Source code in <code>aigve/metrics/video_quality_assessment/nn_based/simplevqa/simplevqa_metric.py</code> <pre><code>def process(self, data_batch: list, data_samples: list) -&gt; None:\n    \"\"\"\n    Process a batch of extracted deep features for SimpleVQA evaluation.\n    Args:\n        data_batch (Sequence): A batch of data from the dataloader (not used here).\n        data_samples (List[ Tuple[torch.Tensor], List[Tuple[torch.Tensor]], Tuple[str] ]):\n            A list containing three tuples:\n            - A tuple of `spatial_features` (torch.Tensor): Shape [v_len_second, 3, 448, 448]. \n                `v_len_second` is total seconds of the video (though 2 for toy dataset) with minium 8 (i.e. min_video_seconds). \n                The len of the tuple is the batch size. \n            - A list of `motion_features` (Tuple[torch.Tensor]): \n                len(List) is total seconds of the video, with minium 8 (i.e. min_video_seconds).\n                Each item of the list is a Tuple of motion feature tensors. Each has shape [32, 3, 224, 224].\n                The len of the tuple is the batch size.\n            - A tuple of `video_name` (str): Video filename. The len of the tuple is the batch size.\n    \"\"\"\n    from .SimpleVQA.test_demo import pack_pathway_output\n\n    results = []\n    # print(type(data_samples)) # list\n    spatial_features_tuple, motion_features_list, video_name_tuple = data_samples\n    # print(len(spatial_features_tuple)) # 1\n    # print(spatial_features_tuple[0].shape) # torch.Size([8, 3, 448, 448])\n\n    # print(type(motion_features_list)) # List\n    # print(len(motion_features_list)) # 8\n    # print(type(motion_features_list[0])) # tuple\n    # print(len(motion_features_list[0])) # 1\n    # print(type(motion_features_list[0][0])) # Tensor\n    # print(motion_features_list[0][0].shape) # torch.Size([32, 3, 224, 224])\n\n    batch_size = len(spatial_features_tuple)\n    with torch.no_grad():\n        for i in range(batch_size):\n            video_name = video_name_tuple[i]\n            spatial_features = spatial_features_tuple[i].to(self.device).unsqueeze(0)  # Add batch dim. Shape: tensor with Size([1, v_len_second, 3, 448, 448])\n\n            # Take the i-th element from each tuple in motion_features_list\n            motion_features = [motion_features_list[j][i] for j in range(len(motion_features_list))] # Shape: List[tensor with Size([32, 3, 224, 224])], len of it is total seconds of the video, with minium 8.\n\n            if not all(isinstance(mf, torch.Tensor) for mf in motion_features):\n                raise TypeError(\"Expected motion_features to be a list of tensors.\")\n\n            if len(motion_features) == 0:  # Edge case: No valid motion features\n                results.append({\"video_name\": video_name, \"SimpleVQA_Score\": 0.0})\n                continue\n\n            n_clip = len(motion_features)  # 8\n            feature_motion = torch.zeros([n_clip, 2048 + 256], device=self.device) \n            # Process each motion clip\n            for idx, clip in enumerate(motion_features):\n                clip = clip.unsqueeze(dim=0).permute(0, 2, 1, 3, 4)  # Reshape to [1, C(3), T(32), H(224), W(224)]\n                clip = pack_pathway_output(clip, self.device)  # Convert to SlowFast format\n                slow_feature, fast_feature = self.model_motion(clip)\n                slow_feature = slow_feature.squeeze()\n                fast_feature = fast_feature.squeeze()\n\n                motion_feature = torch.cat([slow_feature, fast_feature]).unsqueeze(0)  # Shape: [1, 2304]\n                feature_motion[idx] = motion_feature \n\n            feature_motion = feature_motion.unsqueeze(0)  # Shape: [1, n_clip, 2304]\n\n            outputs = self.model(spatial_features, feature_motion)\n            score = outputs.item()\n\n            results.append({\"video_name\": video_name, \"SimpleVQA_Score\": score})\n            print(f\"Processed score {score:.4f} for {video_name}\")\n\n    self.results.extend(results)\n</code></pre>"},{"location":"documentations/metrics/tifa/","title":"class TIFAScore","text":"<p>               Bases: <code>BaseMetric</code></p> <p>Initialize the <code>TIFAScore</code> evaluator.</p> <p>Parameters:</p> Name Type Description Default <code>openai_key</code> <code>str</code> <p>The user's api key of the LLM models openai provides.</p> required <code>llm_model</code> <code>str</code> <p>The name of the LLM model used in the TIFAScore evaluator. Defaults to <code>gpt-3.5-turbo</code>.</p> <code>'gpt-3.5-turbo'</code> <code>unifiedqa_model_name</code> <code>str</code> <p>The name of the <code>UnifiedQAModel</code> used in TIFAScore evaluator. Defaults to <code>allenai/unifiedqa-v2-t5-large-1363200</code>.</p> <code>'allenai/unifiedqa-v2-t5-large-1363200'</code> <code>vqa_model_name</code> <code>str</code> <p>The name of the <code>VQAModel used</code> in TIFAScore evaluator. Defaults to <code>mplug-large</code>.</p> <code>'mplug-large'</code> <p>Returns:</p> Type Description <p>None</p> Source code in <code>aigve/metrics/text_video_alignment/gpt_based/TIFA/tifa_eval.py</code> <pre><code>@METRICS.register_module()\nclass TIFAScore(BaseMetric):\n    \"\"\" Initialize the ``TIFAScore`` evaluator.\n\n    Args:   \n            openai_key (str): The user's api key of the LLM models openai provides.\n            llm_model (str): The name of the LLM model used in the TIFAScore evaluator. Defaults to ``gpt-3.5-turbo``.\n            unifiedqa_model_name (str): The name of the ``UnifiedQAModel`` used in TIFAScore evaluator. Defaults to ``allenai/unifiedqa-v2-t5-large-1363200``.\n            vqa_model_name (str): The name of the ``VQAModel used`` in TIFAScore evaluator. Defaults to ``mplug-large``.\n\n    Returns:\n            None\n    \"\"\"\n    def __init__(self, \n                 openai_key,\n                 llm_model: str = 'gpt-3.5-turbo',\n                 unifiedqa_model_name: str = 'allenai/unifiedqa-v2-t5-large-1363200',\n                 vqa_model_name: str = 'mplug-large'):\n        super().__init__()\n\n        self.openai_key = openai_key\n        self.llm_model = llm_model\n        self.unifiedqa_model_name = unifiedqa_model_name\n        self.openai_completion, self.get_question_and_answers, self.filter_question_and_answers, self.unifiedqa_model, self.tifa_score_single, self.vqa_model = lazy_import()\n        self.unifiedqa_model = self.UnifiedQAModel(self.unifiedqa_model_name)\n        self.vqa_model_name = vqa_model_name\n        self.vqa_model = self.VQAModel(self.vqa_model_name)\n\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n        self.openai_setup()\n\n    def openai_setup(self):\n        print('set up openai client')\n        openai.api_key = self.openai_key\n        assert openai.api_key is not None\n        test_prompt_string = 'hello, how are you doing?'\n        print('test prompt: ', test_prompt_string)\n        response = self.openai_completion(\n            test_prompt_string,\n            model=self.llm_model,\n        )\n        print('test response: ', response)\n\n\n    def process(self, data_batch: Sequence, data_samples: Sequence) -&gt; None:\n        \"\"\" TIFAScore process\n        Process one batch of data samples and predictions. The processed\n        results should be stored in ``self.results``, which will be used to\n        compute the metrics when all batches have been processed.\n\n        Args:\n            data_batch (Sequence): A batch of data from the dataloader.\n            data_samples (Sequence): A batch of data samples that\n                contain annotations and predictions.\n        \"\"\"\n\n        result = dict()\n\n        input_prompts, input_videos = data_samples\n        bsz = len(input_prompts)\n\n        # Ensure prompt_input is a tensor\n        if isinstance(input_prompts, tuple):\n            input_prompts = list(input_prompts)\n\n        if isinstance(input_videos, tuple):\n            input_videos = list(input_videos)\n\n        average_tifa_score_list = []\n        for input_prompt, input_video in zip(input_prompts, input_videos):\n            tifa_score = []\n            # Generate questions with GPT-3.5-turbo\n            gpt3_questions = self.get_question_and_answers(input_prompt)\n            # print(gpt3_questions)\n            # Filter questions with UnifiedQA\n            filtered_questions = self.filter_question_and_answers(self.unifiedqa_model, gpt3_questions)\n            for index, frame_path in enumerate(input_video):\n                # calucluate TIFA score\n                result = self.tifa_score_single(self.vqa_model, filtered_questions, frame_path)\n                # print(result)\n                tifa_score.append(result['tifa_score'])\n            average_tifa_score = sum(tifa_score)/len(tifa_score)\n            average_tifa_score_list.append(average_tifa_score)\n\n        result['tifa_score'] = sum(average_tifa_score_list)/len(average_tifa_score_list)\n\n        self.results.append(result)\n\n\n    def compute_metrics(self, results: list) -&gt; Dict[str, float]:\n        \"\"\"Compute the metrics from processed results.\n\n        Args:\n            results (list): The processed results of each batch.\n\n        Returns:\n            Dict[str, float]: The computed metrics. The keys are the names of\n            the metrics, and the values are corresponding results.\n        \"\"\"\n        logger: MMLogger = MMLogger.get_current_instance()\n\n        tifa_score_np = np.zeros(len(results))\n        for i, result in enumerate(results):\n            tifa_score_np[i] = result['tifa_score']\n\n        tifa_score_np_mean = np.mean(tifa_score_np) \n\n        print(\"Test results: tifa score={:.4f}\"\n              .format(tifa_score_np_mean))\n\n        return result\n</code></pre>"},{"location":"documentations/metrics/tifa/#aigve.metrics.text_video_alignment.gpt_based.TIFA.TIFAScore.compute_metrics","title":"<code>compute_metrics(results)</code>","text":"<p>Compute the metrics from processed results.</p> <p>Parameters:</p> Name Type Description Default <code>results</code> <code>list</code> <p>The processed results of each batch.</p> required <p>Returns:</p> Type Description <code>Dict[str, float]</code> <p>Dict[str, float]: The computed metrics. The keys are the names of</p> <code>Dict[str, float]</code> <p>the metrics, and the values are corresponding results.</p> Source code in <code>aigve/metrics/text_video_alignment/gpt_based/TIFA/tifa_eval.py</code> <pre><code>def compute_metrics(self, results: list) -&gt; Dict[str, float]:\n    \"\"\"Compute the metrics from processed results.\n\n    Args:\n        results (list): The processed results of each batch.\n\n    Returns:\n        Dict[str, float]: The computed metrics. The keys are the names of\n        the metrics, and the values are corresponding results.\n    \"\"\"\n    logger: MMLogger = MMLogger.get_current_instance()\n\n    tifa_score_np = np.zeros(len(results))\n    for i, result in enumerate(results):\n        tifa_score_np[i] = result['tifa_score']\n\n    tifa_score_np_mean = np.mean(tifa_score_np) \n\n    print(\"Test results: tifa score={:.4f}\"\n          .format(tifa_score_np_mean))\n\n    return result\n</code></pre>"},{"location":"documentations/metrics/tifa/#aigve.metrics.text_video_alignment.gpt_based.TIFA.TIFAScore.process","title":"<code>process(data_batch, data_samples)</code>","text":"<p>TIFAScore process Process one batch of data samples and predictions. The processed results should be stored in <code>self.results</code>, which will be used to compute the metrics when all batches have been processed.</p> <p>Parameters:</p> Name Type Description Default <code>data_batch</code> <code>Sequence</code> <p>A batch of data from the dataloader.</p> required <code>data_samples</code> <code>Sequence</code> <p>A batch of data samples that contain annotations and predictions.</p> required Source code in <code>aigve/metrics/text_video_alignment/gpt_based/TIFA/tifa_eval.py</code> <pre><code>def process(self, data_batch: Sequence, data_samples: Sequence) -&gt; None:\n    \"\"\" TIFAScore process\n    Process one batch of data samples and predictions. The processed\n    results should be stored in ``self.results``, which will be used to\n    compute the metrics when all batches have been processed.\n\n    Args:\n        data_batch (Sequence): A batch of data from the dataloader.\n        data_samples (Sequence): A batch of data samples that\n            contain annotations and predictions.\n    \"\"\"\n\n    result = dict()\n\n    input_prompts, input_videos = data_samples\n    bsz = len(input_prompts)\n\n    # Ensure prompt_input is a tensor\n    if isinstance(input_prompts, tuple):\n        input_prompts = list(input_prompts)\n\n    if isinstance(input_videos, tuple):\n        input_videos = list(input_videos)\n\n    average_tifa_score_list = []\n    for input_prompt, input_video in zip(input_prompts, input_videos):\n        tifa_score = []\n        # Generate questions with GPT-3.5-turbo\n        gpt3_questions = self.get_question_and_answers(input_prompt)\n        # print(gpt3_questions)\n        # Filter questions with UnifiedQA\n        filtered_questions = self.filter_question_and_answers(self.unifiedqa_model, gpt3_questions)\n        for index, frame_path in enumerate(input_video):\n            # calucluate TIFA score\n            result = self.tifa_score_single(self.vqa_model, filtered_questions, frame_path)\n            # print(result)\n            tifa_score.append(result['tifa_score'])\n        average_tifa_score = sum(tifa_score)/len(tifa_score)\n        average_tifa_score_list.append(average_tifa_score)\n\n    result['tifa_score'] = sum(average_tifa_score_list)/len(average_tifa_score_list)\n\n    self.results.append(result)\n</code></pre>"},{"location":"documentations/metrics/videophy/","title":"class VideoPhy","text":"<p>               Bases: <code>BaseMetric</code></p> Source code in <code>aigve/metrics/multi_aspect_metrics/videophy/videophy_metric.py</code> <pre><code>@METRICS.register_module()\nclass VideoPhy(BaseMetric):\n    def __init__(self,\n                hf_token: str,\n                collect_device: Optional[Union[str, torch.device]] = None,\n                prefix: Optional[str] = None,\n                metric_path: str = None,\n                model_path: str = 'videophysics/videocon_physics',\n                datainfo_path: str = None,\n                test_index: int = None,\n                 **kwargs):\n\n        \"\"\"\n        This function is used to initialize the VideoPhy metric.\n\n        Args:\n            collect_device (str or torch.device): The device to use for collecting the data\n            prefix (str): The prefix to use for the metric name\n            metric_path (str): The path to the metric\n            model_path (str): The path to the model\n            datainfo_path (str): The path to the data info\n            test_index (int): The index of the test\n        \"\"\"\n\n        super().__init__(collect_device=collect_device, prefix=prefix)\n        # self.train_index = train_index\n        self.metric_path = metric_path\n        self.model_path = model_path\n        self.datainfo_path = datainfo_path\n        self.test_index = test_index\n        self.hf_token = hf_token\n        self.results = []\n\n        # self.submodule_path = './metrics/aigve'\n        # if not submodule_exists(self.submodule_path):\n        #     add_git_submodule(\n        #         repo_url='https://github.com/Hritikbansal/videophy.git',\n        #         submodule_path=self.submodule_path\n        #     )\n\n        self.tokenizer = LlamaTokenizer.from_pretrained(self.model_path, token=self.hf_token)\n        self.image_processor = MplugOwlImageProcessor.from_pretrained(self.model_path)\n        self.processor = MplugOwlProcessor(self.image_processor, self.tokenizer)\n        self.model = MplugOwlForConditionalGeneration.from_pretrained(\n            self.model_path,\n            torch_dtype=torch.bfloat16,\n        ).to('cuda')\n        self.model.eval()\n\n    def get_entail(self, logits, input_ids):\n        \"\"\"\n        This function is used to get the entailment scores.\n\n        Args:\n            logits (torch.Tensor): A tensor containing the logits\n            input_ids (torch.Tensor): A tensor containing the input IDs\n        \"\"\"\n        softmax = nn.Softmax(dim=2)\n        logits = softmax(logits)\n        token_id_yes = self.tokenizer.encode('Yes', add_special_tokens=False)[0]\n        token_id_no = self.tokenizer.encode('No', add_special_tokens=False)[0]\n        entailment = []\n        for j in range(len(logits)):\n            for i in range(len(input_ids[j])):\n                if input_ids[j][i] == self.tokenizer.pad_token_id:  # pad token if the answer is not present\n                    i = i - 1\n                    break\n                elif i == len(input_ids[j]) - 1:\n                    break\n            score = logits[j][i][token_id_yes] / (logits[j][i][token_id_yes] + logits[j][i][token_id_no])\n            entailment.append(score)\n        entailment = torch.stack(entailment)\n        return entailment\n\n    def get_logits(self, data_batch):\n        \"\"\"\n        This function is used to get the logits for each input in the data batch.\n\n        Args:\n            data_batch (dict): A dictionary containing the data batch\n        Returns:\n            logits (torch.Tensor): A tensor containing the logits for each input in the data batch\n        \"\"\"\n        # Iterate over each item in the data batch\n        for k, v in data_batch.items():\n            # Check if the item is a tensor\n            if torch.is_tensor(v):\n                # Convert float tensors to bfloat16\n                if v.dtype == torch.float:\n                    data_batch[k] = v.bfloat16()\n                # Move the tensor to the model's device (e.g., GPU)\n                data_batch[k] = data_batch[k].to(self.model.device)\n\n        # print(\"Data batch: \", data_batch.keys())\n        outputs = self.model(pixel_values=data_batch['pixel_values'], video_pixel_values=data_batch['video_pixel_values'],\n                        labels=None, \\\n                        num_images=data_batch['num_images'], num_videos=data_batch['num_videos'], input_ids=data_batch['input_ids'],\n                        non_padding_mask=data_batch['non_padding_mask'], \\\n                        non_media_mask=data_batch['non_media_mask'], prompt_mask=data_batch['prompt_mask'])\n        logits = outputs['logits']\n        return logits\n\n\n    def process(self, data_batch: Any, data_samples: Sequence[dict]) -&gt; None:\n        \"\"\"\n        This function is used to process the data batch and compute the metric.\n\n        Args:\n            data_batch (dict): A dictionary containing the data batch\n            data_samples (list): A list of dictionaries containing the data samples\n        \"\"\"\n        logits = self.get_logits(data_batch)\n        entails_scores =  self.get_entail(logits, data_batch['input_ids'])\n\n        self.results.extend(entails_scores.cpu().detach().to(torch.float32).numpy().tolist())\n        # self.results = entails_scores.cpu().detach().to(torch.float32).numpy().tolist()\n        # print(self.results)\n\n\n    def compute_metrics(self, results: list) -&gt; dict:\n        \"\"\"\n        This function is used to compute the metrics.\n\n        Args:\n            results (list): A list of results\n        \"\"\"\n        return {\n            'entailment': float(np.mean(results))\n        }\n</code></pre>"},{"location":"documentations/metrics/videophy/#aigve.metrics.multi_aspect_metrics.videophy.videophy_metric.VideoPhy.__init__","title":"<code>__init__(hf_token, collect_device=None, prefix=None, metric_path=None, model_path='videophysics/videocon_physics', datainfo_path=None, test_index=None, **kwargs)</code>","text":"<p>This function is used to initialize the VideoPhy metric.</p> <p>Parameters:</p> Name Type Description Default <code>collect_device</code> <code>str or device</code> <p>The device to use for collecting the data</p> <code>None</code> <code>prefix</code> <code>str</code> <p>The prefix to use for the metric name</p> <code>None</code> <code>metric_path</code> <code>str</code> <p>The path to the metric</p> <code>None</code> <code>model_path</code> <code>str</code> <p>The path to the model</p> <code>'videophysics/videocon_physics'</code> <code>datainfo_path</code> <code>str</code> <p>The path to the data info</p> <code>None</code> <code>test_index</code> <code>int</code> <p>The index of the test</p> <code>None</code> Source code in <code>aigve/metrics/multi_aspect_metrics/videophy/videophy_metric.py</code> <pre><code>def __init__(self,\n            hf_token: str,\n            collect_device: Optional[Union[str, torch.device]] = None,\n            prefix: Optional[str] = None,\n            metric_path: str = None,\n            model_path: str = 'videophysics/videocon_physics',\n            datainfo_path: str = None,\n            test_index: int = None,\n             **kwargs):\n\n    \"\"\"\n    This function is used to initialize the VideoPhy metric.\n\n    Args:\n        collect_device (str or torch.device): The device to use for collecting the data\n        prefix (str): The prefix to use for the metric name\n        metric_path (str): The path to the metric\n        model_path (str): The path to the model\n        datainfo_path (str): The path to the data info\n        test_index (int): The index of the test\n    \"\"\"\n\n    super().__init__(collect_device=collect_device, prefix=prefix)\n    # self.train_index = train_index\n    self.metric_path = metric_path\n    self.model_path = model_path\n    self.datainfo_path = datainfo_path\n    self.test_index = test_index\n    self.hf_token = hf_token\n    self.results = []\n\n    # self.submodule_path = './metrics/aigve'\n    # if not submodule_exists(self.submodule_path):\n    #     add_git_submodule(\n    #         repo_url='https://github.com/Hritikbansal/videophy.git',\n    #         submodule_path=self.submodule_path\n    #     )\n\n    self.tokenizer = LlamaTokenizer.from_pretrained(self.model_path, token=self.hf_token)\n    self.image_processor = MplugOwlImageProcessor.from_pretrained(self.model_path)\n    self.processor = MplugOwlProcessor(self.image_processor, self.tokenizer)\n    self.model = MplugOwlForConditionalGeneration.from_pretrained(\n        self.model_path,\n        torch_dtype=torch.bfloat16,\n    ).to('cuda')\n    self.model.eval()\n</code></pre>"},{"location":"documentations/metrics/videophy/#aigve.metrics.multi_aspect_metrics.videophy.videophy_metric.VideoPhy.compute_metrics","title":"<code>compute_metrics(results)</code>","text":"<p>This function is used to compute the metrics.</p> <p>Parameters:</p> Name Type Description Default <code>results</code> <code>list</code> <p>A list of results</p> required Source code in <code>aigve/metrics/multi_aspect_metrics/videophy/videophy_metric.py</code> <pre><code>def compute_metrics(self, results: list) -&gt; dict:\n    \"\"\"\n    This function is used to compute the metrics.\n\n    Args:\n        results (list): A list of results\n    \"\"\"\n    return {\n        'entailment': float(np.mean(results))\n    }\n</code></pre>"},{"location":"documentations/metrics/videophy/#aigve.metrics.multi_aspect_metrics.videophy.videophy_metric.VideoPhy.get_entail","title":"<code>get_entail(logits, input_ids)</code>","text":"<p>This function is used to get the entailment scores.</p> <p>Parameters:</p> Name Type Description Default <code>logits</code> <code>Tensor</code> <p>A tensor containing the logits</p> required <code>input_ids</code> <code>Tensor</code> <p>A tensor containing the input IDs</p> required Source code in <code>aigve/metrics/multi_aspect_metrics/videophy/videophy_metric.py</code> <pre><code>def get_entail(self, logits, input_ids):\n    \"\"\"\n    This function is used to get the entailment scores.\n\n    Args:\n        logits (torch.Tensor): A tensor containing the logits\n        input_ids (torch.Tensor): A tensor containing the input IDs\n    \"\"\"\n    softmax = nn.Softmax(dim=2)\n    logits = softmax(logits)\n    token_id_yes = self.tokenizer.encode('Yes', add_special_tokens=False)[0]\n    token_id_no = self.tokenizer.encode('No', add_special_tokens=False)[0]\n    entailment = []\n    for j in range(len(logits)):\n        for i in range(len(input_ids[j])):\n            if input_ids[j][i] == self.tokenizer.pad_token_id:  # pad token if the answer is not present\n                i = i - 1\n                break\n            elif i == len(input_ids[j]) - 1:\n                break\n        score = logits[j][i][token_id_yes] / (logits[j][i][token_id_yes] + logits[j][i][token_id_no])\n        entailment.append(score)\n    entailment = torch.stack(entailment)\n    return entailment\n</code></pre>"},{"location":"documentations/metrics/videophy/#aigve.metrics.multi_aspect_metrics.videophy.videophy_metric.VideoPhy.get_logits","title":"<code>get_logits(data_batch)</code>","text":"<p>This function is used to get the logits for each input in the data batch.</p> <p>Parameters:</p> Name Type Description Default <code>data_batch</code> <code>dict</code> <p>A dictionary containing the data batch</p> required <p>Returns:     logits (torch.Tensor): A tensor containing the logits for each input in the data batch</p> Source code in <code>aigve/metrics/multi_aspect_metrics/videophy/videophy_metric.py</code> <pre><code>def get_logits(self, data_batch):\n    \"\"\"\n    This function is used to get the logits for each input in the data batch.\n\n    Args:\n        data_batch (dict): A dictionary containing the data batch\n    Returns:\n        logits (torch.Tensor): A tensor containing the logits for each input in the data batch\n    \"\"\"\n    # Iterate over each item in the data batch\n    for k, v in data_batch.items():\n        # Check if the item is a tensor\n        if torch.is_tensor(v):\n            # Convert float tensors to bfloat16\n            if v.dtype == torch.float:\n                data_batch[k] = v.bfloat16()\n            # Move the tensor to the model's device (e.g., GPU)\n            data_batch[k] = data_batch[k].to(self.model.device)\n\n    # print(\"Data batch: \", data_batch.keys())\n    outputs = self.model(pixel_values=data_batch['pixel_values'], video_pixel_values=data_batch['video_pixel_values'],\n                    labels=None, \\\n                    num_images=data_batch['num_images'], num_videos=data_batch['num_videos'], input_ids=data_batch['input_ids'],\n                    non_padding_mask=data_batch['non_padding_mask'], \\\n                    non_media_mask=data_batch['non_media_mask'], prompt_mask=data_batch['prompt_mask'])\n    logits = outputs['logits']\n    return logits\n</code></pre>"},{"location":"documentations/metrics/videophy/#aigve.metrics.multi_aspect_metrics.videophy.videophy_metric.VideoPhy.process","title":"<code>process(data_batch, data_samples)</code>","text":"<p>This function is used to process the data batch and compute the metric.</p> <p>Parameters:</p> Name Type Description Default <code>data_batch</code> <code>dict</code> <p>A dictionary containing the data batch</p> required <code>data_samples</code> <code>list</code> <p>A list of dictionaries containing the data samples</p> required Source code in <code>aigve/metrics/multi_aspect_metrics/videophy/videophy_metric.py</code> <pre><code>def process(self, data_batch: Any, data_samples: Sequence[dict]) -&gt; None:\n    \"\"\"\n    This function is used to process the data batch and compute the metric.\n\n    Args:\n        data_batch (dict): A dictionary containing the data batch\n        data_samples (list): A list of dictionaries containing the data samples\n    \"\"\"\n    logits = self.get_logits(data_batch)\n    entails_scores =  self.get_entail(logits, data_batch['input_ids'])\n\n    self.results.extend(entails_scores.cpu().detach().to(torch.float32).numpy().tolist())\n</code></pre>"},{"location":"documentations/metrics/viescore/","title":"class VideoScore","text":"<p>               Bases: <code>BaseMetric</code></p> Source code in <code>aigve/metrics/multi_aspect_metrics/videoscore/videoscore_metric.py</code> <pre><code>@METRICS.register_module()\nclass VideoScore(BaseMetric):\n    def __init__(self,\n                collect_device: Optional[Union[str, torch.device]] = None,\n                prefix: Optional[str] = None,\n                metric_path: str = None,\n                model_path: str = 'TIGER-Lab/VideoScore-v1.1',\n                datainfo_path: str = None,\n                test_index: int = None,\n                 **kwargs):\n        \"\"\"\n        Args:\n            collect_device (Optional[Union[str, torch.device]]): The device to collect the data on.\n            prefix (Optional[str]): The prefix to use for the metric.\n            metric_path (str): The path to the metric file.\n            model_path (str): The path to the model file.\n            datainfo_path (str): The path to the datainfo file.\n            test_index (int): The index of the test data.\n        \"\"\"\n        super().__init__(collect_device=collect_device, prefix=prefix)\n        # self.train_index = train_index\n        # TODO: ARE THERE PARAMETERS REQUIRED FOR THIS METRIC?\n        self.metric_path = metric_path\n        self.model_path = model_path\n        self.datainfo_path = datainfo_path\n        self.test_index = test_index\n\n\n        self.model = Idefics2ForSequenceClassification.from_pretrained(self.model_path, torch_dtype=torch.bfloat16).eval()\n        self.device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n        self.model.to(self.device)\n\n        self.results = []\n\n    def process(self, data_batch: Any, data_samples: Sequence[dict]) -&gt; None:\n        \"\"\"\n        Args:\n            data_batch (Any): The data batch to process.\n            data_samples (Sequence[dict]): The data samples to process.\n        \"\"\"\n\n\n        data_batch = {k: v[0].to(self.model.device) for k, v in data_batch.items()}\n\n        with torch.no_grad():\n            outputs = self.model(**data_batch)\n\n        logits = outputs.logits.cpu().detach().to(torch.float32).numpy()\n        num_aspects = logits.shape[-1]\n\n        aspect_scores = []\n        for i in range(num_aspects):\n            aspect_scores.append(round(logits[0, i].item(), 3))\n\n        self.results.append(aspect_scores)\n\n    def compute_metrics(self, results: list) -&gt; dict:\n        \"\"\"\n        Args:\n            results (list): The results to compute the metrics from.\n        \"\"\"\n        results = np.array(results)\n        mean_scores = np.mean(results, axis=1)\n\n        return {'visual_quailty': results[:, 0].tolist(),\n                'temporal_consistency': results[:, 1].tolist(),\n                'dynamic_degree': results[:, 2].tolist(),\n                'text-to-video_alignment': results[:, 3].tolist(),\n                'factual_consistency': results[:, 4].tolist(),\n                'summary': {'visual_quality': mean_scores[0], 'temporal_consistency': mean_scores[1],\n                            'dynamic_degree': mean_scores[2], 'text-to-video_alignment': mean_scores[3],\n                            'factual_consistency': mean_scores[4]}}\n</code></pre>"},{"location":"documentations/metrics/viescore/#aigve.metrics.multi_aspect_metrics.videoscore.videoscore_metric.VideoScore.__init__","title":"<code>__init__(collect_device=None, prefix=None, metric_path=None, model_path='TIGER-Lab/VideoScore-v1.1', datainfo_path=None, test_index=None, **kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>collect_device</code> <code>Optional[Union[str, device]]</code> <p>The device to collect the data on.</p> <code>None</code> <code>prefix</code> <code>Optional[str]</code> <p>The prefix to use for the metric.</p> <code>None</code> <code>metric_path</code> <code>str</code> <p>The path to the metric file.</p> <code>None</code> <code>model_path</code> <code>str</code> <p>The path to the model file.</p> <code>'TIGER-Lab/VideoScore-v1.1'</code> <code>datainfo_path</code> <code>str</code> <p>The path to the datainfo file.</p> <code>None</code> <code>test_index</code> <code>int</code> <p>The index of the test data.</p> <code>None</code> Source code in <code>aigve/metrics/multi_aspect_metrics/videoscore/videoscore_metric.py</code> <pre><code>def __init__(self,\n            collect_device: Optional[Union[str, torch.device]] = None,\n            prefix: Optional[str] = None,\n            metric_path: str = None,\n            model_path: str = 'TIGER-Lab/VideoScore-v1.1',\n            datainfo_path: str = None,\n            test_index: int = None,\n             **kwargs):\n    \"\"\"\n    Args:\n        collect_device (Optional[Union[str, torch.device]]): The device to collect the data on.\n        prefix (Optional[str]): The prefix to use for the metric.\n        metric_path (str): The path to the metric file.\n        model_path (str): The path to the model file.\n        datainfo_path (str): The path to the datainfo file.\n        test_index (int): The index of the test data.\n    \"\"\"\n    super().__init__(collect_device=collect_device, prefix=prefix)\n    # self.train_index = train_index\n    # TODO: ARE THERE PARAMETERS REQUIRED FOR THIS METRIC?\n    self.metric_path = metric_path\n    self.model_path = model_path\n    self.datainfo_path = datainfo_path\n    self.test_index = test_index\n\n\n    self.model = Idefics2ForSequenceClassification.from_pretrained(self.model_path, torch_dtype=torch.bfloat16).eval()\n    self.device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n    self.model.to(self.device)\n\n    self.results = []\n</code></pre>"},{"location":"documentations/metrics/viescore/#aigve.metrics.multi_aspect_metrics.videoscore.videoscore_metric.VideoScore.compute_metrics","title":"<code>compute_metrics(results)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>results</code> <code>list</code> <p>The results to compute the metrics from.</p> required Source code in <code>aigve/metrics/multi_aspect_metrics/videoscore/videoscore_metric.py</code> <pre><code>def compute_metrics(self, results: list) -&gt; dict:\n    \"\"\"\n    Args:\n        results (list): The results to compute the metrics from.\n    \"\"\"\n    results = np.array(results)\n    mean_scores = np.mean(results, axis=1)\n\n    return {'visual_quailty': results[:, 0].tolist(),\n            'temporal_consistency': results[:, 1].tolist(),\n            'dynamic_degree': results[:, 2].tolist(),\n            'text-to-video_alignment': results[:, 3].tolist(),\n            'factual_consistency': results[:, 4].tolist(),\n            'summary': {'visual_quality': mean_scores[0], 'temporal_consistency': mean_scores[1],\n                        'dynamic_degree': mean_scores[2], 'text-to-video_alignment': mean_scores[3],\n                        'factual_consistency': mean_scores[4]}}\n</code></pre>"},{"location":"documentations/metrics/viescore/#aigve.metrics.multi_aspect_metrics.videoscore.videoscore_metric.VideoScore.process","title":"<code>process(data_batch, data_samples)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>data_batch</code> <code>Any</code> <p>The data batch to process.</p> required <code>data_samples</code> <code>Sequence[dict]</code> <p>The data samples to process.</p> required Source code in <code>aigve/metrics/multi_aspect_metrics/videoscore/videoscore_metric.py</code> <pre><code>def process(self, data_batch: Any, data_samples: Sequence[dict]) -&gt; None:\n    \"\"\"\n    Args:\n        data_batch (Any): The data batch to process.\n        data_samples (Sequence[dict]): The data samples to process.\n    \"\"\"\n\n\n    data_batch = {k: v[0].to(self.model.device) for k, v in data_batch.items()}\n\n    with torch.no_grad():\n        outputs = self.model(**data_batch)\n\n    logits = outputs.logits.cpu().detach().to(torch.float32).numpy()\n    num_aspects = logits.shape[-1]\n\n    aspect_scores = []\n    for i in range(num_aspects):\n        aspect_scores.append(round(logits[0, i].item(), 3))\n\n    self.results.append(aspect_scores)\n</code></pre>"},{"location":"documentations/utils/","title":"aigve.utils","text":""},{"location":"documentations/utils/#aigve.utils.LoadVideoFromFile","title":"<code>LoadVideoFromFile</code>","text":"<p>               Bases: <code>BaseTransform</code></p> <p>Load a video from file.</p> <p>Required Keys:</p> <pre><code>- video_path_pd\n</code></pre> Modified Keys <ul> <li>video_pd</li> </ul> <p>Parameters:</p> Name Type Description Default <code>height</code> <code>int</code> <p>int, default is -1 Desired output height of the video, unchanged if <code>-1</code> is specified.</p> <code>-1</code> <code>width</code> <code>int</code> <p>int, default is -1 Desired output width of the video, unchanged if <code>-1</code> is specified. See details in: https://github.com/dmlc/decord/blob/master/python/decord/video_reader.py#L18</p> <code>-1</code> Source code in <code>aigve/utils/loading.py</code> <pre><code>@TRANSFORMS.register_module()\nclass LoadVideoFromFile(BaseTransform):\n    \"\"\"Load a video from file.\n\n    Required Keys:\n\n        - video_path_pd\n\n    Modified Keys:\n        - video_pd\n\n    Args:\n        height: int, default is -1\n            Desired output height of the video, unchanged if `-1` is specified.\n        width: int, default is -1\n            Desired output width of the video, unchanged if `-1` is specified.\n            See details in: https://github.com/dmlc/decord/blob/master/python/decord/video_reader.py#L18\n    \"\"\"\n\n    def __init__(self, height: int = -1, width: int = -1):\n        self.height = height\n        self.width = width\n\n\n    def transform(self, results: dict) -&gt; Optional[dict]:\n        \"\"\"Functions to load video. \n        Referred to 'https://github.com/Vchitect/VBench/blob/master/vbench/utils.py#L103'\n\n        The function supports loading video in GIF (.gif), PNG (.png), and MP4 (.mp4) formats.\n        Depending on the format, it processes and extracts frames accordingly.\n\n        Args:\n            results (dict): Result dict from\n                :class:`mmengine.dataset.BaseDataset`.\n\n        Returns:\n            dict: The dict contains loaded video in shape (F, C, H, W) and \n            meta information if needed. F is the number of frames, C is the \n            number of channels, H is the height, and W is the width.\n\n        Raises:\n            - NotImplementedError: If the video format is not supported.\n\n        The function first determines the format of the video file by its extension.\n        For GIFs, it iterates over each frame and converts them to RGB.\n        For PNGs, it reads the single frame, converts it to RGB.\n        For MP4s, it reads the frames using the VideoReader class and converts them to NumPy arrays.\n        If a data_transform is provided, it is applied to the buffer before converting it to a tensor.\n        Finally, the tensor is permuted to match the expected (F, C, H, W) format.\n        \"\"\"\n\n        video_path = results['video_path_pd']\n        if video_path.endswith('.gif'):\n            frame_ls = []\n            img = Image.open(video_path)\n            for frame in ImageSequence.Iterator(img):\n                frame = frame.convert('RGB')\n                frame = np.array(frame).astype(np.uint8)\n                frame_ls.append(frame)\n            buffer = np.array(frame_ls).astype(np.uint8) # (F, H, W, C), np.uint8\n        elif video_path.endswith('.png'):\n            frame = Image.open(video_path)\n            frame = frame.convert('RGB')\n            frame = np.array(frame).astype(np.uint8)\n            frame_ls = [frame]\n            buffer = np.array(frame_ls) # (1, H, W, C), np.uint8\n        elif video_path.endswith('.mp4'):\n            import decord\n            decord.bridge.set_bridge('native')\n            if self.width and self.height:\n                video_reader = VideoReader(video_path, width=self.width, height=self.height, num_threads=1)\n            else:\n                video_reader = VideoReader(video_path, num_threads=1)\n            frames = video_reader.get_batch(range(len(video_reader)))  # (F, H, W, C), torch.uint8\n            buffer = frames.asnumpy().astype(np.uint8) # (F, H, W, C), np.uint8\n        else:\n            raise NotImplementedError\n\n        frames = torch.Tensor(buffer)\n        frames = frames.permute(0, 3, 1, 2) # (F, C, H, W), torch.uint8\n        results['video_pd'] = frames\n\n        return results\n\n    def __repr__(self):\n        repr_str = (f'{self.__class__.__name__}, '\n                    f'height={self.height}, '\n                    f'width={self.width}')\n</code></pre>"},{"location":"documentations/utils/#aigve.utils.LoadVideoFromFile.transform","title":"<code>transform(results)</code>","text":"<p>Functions to load video.  Referred to 'https://github.com/Vchitect/VBench/blob/master/vbench/utils.py#L103'</p> <p>The function supports loading video in GIF (.gif), PNG (.png), and MP4 (.mp4) formats. Depending on the format, it processes and extracts frames accordingly.</p> <p>Parameters:</p> Name Type Description Default <code>results</code> <code>dict</code> <p>Result dict from :class:<code>mmengine.dataset.BaseDataset</code>.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>Optional[dict]</code> <p>The dict contains loaded video in shape (F, C, H, W) and </p> <code>Optional[dict]</code> <p>meta information if needed. F is the number of frames, C is the </p> <code>Optional[dict]</code> <p>number of channels, H is the height, and W is the width.</p> <p>Raises:</p> Type Description <code>-NotImplementedError</code> <p>If the video format is not supported.</p> <p>The function first determines the format of the video file by its extension. For GIFs, it iterates over each frame and converts them to RGB. For PNGs, it reads the single frame, converts it to RGB. For MP4s, it reads the frames using the VideoReader class and converts them to NumPy arrays. If a data_transform is provided, it is applied to the buffer before converting it to a tensor. Finally, the tensor is permuted to match the expected (F, C, H, W) format.</p> Source code in <code>aigve/utils/loading.py</code> <pre><code>def transform(self, results: dict) -&gt; Optional[dict]:\n    \"\"\"Functions to load video. \n    Referred to 'https://github.com/Vchitect/VBench/blob/master/vbench/utils.py#L103'\n\n    The function supports loading video in GIF (.gif), PNG (.png), and MP4 (.mp4) formats.\n    Depending on the format, it processes and extracts frames accordingly.\n\n    Args:\n        results (dict): Result dict from\n            :class:`mmengine.dataset.BaseDataset`.\n\n    Returns:\n        dict: The dict contains loaded video in shape (F, C, H, W) and \n        meta information if needed. F is the number of frames, C is the \n        number of channels, H is the height, and W is the width.\n\n    Raises:\n        - NotImplementedError: If the video format is not supported.\n\n    The function first determines the format of the video file by its extension.\n    For GIFs, it iterates over each frame and converts them to RGB.\n    For PNGs, it reads the single frame, converts it to RGB.\n    For MP4s, it reads the frames using the VideoReader class and converts them to NumPy arrays.\n    If a data_transform is provided, it is applied to the buffer before converting it to a tensor.\n    Finally, the tensor is permuted to match the expected (F, C, H, W) format.\n    \"\"\"\n\n    video_path = results['video_path_pd']\n    if video_path.endswith('.gif'):\n        frame_ls = []\n        img = Image.open(video_path)\n        for frame in ImageSequence.Iterator(img):\n            frame = frame.convert('RGB')\n            frame = np.array(frame).astype(np.uint8)\n            frame_ls.append(frame)\n        buffer = np.array(frame_ls).astype(np.uint8) # (F, H, W, C), np.uint8\n    elif video_path.endswith('.png'):\n        frame = Image.open(video_path)\n        frame = frame.convert('RGB')\n        frame = np.array(frame).astype(np.uint8)\n        frame_ls = [frame]\n        buffer = np.array(frame_ls) # (1, H, W, C), np.uint8\n    elif video_path.endswith('.mp4'):\n        import decord\n        decord.bridge.set_bridge('native')\n        if self.width and self.height:\n            video_reader = VideoReader(video_path, width=self.width, height=self.height, num_threads=1)\n        else:\n            video_reader = VideoReader(video_path, num_threads=1)\n        frames = video_reader.get_batch(range(len(video_reader)))  # (F, H, W, C), torch.uint8\n        buffer = frames.asnumpy().astype(np.uint8) # (F, H, W, C), np.uint8\n    else:\n        raise NotImplementedError\n\n    frames = torch.Tensor(buffer)\n    frames = frames.permute(0, 3, 1, 2) # (F, C, H, W), torch.uint8\n    results['video_pd'] = frames\n\n    return results\n</code></pre>"},{"location":"documentations/utils/#aigve.utils.read_image_detectron2","title":"<code>read_image_detectron2(file_name, format=None)</code>","text":"<p>Read an image into the given format. Will apply rotation and flipping if the image has such exif information.</p> <p>Parameters:</p> Name Type Description Default <code>file_name</code> <code>str</code> <p>image file path</p> required <code>format</code> <code>str</code> <p>one of the supported image modes in PIL, or \"BGR\" or \"YUV-BT.601\".</p> <code>None</code> <p>Returns:</p> Name Type Description <code>image</code> <code>ndarray</code> <p>an HWC image in the given format, which is 0-255, uint8 for supported image modes in PIL or \"BGR\"; float (0-1 for Y) for YUV-BT.601.</p> Source code in <code>aigve/utils/image_reading.py</code> <pre><code>def read_image_detectron2(file_name, format=None):\n    \"\"\"\n    Read an image into the given format.\n    Will apply rotation and flipping if the image has such exif information.\n\n    Args:\n        file_name (str): image file path\n        format (str): one of the supported image modes in PIL, or \"BGR\" or \"YUV-BT.601\".\n\n    Returns:\n        image (np.ndarray):\n            an HWC image in the given format, which is 0-255, uint8 for\n            supported image modes in PIL or \"BGR\"; float (0-1 for Y) for YUV-BT.601.\n    \"\"\"\n    try:\n        import detectron2\n    except ImportError:\n        print(\"detectron2 is not installed. Installing...\")\n        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"detectron2\"])\n\n        return detectron2.data.detection_utils.read_image(img_src, format=\"BGR\")\n</code></pre>"},{"location":"examples/","title":"Examples","text":"<p>...</p>"},{"location":"guides/","title":"Get Started with AIGVE","text":"<ul> <li> <p> About us</p> <p>Get more information about <code>AIGVE</code> and use it in your function learning projects.</p> <p> More Information</p> </li> <li> <p> Installation</p> <p>Install <code>AIGVE</code> with <code>pip</code> and set up the dependency in just minutes.</p> <p> How to Install</p> </li> <li> <p> Quickstart</p> <p>A quickstart tutorial to help you use <code>AIGVE</code> APIs for AIGVE-Tool model training.</p> <p> Get Started</p> </li> </ul>"},{"location":"guides/about_us/","title":"About us","text":"<p>AIGVE-Tool is a website hosting the documentations, tutorials, examples and the latest updates about the <code>AIGVE</code> library.</p>"},{"location":"guides/about_us/#what-is-aigve","title":"\ud83d\ude80 What is <code>AIGVE</code>?","text":"<p><code>AIGVE</code> (AI Generated Video Evaluation Toolkit) provides a comprehensive and structured evaluation framework for assessing AI-generated video quality developed by the IFM Lab. It integrates multiple evaluation metrics, covering diverse aspects of video evaluation, including neural-network-based assessment, distribution comparison, vision-language alignment, and multi-faceted analysis.</p> <ul> <li>Official Website: https://www.aigve.org/</li> <li>Github Repository: https://github.com/ShaneXiangH/VQA_Toolkit</li> </ul> <ul> <li>IFM Lab https://www.ifmlab.org/</li> </ul>"},{"location":"guides/about_us/#citing-us","title":"Citing Us","text":"<p>If you find <code>AIGVE</code> library and <code>...</code> papers useful in your work, please cite the papers as follows: <pre><code>\n</code></pre></p>"},{"location":"guides/about_us/#library-organization","title":"Library Organization","text":""},{"location":"guides/about_us/#distribution-comparison-based-evaluation-metrics","title":"\ud83d\udcca Distribution Comparison-Based Evaluation Metrics","text":"<p>These metrics assess the quality of generated videos by comparing the distribution of real and generated samples.</p> <ul> <li>\u2705 FID: Frechet Inception Distance (FID) quantifies the similarity between real and generated video feature distributions by measuring the Wasserstein-2 distance.</li> <li>\u2705 FVD: Frechet Video Distance (FVD) extends the FID approach to video domain by leveraging spatio-temporal features extracted from action recognition networks.</li> <li>\u2705 IS: Inception Score (IS) evaluates both the quality and diversity of generated content by analyzing conditional label distributions.</li> </ul>"},{"location":"guides/about_us/#video-only-neural-network-based-evaluation-metrics","title":"\ud83e\udde0 Video-only Neural Network-Based Evaluation Metrics","text":"<p>These metrics leverage deep learning models to assess AI-generated video quality based on learned representations.</p> <ul> <li>\u2705 GSTVQA: Generalized Spatio-Temporal VQA (GSTVQA) employs graph-based spatio-temporal analysis to assess video quality.</li> <li>\u2705 SimpleVQA: Simple Video Quality Assessment (Simple-VQA) utilizes deep learning features for no-reference video quality assessment.</li> <li>\u2705 LightVQA+: Light Video Quality Assessment Plus (Light-VQA+) incorporates exposure quality guidance to evaluate video quality.</li> </ul>"},{"location":"guides/about_us/#vision-language-similarity-based-evaluation-metrics","title":"\ud83d\udd0d Vision-Language Similarity-Based Evaluation Metrics","text":"<p>These metrics evaluate alignment, similarity, and coherence between visual and textual representations, often using embeddings from models like CLIP and BLIP.</p> <ul> <li>\u2705 CLIPSim: CLIP Similarity (CLIPSim) leverages CLIP embeddings to measure semantic similarity between videos and text.</li> <li>\u2705 CLIPTemp: CLIP Temporal (CLIPTemp) extends CLIPSim by incorporating temporal consistency assessment.</li> <li>\u2705 BLIPSim: Bootstrapped Language-Image Pre-training Similarity (BLIPSim) uses advanced pre-training techniques to improve video-text alignment evaluation.</li> <li>\u2705 Pickscore: PickScore incorporates human preference data to provide more perceptually aligned measurement of video-text matching.</li> </ul>"},{"location":"guides/about_us/#vision-language-understanding-based-evaluation-metrics","title":"\ud83e\udde0 Vision-Language Understanding-Based Evaluation Metrics","text":"<p>These metrics assess higher-level understanding, reasoning, and factual consistency in vision-language models.</p> <ul> <li>\u2705 VIEScore: Video Information Evaluation Score (VIEScore) provides explainable assessments of conditional image synthesis.</li> <li>\u2705 TIFA: Text-Image Faithfulness Assessment (TIFA) employs question-answering techniques to evaluate text-to-image alignment.</li> <li>\u2705 DSG: Davidsonian Scene Graph (DSG) improves fine-grained evaluation reliability through advanced scene graph representations.</li> </ul>"},{"location":"guides/about_us/#multi-faceted-evaluation-metrics","title":"\ud83d\udd04 Multi-Faceted Evaluation Metrics","text":"<p>These metrics integrate structured, multi-dimensional assessments to provide a holistic benchmarking framework for AI-generated videos.</p> <ul> <li>\u2705 VideoPhy: Video Physics Evaluation (VideoPhy) specifically assesses the physical plausibility of generated videos.</li> <li>\u2705 VideoScore: Video Score (VideoScore) simulates fine-grained human feedback across multiple evaluation dimensions.</li> </ul>"},{"location":"guides/about_us/#key-features","title":"Key Features","text":"<ul> <li>Multi-Dimensional Evaluation: Covers video coherence, physics, and benchmarking.</li> <li>Open-Source &amp; Customizable: Designed for easy integration.</li> <li>Cutting-Edge AI Assessment: Supports various AI-generated video tasks.</li> </ul>"},{"location":"guides/about_us/#license-copyright","title":"License &amp; Copyright","text":"<p>Copyright \u00a9 2025 IFM Lab. All rights reserved.</p> <ul> <li><code>AIGVE</code> source code is published under the terms of the MIT License. </li> <li><code>AIGVE</code> documentation and the <code>...</code> papers are licensed under a Creative Commons Attribution-Share Alike 4.0 Unported License (CC BY-SA 4.0). </li> </ul>"},{"location":"guides/installation/","title":"Installation of the AIGVE-Tool Library","text":""},{"location":"guides/installation/#how-to-install","title":"\ud83d\udccc How to Install","text":"<pre><code>pip install aigve\n</code></pre>"},{"location":"guides/quick_start/","title":"Quickstart Tutorial","text":""},{"location":"guides/quick_start/#what-is-aigve","title":"\ud83d\ude80 What is AIGVE?","text":"<p>AIGVE (AI Generated Video Evaluation Toolkit) provides a comprehensive and structured evaluation framework for assessing AI-generated video quality developed by the IFM Lab. It integrates multiple evaluation metrics, covering diverse aspects of video evaluation, including neural-network-based assessment, distribution comparison, vision-language alignment, and multi-faceted analysis.</p>"},{"location":"news/","title":"News","text":"<p>...</p>"},{"location":"tutorials/","title":"Tutorials","text":"<p>...</p>"}]}